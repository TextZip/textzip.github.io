[
  {
    "title": "Bi-Manual Loco-Manipulation in Quadrupeds for Opening Doors",
    "url": "/posts/BiMan-DRL/",
    "categories": "Projects, Manipulators",
    "tags": "reinforcement learning, sim2real, manipulators, quadrupeds",
    "date": "2024-12-02 13:13:20 +0530",
    "snippet": "This project explores the use of Action Chunking Transformers and related architectures for learning via demonstration for solving loco-manipulation tasks, in paticular interacting with a varity of doors via various action groups such as push, pull and re-grasping.Brief OverviewSome Early ResultsPushing DoorPulling DoorMore details about the project to be released soon…"
  },
  {
    "title": "Getting Started with SO100 for Reinforcement Learning",
    "url": "/posts/GS-SO-100/",
    "categories": "Projects, Manipulators",
    "tags": "reinforcement learning, sim2real, manipulators",
    "date": "2024-10-08 13:13:20 +0530",
    "snippet": "Over the past few years, I’ve had the opportunity to tinker with various robots—quadrupeds, bipeds, wheeled robots, and more. This hands-on experience has helped me apply and refine my skills in reinforcement learning. However, despite all this, I’ve always wanted to explore the full Sim2Real pipeline—from designing a robot from scratch (CAD and mechanical design) to fabrication, simulation, training, and ultimately deploying it in the real world.This blog post captures my journey of attempting this process with the SO-100, a low-cost, easy-to-build robotic arm.In particular, I will cover the following topics:  Designing the robot in CAD (SolidWorks) and generating a URDF  Importing the URDF into Isaac Sim/USD Composer  Adding sensors and actuators in Isaac Sim  Performing system identification  Training a basic policy  Sim2Real transfer and deploymentMore details about the project to be added soon…"
  },
  {
    "title": "Locomotion with Weighted Belief in Exteroception",
    "url": "/posts/LIDAR-DRL/",
    "categories": "Projects, Quadrupeds",
    "tags": "reinforcement learning, lidar, quadruped",
    "date": "2024-04-12 13:13:20 +0530",
    "snippet": "This project goes over the integration of elevation maps (typically obtained from LIDARs/Depth Cameras) into the locomotion pipeline and the intricacies involved in this process. We will try to reimplement and upgrade(in my humble opinion) a pretty well known paper from ETH RSL titled Learning robust perceptive locomotion for quadrupedal robots in the wild et al. Takahiro Miki.  Note: Detailed results and video clips can be found in the Results section below.Paper OverviewAs you might have noticed, this paper is pretty huge (in both quality and quantity), and in a way is a compilation of their past work and new results. Therefore I will try to break this paper into several sections and focus on one topic at a time.The entire paper can be broken down into the following parts:  Foot Trajectory Generator for Gait  Noise Model for the LIDAR/Elevation Points  Belief State Encoder/Decoder ArchFoot Trajectory GeneratorsThe paper builds on top of their older work on blind locomotion using FTG, so if you are unaware of FTGs please refer to my blog post on it here and also to these two papers[1],[2].But a TLDR about FTGs is basically the fact that they provide a method to offload the gait generation from the locomotion policy via the the use of a bunch of Trajectory Generation Equations, and the policy is given control over some of the parameters that are embedded into these equations and finally the output of the policy and the output of the FTG is added before being passed onto the robot.Noise in LIDARA crucial part of the paper involves the noise model that is used to bridge the sim2real gap between the scan dots that are obtained using ground truth in sim vs the real world noisy values. The following snippet from the paper explains all the different types of noises that are superimposed and added to the ground truth.As you might have noticed from the above annotated image, the paper has some missing elements for the noise parameters, I will be sharing the noise parameters I have used in the custom implementation section.Belief State Encoder/DecoderCustom ImplementationTeacher TrainingStudent TrainingModified Belief State Encoder/DecoderDeploymentResults"
  },
  {
    "title": "Neural Style Transfer for Locomotion",
    "url": "/posts/NST-DRL/",
    "categories": "Projects, Quadrupeds",
    "tags": "reinforcement learning, gait, quadruped",
    "date": "2024-01-20 13:13:20 +0530",
    "snippet": "How do you ensure that a reinforcement learning (RL)-based locomotion policy produces a natural and efficient gait? This project is particularly close to my heart because it helped me answer this question I’ve had since I first started working on locomotion policies in RL. In this blog, I’ll walk you through an approach that can help you move beyond tedious reward tuning and instead focus on achieving the perfect gait for your robot—without the frustration and the transfer this gait across multiple robot embodiments.ResultsTo be updated soon"
  },
  {
    "title": "Policy Modulated Trajectory Generation for Quadrupeds",
    "url": "/posts/FTG-DRL/",
    "categories": "Projects, Quadrupeds",
    "tags": "reinforcement learning, gait, quadruped",
    "date": "2024-01-12 13:13:20 +0530",
    "snippet": "This blog post explores the idea of using a set of trajectory generation equations in conjunction with a neural network for quadruped locomotion. We will discuss the advantages of this approach and how it compares to directly using a neural network to generate joint angle deltas.My implementation is inspired by the method presented in the paper Policies Modulating Trajectory Generators et al. Atil Iscen, with a few modifications.  Note: Detailed results and video clips can be found in the Results section below.Why Use a Trajectory Generator?You might wonder why this method is necessary when direct neural network-based control has already shown promising results, as demonstrated in our previous post on blind locomotion using proprioception. The key advantage of using a trajectory generator (TG) is that it produces cleaner gaits with minimal reward function tuning. Since the gait is defined by a system of equations, the neural network is relieved from learning the complexities of generating a stable gait. Instead, the network focuses on modulation and stabilization, leading to more efficient learning and better overall performance.Paper OverviewBefore diving into our custom implementation, let’s examine the approach used in the original paper.As illustrated in the figure below, the trajectory generator is indirectly controlled by a subset of the policy’s output (TG Parameters). These parameters are set based on the TG state along with other inputs to the policy. The final joint position command sent to the robot is the sum of the TG’s output and the policy’s modulation.In an ideal scenario, even if the policy outputs zero joint modifications, the trajectory generator should still produce a trotting motion, allowing the robot to trot in place. The policy’s primary task is to stabilize this motion and modulate it as needed to achieve locomotion by adjusting the phase, height, or stride length of the legs.To understand the implementation in more detail, let’s take a look at the parameters that the policy sets.From the paper:  The detailed architecture adapted to quadruped locomotion is shown in Fig. 5. At every timestep, the policy receives observations (s), desired velocity (v, control input) and the phase (φ) of the trajectory generator. It computes 3 parameters for the TG (frequency f, amplitude a and walking height h) and 8 actions for the legs of the robot ($u_{fb}$) that will directly be added to the TG’s calculated leg positions ($u_{tg}$). The sum of these actions is used as desired motor positions, which are tracked by Proportional-Derivative controllers. Since the policy dictates the frequency at each time step, it dictates the step-size that will be added to TG’s phase. This eventually allows the policy to warp time and use the TG in a time-independent fashion.Custom ImplementationThe custom implementation is strongly inspired by the above paper and method but has some crucial changes that helped with a smoother implementation.The most important changes are to the inputs to the FTG from the policy consisting of only the residual phase differences and the policy input consisting of total phase and current phases for each legs.Here is a detailed breakdown to further explain things:The policy output can be broken down into two components: joint actions ${(a_t^P)}$ and residual phase difference $(\\Delta\\phi_{res})$The height of each leg as a function of the total phase$(\\Phi)$ of the leg $h(\\Phi)$ in the FTG can be expressed using the following formula\\[\\begin{aligned}h(\\Phi) =\\begin{cases}h_{\\max} \\left(-2\\left(\\dfrac{2}{\\pi}\\Phi\\right)^3 + 3\\left(\\dfrac{2}{\\pi}\\Phi\\right)^2\\right), &amp;amp; \\text{if } 0 \\leq \\Phi \\leq \\frac{\\pi}{2}, \\\\h_{\\max} \\left(2\\left(\\dfrac{2}{\\pi}\\Phi - 1\\right)^3 - 3\\left(\\dfrac{2}{\\pi}\\Phi - 1\\right)^2 + 1\\right), &amp;amp; \\text{if } \\frac{\\pi}{2} &amp;lt; \\Phi \\leq \\pi, \\\\0, &amp;amp; \\text{if } \\pi &amp;lt; \\Phi &amp;lt; 2\\pi.\\end{cases}\\end{aligned}\\]The total phase of the leg $(\\Phi)$ is computed using the following formula:In the above equation, $\\phi$ is the current phase that is incremented each time step as follows $\\phi_{t+1} = ( \\phi_t + \\Delta\\phi) \\text{ mod}(2\\pi)$ where $\\Delta\\phi$ is a constant like 0.2In the above equation, $\\phi_{base}$ is used to encode a gait prior by setting in the phase difference between different legs in the robot, its value is usually like\\[\\phi_{base} = \\begin{cases}[0, \\pi, \\pi, 0], &amp;amp; \\text{if walk}\\\\[\\pi, \\pi, 0, 0], &amp;amp; \\text{if gallop}\\\\[0, 0, 0, 0], &amp;amp; \\text{if stand}\\\\\\end{cases}\\]And finally $\\Delta\\phi_{res}$ is the residual phase difference that can be set by the policy as part of its output to make adjustments to the total phase of each leg.The height of each foot is now used to compute the joint angles for each leg using Inverse Kinematics as follows:For the First Joint Angle $(q_1)$\\[L = \\sqrt{p_y^2 + p_z^2 - l_1^2}\\]\\[q_1 = \\arctan\\!\\left(\\frac{p_z\\, l_1 + p_y\\, L}{p_y\\, l_1 - p_z\\, L}\\right)\\]For the Third Joint Angle $(q_3)$\\[\\text{temp} = \\frac{b_{3z}^2 + b_{4z}^2 - b^2}{2\\,\\left|b_{3z}\\, b_{4z}\\right|}\\]\\[q_3 = -\\Bigl(\\pi - \\arccos\\bigl(\\text{temp}\\bigr)\\Bigr)\\]For the Second Joint Angle $(q_2)$\\[a_1 = p_y \\sin(q_1) - p_z \\cos(q_1)\\]\\[a_2 = p_x\\]\\[m_1 = b_{4z} \\sin(q_3)\\]\\[m_2 = b_{3z} + b_{4z} \\cos(q_3)\\]\\[q_2 = \\arctan\\!\\left(\\frac{m_1\\, a_1 + m_2\\, a_2}{m_1\\, a_2 - m_2\\, a_1}\\right)\\]Body Orientation Adjustment is done using the following transformationsRoll rotation matrix (for roll angle $\\alpha$)\\[R\\_{\\text{roll}} =\\begin{pmatrix}1 &amp;amp; 0 &amp;amp; 0 \\\\0 &amp;amp; \\cos\\alpha &amp;amp; -\\sin\\alpha \\\\0 &amp;amp; \\sin\\alpha &amp;amp; \\cos\\alpha\\end{pmatrix}\\]Pitch rotation matrix (for pitch angle $\\beta$)\\[R\\_{\\text{pitch}} =\\begin{pmatrix}\\cos\\beta &amp;amp; 0 &amp;amp; -\\sin\\beta \\\\0 &amp;amp; 1 &amp;amp; 0 \\\\\\sin\\beta &amp;amp; 0 &amp;amp; \\cos\\beta\\end{pmatrix}\\]Combined rotation to adjust the foot position vector $\\mathbf{p}$\\[\\mathbf{p}_{\\text{adjusted}} = R_{\\text{pitch}} \\, R\\_{\\text{roll}} \\, \\mathbf{p}\\]For your reference here is what each of these variables mean:$p_x,p_y,p_z$ are the Cartesian coordinates of the foot’s target position in the robot’s coordinate system.  $p_x$ is the position along the forward-backward direction.  $p_y$ is the lateral position (side-to-side).  $p_z$ is the vertical position (height).$l_1,l_2,l_3$ are the lengths of the segments (links) in the leg.  $l_1$ corresponds to the distance from the hip to the first joint.  $l_2$ corresponds to the second segment (e.g., thigh length).  $l_3$ corresponds to the third segment (e.g., shank length).$L$ is an auxiliary variable defined as:\\[L =\\sqrt{p_y^2+p_z^2-l_1^2}\\]It helps in computing the first joint angle by combining the lateral and vertical components.  $q_1$ The first joint angle, computed using the positions $p_x$ and $p_y$ along with the link length $l_1$. It primarily handles the orientation of the leg in the vertical plane.  $b_{3z}$ and $b_{4z}$ These represent the effective lengths (or offsets) associated with the second and third segments of the leg. In many implementations, they are set as $-l_2$ and $-l_3$The final actions $a_t$ that are passed on to the PD controller can now be computed using the following equation:\\[a_t = \\alpha .a_t^P + a_t^F\\]Where $\\alpha$ is the weight given to the joint actions from the policy.ResultsHere is a compilation video of the FTG based locomotion policy being implemented, this was just a quick test of the method for a richer and complete implementation have a look at my post on Locomotion with Weighted Belief in Exteroception where FTG is reused."
  },
  {
    "title": "Proprioceptive Locomotion in Unstructured Environments",
    "url": "/posts/Loco-DRL/",
    "categories": "Projects, Quadrupeds",
    "tags": "reinforcement learning, sim2real, quadruped",
    "date": "2023-05-08 13:13:20 +0530",
    "snippet": "The following post goes over and outlines a few methods for training a blind locomotion policy for a quadruped robot, most of the work shown here has been done during my time at the CNRS-AIST JRL (Joint Robotics Laboratory), IRL, Japan for my undergraduate thesis under the supervision of Dr. Mitsuharu Morisawa with support from Rohan Singh.  Note: Detailed results and video clips can be found in the Results section below.The following GitHub Repo can be used to replicate the results from the videos show in this blog post.This work is a continuation of my previous post that can be found here.Let’s establish a few priors and baselines so that we can gradually build on top of them to achieve what can be considered a robust proprioceptive baseline.Base PolicyThe simplest policy can be designed by passing the current state of the robot $S_t$ — which consists of joint velocities, joint positions, roll, pitch, base angular velocity, and user commands — along with the actions from the previous timestep $a_{t-1}$, into a Multi-Layer Perceptron (MLP) network. The MLP outputs actions that are then used to compute the joint angles.Base Policy with Observation HistoryWhile the base policy with only the previous timestep’s actions is a good starting point, it works well only when the quadruped is walking on a flat surface. Introducing even the smallest obstacles can cause the robot to stumble and fall. This shouldn’t come as a surprise since the policy has no awareness of what occurred outside the current timestep. Reacting to external disturbances based solely on a single timestep is difficult, so the next logical upgrade is to provide the policy with a history of past states to improve its ability to handle disturbances.This can be achieved in various ways. For example, the MLP can be replaced with an RNN-based approach like LSTM or GRU. However, since we are building things from the ground up, we’ll use a simpler method: concatenating the previous 4 states with the current state to create a total of 5 states within a sliding window.For this particular use case, I found empirically that performance improves as the state window increases from 0 to 4. Beyond this, the gains become insignificant, so I limited the window to 4 past states. Your mileage may vary, so feel free to experiment with the window size. If you want to consider much longer horizons, such as 50 or 100 past states, running these states through a 1D CNN layer before feeding them into the base policy can help avoid creating an excessively large input layer.Effects of Observation HistoryAt this point, you might be curious (and perhaps a little impatient) to test whether adding just 4 past states actually makes a difference. Here’s a plot illustrating the impact of observation history:Leaving aside the cringe-worthy names in the plot legend and using the labels at the bottom, it’s clear that the agent reaches the full episode length (i.e., the agent doesn’t “die”) much sooner when provided with observation history. Additionally, the reward collected by the agent is higher when observation history is included.It’s important to note that while the difference between the two architectures appears to narrow as the number of steps increases, their performances do not completely merge or match. (evident from the steady state difference in their rewards)To give a more concrete idea of how all this actually performs on a real robot, here is a quick test of the Base Policy with Observation History compared against the default MPC controller for the Unitree Go1 Robot.Note that the default controller experiment had to be cut short due to the fact that the constant trotting by the MPC was causing the robot to swerve to the sides and would have likely resulted in a fall if the experiment continued.Here is another quick outdoor run using the default MPC controller and the Base Policy with Observation History.Do note that the default MPC controller couldn’t even go over the initial ledge. While the RL controller was able to do it somtimes, you can clearly see the shotcomings of both the controllers. Now lets take a look at the next possible upgrade to improve our RL controller.Privileged InformationPrivileged Information refers to the set of observations that are typically not available in the real world but can easily be obtained inside a physics engine/simulation. Some of this information might be relavent and can improve the performance of the base policy significantly if provided.For example, it might be of use to give the friction cofficient of the surface the robot is currently walking on as an input to the policy but in real life its hard to retrofit the robot with sensors to measure the friction.A small note on the nomeclature, a policy that is directly fed in priviliged information is typically refered to as an oracle since it symbolizes the best case performance a policy can achieve in an ideal sitatuion where any information needed can be supplied, in some cases such an oracle is also refered to as a teacher since its used to teach other policies. TLDR: Base Policy with Privileiged information is sometimes also called as a teacher or an oracle.At this point you might be wondering what is the point of discussing about such information that is not easy to read or measure in the real world since our final goal is to deploy policies on a real robot. Well reader, while its not easy to measure these parameters in the real world it doesn’t stop us from indirectly infereing these values by other means.For example even in humans, while we cannot in many cases look at the surface and guess the friction we can definetly get a feel of the friction the movement we start walking over it, this could be due to the difference in effort required to move our leg or the rate at which our leg moves for a given effort and so on… therefore we can indirectly feel the friction via our senses.Similarly, in this case we are interested in certain priviliged information metrics that while cannot be directly measured can be indirectly infered via other indirect metrics such as joint torques, joint velocities and so on via proprioception.Effects of Privileged InformationBefore we dive into all the different ways in which privileged information can be indirectly obtained and infered, lets take a look at the ideal case where we have all the priviledged information we want and compare such a policies performance with the base policy that uses only observation history and a base policy without observation history.Let’s try to spend some time to understand the plots in the above image, the orange line represents the base policy with only observation history whereas the gray plot represents the base policy with priviliged information and observation history.As you can notice from the plot on terrain level, the priviliged information policy reaches terrains with higher difficulty level faster when compared to the policy that just uses observation history. The plot of mean reward is a clear indicator that while both the policies might reach the max episode length, the priviliged information based policy clearly scores a higher reward when comapred to the policy with only observation history.Another interesting point to keep in mind is that the episode length plot has some intial ups and downs where at one point the policy with only observation history seems to be surviving for longer when compared to the policy with privileged information and if this seems counter intutive, keep in mind that the policy with the privilieged information is also progressing towards much more difficult terrain as can be infered from the terrain level plot and this could result in its episode length going slightly lower.Transfer Learning Methods to use Priviliged InformationLet us now shift our focus towards how we can infer priviliged information indirectly so that they can be used during real world deployments. If implemented properly we are looking for a performance that is better than the base policy with just observation history but is lower than the performance of the oracle/teacher. (Take a look at the reward plot from the picture above - we want to basically introduce a new method that will lie between the lines)Almost all the methods that try to infer privileged information try to derive this either explicitly or implicitly from the observation history. And likewise most methods that have a training component in simulation try to build an oracle/teacher and use a transfer learning/knowledge distillation framework.Teacher - Student Architecture  Note that while the network architeture is based on the original paper, the observations being passed have been tailored to our current use case.The first method we will take a look at is originally from a paper called learning by cheating, and is quite commonly referred to as the teacher-student transfer learning framework.This method consists of two-phases, the first phase involves the training of an oracle/teacher (typically done in simulation) with all the ground truth/privileged information given to the base policy as part of its observations.The second phase consists of freezing the trained teacher policy and training a seperate student network to mimic the outputs of the teacher network through regression without the privilieged information being passed in as observations.The important distinction to note here is the fact that the teacher and the student are two different networks with different sizes and the teacher cannot be copied or reused as a warmstarter for the student since the number of inputs for both (aka. their observations) are different.Rapid Motor Adaptation  Note that while the network architeture is based on the original paper, the observations being passed have been tailored to our current use case.The second method we are going to take a look at tries to take a slightly different approach to essential obtain the same results(infer priviliged information) but with a few advantages and perks.This approach is from a paper titled “Rapid Motor Adaptation” where they essentially propose an architeture that will enable us to reuse the bases policy or the teacher network in the second phase of the training instead of starting with a fresh student network with random weights.This is accomplished by passing the priviliged information as latent information instead of their raw values during phase one of the training with the help of an encoder called the priviliged information encoder in the figure.During phase two training, a new network called the observation history encoder is trained to mimic the output of the privliged information encoder despite only being passed the observation history and the output of this network is used to replace the latent information that was being sent during phase one by the privilged information encoder. This enables us to reuse the teacher/base policy from phase one in phase two as well.One advantage of this method is the fact that since the privileged information is being compressed into a latent vector, more amount of privilieged information can be input to the encoders without the need for increasing the size of the network/input layers when compared to passing this data as raw values.Asym. Actor CriticOur final method is called Asymmetrical Actor Critic that was originally published in “Asymmetric actor critic for image-based robot learning”. This method has several advantages and one of the most significant one is the fact that it has only a single phase of training involved.This is accomplished by passing the privilged information either in a latent state via an encoder or directly as raw inputs to the critic network, the idea behind this is the fact that the critic is used to compute the value function for state/action transitions and the more information the critic has the better it can assign value functions and therefore the better a critic can judge or help guide the actor, the better the actor performs and learns. While this method might seem like a solid replacement for the above two and a clear winner, it has a few pratical issues and perks that one should be aware of, one of them is the fact that this is a very indirect method of latent information transfer with no visibilty of the transfer to inspect or verify. Another is the fact that asym. actor-critic arch is prone to instability in certain scenarios which can lead to sudden collapses in the loss and/or value functions.ResultsIn the final figure you can see that we have accomplished our objecteing sive of finding a method that would perform better than the observation history base line and be below the therotical limit of performace possible (i.e the oracle).Here is a video compilation of the above policy architeture trained and deployed on a Unitree Go1. If you wish to try out the pre-trained policy on your own Go1 robot, please refer to the GitHub Repo associated with this post.While the training code for this paticular implementation cannot be shared at this point in time, the deployment code is a good starting point to understand the observations and the network arch being used, this combined with other resouces linked below will get you started training your own policy in no time. Furthermore, I will also provide links and discuss in more detail about sim2real for first time deplyoments and some more tips and tricks to get a policy sucessfully transfered from sim to your robot.Here is a video version of the above post as part of my JRL Seminar Talks"
  },
  {
    "title": "Deep Reinforcement Learning - Part 4 - Monte Carlo, Temporal Difference &amp; Bootstrapping Methods",
    "url": "/posts/DRL-4/",
    "categories": "Resources, Deep Reinforcement Learning",
    "tags": "mdp, optimal value, bellman",
    "date": "2023-01-23 13:13:20 +0530",
    "snippet": "This post is part of the Deep Reinforcement Learning Blog Series.  For a detailed introduction about the series, the curicullum, sources and references used, please check Part 0 - Getting Started.Structure of the blog  Part 0 - Getting Started  Part 1 - Multi-Arm Bandits  Part 2 - Finite MDP | Prerequisites: python, gymnasium, numpy  Part 3 - Dynamic Programming | Prerequisites: python, gymnasium, numpy  Part 4 - Monte Carlo, Temporal Difference &amp;amp; Bootstrapping Methods | Prerequisites: Python, gymnasium, numpy  Part 5 - Deep SARSA and Q-Learning | Prerequisites: python, gymnasium, numpy, torch  Part 6 - REINFORCE, AC, DDPG, TD3, SAC  Part 7 - A2C, PPO, TRPO, GAE, A3C  TBA (HER, PER, Distillation)Monte Carlo MethodsMC methods improvise over DP methods as they can be used in cases where we do not have a model of the environment. They do this by learning from episodes of experience. Therefore one caviat of MC methods is that they do not work on continous MDPs and learn only from complete episodes (Episodes must terminate).Monte Carlo PredictionWe begin by considering Monte Carlo methods for learning the state-value function for a given policy. Recall that the value of a state is the expected return—expected cumulative future discounted reward—starting from that state. An obvious way to estimate it from experience, then, is simply to average the returns observed after visits to that state. As more returns are observed, the average should converge to the expected value. This idea underlies all Monte Carlo methods.On-Policy MC PredictionWe have a small variation in the fact that we can consider each state only the first time it has been visited while estimating the mean return or we can account for multiple visits to the same state(if any) and the follwoing pseudocodes illustrate both the variations The every-visit version can be implemented by removing the “Unless $S_t$ appears in $S_0$, $S_1$, … $S_{t-1}$” line.Incremental UpdatesA more computationally efficient method would be to calculate the mean incrementally as follows:  Update $V(s)$ incrementally after episode $S_1,A_1,R_2,….,S_T$  For each state $S_t$ with return $G_t$\\[N(S_t) \\leftarrow  N(S_t) + 1\\]\\[V(S_t) \\leftarrow  V(S_t) + \\dfrac{1}{N(S_t)}(G_t-V(S_t))\\]For non-stationary problems, it can be useful to track a running mean (forgets old epiosdes and gives more weight to recent experiences).\\[V(S_t) \\leftarrow  V(S_t) + \\alpha(G_t-V(S_t))\\]Off-Policy MC PredictionAlmost all off-policy methods utilize importance sampling, a general technique for estimating expected values under one distribution given samples from another. We apply importance sampling to off-policy learning by weighting returns according to the relative probability of their trajectories occurring under the target and behavior policies, called the importance-sampling ratio. Off-Policy Monte Carlo Prediction can be implemented via two variations of importance sampling which are discussed below.Ordinary Importance SamplingFor evaluating a terget policy $\\pi(a|s)$ to compute $v_\\pi(s)$ or $q_\\pi(s,a)$ while following a behaviour policy $\\mu(a|s)$.Given, \\(\\{S_1,A_1,R_2,...,S_T\\} \\sim \\mu\\)We can weight returns $G_t$ according to similarity between the two policies. By multiplying the importance sampling corrections along the whole episode we get:\\[G_t^{\\pi/\\mu} = \\dfrac{\\pi(A_t|S_t)\\pi(A_{t+1}|S_{t+1})...\\pi(A_{T}|S_{T})}{\\mu(A_{t}|S_{t})\\mu(A_{t+1}|S_{t+1})...\\mu(A_{T}|S_{T})}G_t\\]We can then update the state value towards the corrected return like this\\[V(S_t) \\leftarrow  V(S_t) + \\alpha(G_t^{\\pi/\\mu}-V(S_t))\\]Note that we cannot use this if $\\mu$ is zero when $\\pi$ is non-zero, also that importance sampling can increase variance.Weighted Importance SamplingThe derivation of the Weighted Importance Sampling equations has been left-out for the time being.Ordinary importance sampling is unbiased whereas weighted importance sampling is biased (though the bias convergesasymptotically to zero). On the other hand, the variance of ordinary importance sampling is in general unbounded because the variance of the ratios can be unbounded, whereas in the weighted estimator the largest weight on any single return is one. In fact, assuming bounded returns, the variance of the weighted importance-sampling estimator converges to zero even if the variance of the ratios themselves is infinite (Precup, Sutton, andDasgupta 2001). In practice, the weighted estimator usually has dramatically lower variance and is strongly preferred.Monte Carlo ControlWhile it might seem straight forward to implement MC Methods in GPI by plugging MC Prediction for policy evaluation and using greedy policy improvement to complete the cycle, there is one key problem that needs to be addressed.Greedy policy improvement over $V(s)$ requires knowledge about the MDP\\(\\pi&#39;(s) = \\mathtt{argmax}_{a\\in A}  r(a|s) + p(s&#39;|s,a)V(s&#39;)\\)To remain model free we can instead switch to action value functions which will not require prior details about the MDP\\[\\pi&#39;(s) = \\mathtt{argmax}_{a\\in A} Q(s,a)\\]While this solves the issue of knowing the model MDP, we now have a deterministic policy and we will never be able to collect experiences of alternative actions and therefore might miss out on exploration altogether.This can be solved in the following ways:      Exploring Starts: Every state-action pair has a non-zero probability of being selected as the starting pair, this ensures sufficient exploration but in reality, this might not always be possible.        $\\epsilon-$ soft policies: A small probability to explore every time an action is to be choosen.        Off-Policy: Use a different policy to collect experience than the one target policy being improved.  On-Policy MC ControlExploring StartsThe pseudocode for exploring starts can be found below:On-Policy First Visit MC ControlThe pseudocode for On-Policy First Visit MC Control can be found below:The python implementation for the following can be found below:This code is part of my collection of RL algorithms, that can be found in my GitHub repo drl-algorithms.import gymnasium as gymimport numpy as npenv = gym.make(&#39;FrozenLake-v1&#39;, desc=None, map_name=&quot;8x8&quot;,               is_slippery=False, render_mode=&quot;rgb_array&quot;)action_values = np.zeros((env.observation_space.n,env.action_space.n))def policy(state,epsilon=0.2):    if np.random.random()&amp;lt;epsilon:        return np.random.choice(env.action_space.n)    else:        action_value = action_values[state]        return np.random.choice(np.flatnonzero(action_value == action_value.max()))def on_policy_monte_carlo(policy,action_values,episodes,gamma=0.99,epsilon=0.2):    sa_returns = {}    for episode in range(1, episodes+1):        state,info = env.reset()        done = False        transitions = []        while not done:            action = policy(state=state,epsilon=epsilon)            next_state,reward,terminated,truncated,info = env.step(action=action)            done = terminated or truncated            transitions.append([state,action,reward])            state = next_state        G = 0        for state_t,action_t,reward_t in reversed(transitions):            G = reward_t + gamma*G            if not (state_t,action_t) in sa_returns:                sa_returns[(state_t,action_t)] = []                sa_returns[(state_t,action_t)].append(G)                action_values[state_t,action_t] = np.mean(sa_returns[(state_t,action_t)])    env.close()on_policy_monte_carlo(policy=policy,action_values=action_values,episodes=10000)env = gym.make(&#39;FrozenLake-v1&#39;, desc=None, map_name=&quot;8x8&quot;,               is_slippery=False, render_mode=&quot;human&quot;)observation,info = env.reset()terminated = Falsewhile not terminated:    action = policy(observation,epsilon=0)    observation, reward, terminated, truncated, info = env.step(action=action)env.close()On-Policy Every Visit MC ControlOn-Policy Every Visit MC Control can be implemented by making a small change to the inner loop of the above code for the first visit version as follows:This code is part of my collection of RL algorithms, that can be found in my GitHub repo drl-algorithms.def on_policy_monte_carlo(policy,action_values,episodes,gamma=0.99,epsilon=0.2):    sa_returns = {}    for episode in range(1, episodes+1):        state,info = env.reset()        done = False        transitions = []        while not done:            action = policy(state=state,epsilon=epsilon)            next_state,reward,terminated,truncated,info = env.step(action=action)            done = terminated or truncated            transitions.append([state,action,reward])            state = next_state        G = 0        for state_t,action_t,reward_t in reversed(transitions):            G = reward_t + gamma*G            if not (state_t,action_t) in sa_returns:                sa_returns[(state_t,action_t)] = []            sa_returns[(state_t,action_t)].append(G)            action_values[state_t,action_t] = np.mean(sa_returns[(state_t,action_t)])    env.close()On-Policy Every Visit Constant Alpha MC ControlThe constant alpha version is based on the idea of using a running mean instead of using a normal return to deal with non-stationary problems.The major change being the following equation: \\(Q(S_t|A_t) \\leftarrow  Q(S_t|A_t) + \\alpha(G_t-Q(S_t|A_t))\\)The python implementation can be found below:This code is part of my collection of RL algorithms, that can be found in my GitHub repo drl-algorithms.import gymnasium as gymimport numpy as npenv = gym.make(&#39;FrozenLake-v1&#39;, desc=None, map_name=&quot;8x8&quot;,               is_slippery=False, render_mode=&quot;rgb_array&quot;)action_values = np.zeros((env.observation_space.n,env.action_space.n))def policy(state,epsilon=0.2):    if np.random.random()&amp;lt;epsilon:        return np.random.choice(env.action_space.n)    else:        action_value = action_values[state]        return np.random.choice(np.flatnonzero(action_value == action_value.max()))def constant_alpha_monte_carlo(policy,action_values,episodes,gamma=0.99,epsilon=0.2,alpha=0.2):    for episode in range(1, episodes+1):        state,info = env.reset()        done = False        transitions = []        while not done:            action = policy(state=state,epsilon=epsilon)            next_state,reward,terminated,truncated,info = env.step(action=action)            done = terminated or truncated            transitions.append([state,action,reward])            state = next_state        G = 0        for state_t,action_t,reward_t in reversed(transitions):            G = reward_t + gamma*G            old_value = action_values[state_t,action_t]            action_values[state_t,action_t] += alpha*(G-old_value)    env.close()constant_alpha_monte_carlo(policy=policy,action_values=action_values,episodes=20000)env = gym.make(&#39;FrozenLake-v1&#39;, desc=None, map_name=&quot;8x8&quot;,               is_slippery=False, render_mode=&quot;human&quot;)observation,info = env.reset()terminated = Falsewhile not terminated:    action = policy(observation,epsilon=0)    observation, reward, terminated, truncated, info = env.step(action=action)env.close()Off-Policy MC ControlIn Off-Policy methods, the policy used to generate behavior, called the behavior policy, may in fact be unrelated to the policy that is evaluated and improved, called the target policy. An advantage of this separation is that the target policy may be deterministic (e.g., greedy), while the behavior policy can continue to sample all possible actions.Off-policy Monte Carlo control methods use one of the techniques presented in the preceding two sections. They follow the behavior policy while learning about and improving the target policy. These techniques require that the behavior policy has a nonzero probability of selecting all actions that might be selected by the target policy (coverage). To explore all possibilities, we require that the behavior policy be soft (i.e., that it select all actions in all states with nonzero probability).The python implementation of the above can be found here:This code is part of my collection of RL algorithms, that can be found in my GitHub repo drl-algorithms.import gymnasium as gymimport numpy as npenv = gym.make(&#39;FrozenLake-v1&#39;, desc=None, map_name=&quot;8x8&quot;,                is_slippery=False, render_mode=&quot;rgb_array&quot;)action_values = np.zeros((env.observation_space.n,env.action_space.n))def target_policy(state):    action_value = action_values[state]    return np.random.choice(np.flatnonzero(action_value == action_value.max()))def exploratory_policy(state,epsilon=0.2):    if np.random.random()&amp;lt;epsilon:        return np.random.choice(env.action_space.n)    else:        action_value = action_values[state]        return np.random.choice(np.flatnonzero(action_value == action_value.max()))def off_policy_monte_carlo(action_values,target_policy,exploratory_ploicy,episodes,gamma=0.99,epsilon=0.2):    counter_sa_values = np.zeros((env.observation_space.n,env.action_space.n))    for episode in range(1,episodes+1):        state,_ = env.reset()        done = False        transitions = []         while not done:            action = exploratory_ploicy(state=state,epsilon=epsilon)            next_state,reward,terminated,truncated,_ = env.step(action=action)            done = terminated or truncated            transitions.append([state,action,reward])            state = next_state                G = 0         W = 1             for state_t,action_t,reward_t in reversed(transitions):            G = reward_t + gamma*G            counter_sa_values[state_t,action_t] += W            old_value = action_values[state_t,action_t]            action_values[state_t,action_t] += (W/counter_sa_values[state_t,action_t])* (G - old_value)            if action_t != target_policy(state_t):                break            W = W*(1/(1-epsilon + (epsilon/4)))    env.close()off_policy_monte_carlo(action_values=action_values,target_policy=target_policy,exploratory_ploicy=exploratory_policy,episodes=5000)env = gym.make(&#39;FrozenLake-v1&#39;, desc=None, map_name=&quot;8x8&quot;,               is_slippery=False, render_mode=&quot;human&quot;)observation,_ = env.reset()terminated = Falsewhile not terminated:    action = target_policy(observation)    observation, reward, terminated, truncated, info = env.step(action=action)env.close()Temporal Difference MethodsTemporal Difference methods improvise over MC methods by learning from incomplete episodes of experience using bootstrapping.Temporal Difference PredictionThe simplest temporal-difference learning algorithm TD(0) works as follows:The $V(S_t)$ can be updated using the actual return $G_t$\\[V(S_t) \\leftarrow V(S_t) + \\alpha(G_t - V(S_t))\\]We can replace the actual return $G_t$ with the estimated return $R_{t+1} + \\gamma V(S_{t+1})$ as follows\\[V(S_t) \\leftarrow V(S_t) + \\alpha(R_{t+1} + \\gamma V(S_{t+1}) - V(S_t))\\]Where,$R_{t+1} + \\gamma V(S_{t+1})$ is called the TD target$\\delta_t =  R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) $ is called the TD errorOn-Policy TD PredictionOn-Policy TD prediction can be implemented as explained belowOff-Policy TD PredictionUsing Importance Sampling for Off-Policy Learning, we can implement TD Prediction as follows:Use TD targets generated from $\\mu$ to evaluate $\\pi$. We can weight the TD target $(R + \\gamma V(S’))$ by the importance sampling ratio. Note that we only need to correct a single instane of the prediction unlike MC methods where the whole episde has to be corrected.\\[V(S_t) \\leftarrow V(S_t) + \\alpha \\left( \\dfrac{\\pi(A_t|S_t)}{\\mu(A_t|S_t)}(R_{t+1} + \\gamma V(S_{t+1})) - V(S_t) \\right)\\]Note that the variance is much lower than MC importance samplingTemporal Difference ControlThe basic stratergy for using TD methods for control is to plug them into the GPI framework for policy evaluation by learning action values to remain model-free.The general update rule for action value estimation can be written as:\\[Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha [R_{t+1} + \\gamma Q(S_{t+1},A_{t+1}) - Q(S_t,A_t)]\\]On-Policy TD Control (SARSA)The On-Policy TD Control method is also referred to as SARSA as it uses the quintuple of events $(S_t,A_t,R_{t+1},S_{t+1},A_{t+1})$ for every update.The pseudocode for SARSA can be found belowThe python implementation of the above can be found here:This code is part of my collection of RL algorithms, that can be found in my GitHub repo drl-algorithms.import gymnasium as gymimport numpy as npenv = gym.make(&#39;FrozenLake-v1&#39;, desc=None, map_name=&quot;8x8&quot;,               is_slippery=False, render_mode=&quot;rgb_array&quot;)action_values = np.zeros((env.observation_space.n, env.action_space.n))def policy(state, epsilon=0.2):    if np.random.random() &amp;lt; epsilon:        return np.random.choice(env.action_space.n)    else:        action_value = action_values[state]        return np.random.choice(np.flatnonzero(action_value == action_value.max()))def sarsa(action_values, episodes, policy, alpha=0.1, gamma=0.99, epsilon=0.2):  # on-policy-td-learning    for episode in range(episodes+1):        state, _ = env.reset()        action = policy(state=state, epsilon=epsilon)        done = False        while not done:            next_state, reward, termination, truncation, _ = env.step(                action=action)            done = termination or truncation            next_action = policy(state=next_state, epsilon=epsilon)            action_value = action_values[state, action]            next_action_value = action_values[next_state, next_action]            action_values[state, action] = action_value + alpha * \\                (reward + gamma*next_action_value - action_value)            state = next_state            action = next_action    env.close()sarsa(action_values=action_values, episodes=1000, policy=policy)env = gym.make(&#39;FrozenLake-v1&#39;, desc=None, map_name=&quot;8x8&quot;,               is_slippery=False, render_mode=&quot;human&quot;)observation, _ = env.reset()terminated = Falsewhile not terminated:    action = policy(observation, epsilon=0)    observation, reward, terminated, truncated, info = env.step(action=action)env.close()Off-Policy TD Control (Q-Learning)Off-Policy TD Control is often referred to as Q-Learning and does not require importance sampling. For TD(0) based Q-Learning, since the action $A_t$ is already determined the  importance sampling ratio essentially becomes 1 and therefore can be ignored.The next action is chosen using a behaviour policy $A_{t+1} \\sim \\mu(.|S_t)$ which is $\\epsilon$-greedy w.r.t $Q(S,A)$. But we consider alternative successor action $a \\sim \\pi(.|S_t)$ using the target policy $\\pi$ which is greedy w.r.t $Q(S,A)$The update rule can be written down as follows:\\[Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha[R_{t+1} + \\gamma \\mathtt{max}_{a} Q(S_{t+1},a) - Q(S_t,A_t)]\\]The pseudocode for Q-Learning can be found below: The python implementation of the above can be found here:This code is part of my collection of RL algorithms, that can be found in my GitHub repo drl-algorithms.import gymnasium as gymimport numpy as npenv = gym.make(&#39;FrozenLake-v1&#39;, desc=None, map_name=&quot;8x8&quot;,               is_slippery=False, render_mode=&quot;rgb_array&quot;)action_values = np.zeros((env.observation_space.n, env.action_space.n))def target_policy(state):    action_value = action_values[state]    return np.random.choice(np.flatnonzero(action_value == action_value.max()))def exploratory_policy():    return np.random.randint(env.action_space.n)def q_learning(action_values, episodes, target_policy, exploratory_policy, alpha=0.1, gamma=0.99):  # on-policy-td-learning    for episode in range(episodes+1):        state, _ = env.reset()        done = False        while not done:            action = exploratory_policy()            next_state, reward, termination, truncation, _ = env.step(                action=action)            done = termination or truncation            next_action = target_policy(state=next_state)            action_value = action_values[state, action]            next_action_value = action_values[next_state, next_action]            action_values[state, action] = action_value + alpha * \\                (reward + gamma*next_action_value - action_value)            state = next_state    env.close()q_learning(action_values=action_values, episodes=5000, target_policy=target_policy, exploratory_policy=exploratory_policy, alpha=0.1, gamma=0.99)env = gym.make(&#39;FrozenLake-v1&#39;, desc=None, map_name=&quot;8x8&quot;,               is_slippery=False, render_mode=&quot;human&quot;)observation, _ = env.reset()terminated = Falsewhile not terminated:    action = target_policy(observation)    observation, reward, terminated, truncated, info = env.step(action=action)env.close()Expected SARSAConsider the learning algorithm that is just like Q-learning except that instead of the maximum over next state–action pairs it uses the expected value, taking into account how likely each action is under the current policy. That is, consider the algorithm with the update rule as follows:\\[Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha[R_{t+1} + \\gamma \\mathbf{E}_\\pi[Q(S_{t+1},A_{t+1} | S_{t+1})] - Q(S_t,A_t) ]\\]which can be written as\\[Q(S_t,A_t) = Q(S_t,A_t) + \\alpha[R_{t+1} + \\gamma \\Sigma_a\\pi(a|S_{t+1})Q(S_{t+1},a) - Q(S_t,A_t) ]\\]Expected Sarsa is more complex computationally than Sarsa but, in return, it eliminates the variance due to the random selection of At+1. Given the same amount of experience we might expect it to perform slightly better than Sarss.BootstrappingWhile TD(0) methods take one step and estimate the return, MC methods wait till the end of the episode to calculate the return. While both these methods might appear to be very different they can unified by using TD methods to look n-steps into the future, as shown by the image below."
  },
  {
    "title": "Deep Reinforcement Learning - Part 3 - Dynamic Programming",
    "url": "/posts/DRL-3/",
    "categories": "Resources, Deep Reinforcement Learning",
    "tags": "mdp, optimal value, bellman",
    "date": "2023-01-20 13:13:20 +0530",
    "snippet": "This post is part of the Deep Reinforcement Learning Blog Series.  For a detailed introduction about the series, the curicullum, sources and references used, please check Part 0 - Getting Started.Structure of the blog  Part 0 - Getting Started  Part 1 - Multi-Arm Bandits  Part 2 - Finite MDP | Prerequisites: python, gymnasium, numpy  Part 3 - Dynamic Programming | Prerequisites: python, gymnasium, numpy  Part 4 - Monte Carlo, Temporal Difference &amp;amp; Bootstrapping Methods | Prerequisites: Python, gymnasium, numpy  Part 5 - Deep SARSA and Q-Learning | Prerequisites: python, gymnasium, numpy, torch  Part 6 - REINFORCE, AC, DDPG, TD3, SAC  Part 7 - A2C, PPO, TRPO, GAE, A3C  TBA (HER, PER, Distillation)Dynamic ProgrammingThe term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as a Markov decision process (MDP). Classical DP algorithms are of limited utility in reinforcement learning both because of their assumption of a perfect model and because of their great computational expense, but they are still important theoretically. DP provides an essential foundation for the understanding of the Approx. methods presented in later parts. Infact, all of these methods can be viewed as attempts to achieve much the same effect as DP, only with less computation and without assuming a perfect model of the environment.Policy EvaluationFirst we consider how to compute the state-value function $v_\\pi$ for an arbitrary policy $\\pi$. This is called policy evaluation in the DP literature. We also refer to it as the prediction problem.Recall that,\\[v_\\pi(s) = \\sum_{a}\\pi(a\\mid s)\\sum_{s&#39;,r} p(s&#39;,r\\mid a,s)[r+\\gamma v_\\pi(s&#39;)]\\]We can evaluate a given policy $\\pi$ by iterativly applying the bellman expectation backup as an update rule until the value function $v(s)$ converges to the $v_\\pi(s)$.Policy ImprovementWe can then improve a given policy by acting greedily with respect to the given value function for the policy $v_\\pi(s)$.The new policy $\\pi’$ is better than or equal to the old policy $\\pi$. Therefore, $\\pi’ \\ge \\pi$.If the improvement stops\\[q_\\pi(s,\\pi&#39;(s)) = \\mathtt{max}_{a \\in A} \\  q_\\pi(s,a) \\ =   q_\\pi(s,\\pi(s)) = v_\\pi(s)\\]Therefore,\\[v_\\pi(s) =  \\mathtt{max}_{a \\in A}  q_\\pi(s,a)\\]which is the bellman optimality equation and $v_\\pi(s) = v_\\star(s)$.Policy IterationPolicy Iteration combines the evaluation and improvement steps into a single algorithm where a random policy is taken, its evaluated and then improved upon and the resulting policy is again evaluated and then improved upon and so on until the policy finally converges and becomes the optimal policy.Policy Iteration is also refered to as the control problem in DP litrature as opposed to the prediction problem that is policy evaluation.The algorithm can be summaried as follows:The $\\Delta$ in the above code is used to determine the accuracy of the estimation and can be a value close to 0.The python code for Policy Iteration is as follows:This code is part of my collection of RL algorithms, that can be found in my GitHub repo drl-algorithms.import gymnasium as gymimport numpy as npenv = gym.make(&#39;FrozenLake-v1&#39;, desc=None, map_name=&quot;8x8&quot;,               is_slippery=False, render_mode=&quot;human&quot;)observation, info = env.reset()terminated = False# state-action tablepolicy_probs = np.full((env.observation_space.n, env.action_space.n), 0.25)state_values = np.zeros(shape=(env.observation_space.n))def policy_iteration(policy_probs, state_values, theta=1e-6, gamma=0.99):    policy_stable = False    while policy_stable == False:        # policy eval        delta = 1        while delta &amp;gt; theta:            delta = 0            for state in range(env.observation_space.n):                old_value = state_values[state]                new_state_value = 0                for action in range(env.action_space.n):                    probablity, next_state, reward, info = env.P[state][action][0]                    new_state_value += policy_probs[state, action]*(                        reward + (gamma*state_values[next_state]))                state_values[state] = new_state_value                delta = max(delta, abs(old_value-state_values[state]))        # policy improvement        policy_stable = True        for state in range(env.observation_space.n):            old_action = policy(state=state)            action_values = np.zeros(env.action_space.n)            max_q = float(&#39;-inf&#39;)            for action in range(env.action_space.n):                probablity, next_state, reward, info = env.P[state][action][0]                action_values[action] = probablity * \\                    (reward + (gamma*state_values[next_state]))                if action_values[action] &amp;gt; max_q:                    max_q = action_values[action]                    action_probs = np.zeros(env.action_space.n)                    action_probs[action] = 1            policy_probs[state] = action_probs        # check termination condition and update policy_stable variable            if old_action != policy(state=state):                policy_stable = Falsedef policy(state):    return np.argmax(policy_probs[state])policy_iteration(policy_probs=policy_probs, state_values=state_values)print(&quot;Done&quot;)while not terminated:    action = policy(observation)    observation, reward, terminated, truncated, info = env.step(action=action)env.close()Another important point is the fact that we do not need to wait for the policy evaluation to converge to $v_\\pi$ before performing policy improvement. Therefore, a stopping condition can be introduced without effecting the performance.When the policy evaluation step is stopped after a single step, k = 1 we arrive at a special case of policy evaluation which is equivalent to another method called value iteration.Value IterationThe bellman optimality backup equation can be applied iteratively until convergence to arrive at the optimal value function $v_\\star(s)$. Unlike policy iteration the itermediate value functions do not correspond to any policy. The algorithm can be summaried as follows:The python code for Value Iteration is as follows:This code is part of my collection of RL algorithms, that can be found in my GitHub repo drl-algorithms.import gymnasium as gymimport numpy as npenv = gym.make(&#39;FrozenLake-v1&#39;, desc=None, map_name=&quot;8x8&quot;,               is_slippery=False, render_mode=&quot;human&quot;)observation, info = env.reset()terminated = False# state-action tablepolicy_probs = np.full((env.observation_space.n, env.action_space.n), 0.25)state_values = np.zeros(shape=(env.observation_space.n))def value_iteration(policy_probs, state_values, theta=1e-6, gamma=0.99):    delta = 1    while delta &amp;gt; theta:        delta = 0        for state in range(env.observation_space.n):            old_value = state_values[state]            action_values = np.zeros(env.action_space.n)            max_q = float(&#39;-inf&#39;)            for action in range(env.action_space.n):                probablity, next_state, reward, info = env.P[state][action][0]                action_values[action] = probablity * \\                    (reward + (gamma*state_values[next_state]))                if action_values[action] &amp;gt; max_q:                    max_q = action_values[action]                    action_probs = np.zeros(env.action_space.n)                    action_probs[action] = 1            state_values[state] = max_q            policy_probs[state] = action_probs            delta = max(delta, abs(old_value - state_values[state]))def policy(state):    return np.argmax(policy_probs[state])value_iteration(policy_probs=policy_probs, state_values=state_values)while not terminated:    action = policy(observation)    observation, reward, terminated, truncated, info = env.step(action=action)env.close()This code is part of my collection of RL algorithms, that can be found in my GitHub repo drl-algorithms.            Problem      Bellman Equation      Algorithm                  Prediction      Bellman Expectation Equation      Iterative Policy Evaluation              Control      Bellman Expectation Equation + Greedy Policy Improvement      Policy Iteration              Control      Bellman Optimality Equation      Value Iteration      Async. Dynamic ProgrammingAll the methods discussed till now were synchronous in nature and at each step of the iteration we used a loop to go over all the states. We can however also asynchronously backup states in any order and this will also lead to the same solution as long as all the states are selected atleast once. The added advantage is that this greatly reduces computational time and gives rise to methods like prioritised sweeping and others.Generalized Policy IterationWe use the term generalized policy iteration (GPI) to referto the general idea of letting policy-evaluation and policy improvement processes interact, independent of the granularity and other details of the two processes. Almost all reinforcement learning methods are well described as GPI. That is, all have identifiable policies and value functions, with the policy always being improved with respect to the value function and the value function always being driven toward the value function for the policy. If both the evaluation process and the improvement process stabilize, that is, no longer produce changes, then the value function and policy must be optimal. The value function stabilizes only when it is consistent with the current policy, and the policy stabilizes only when it is greedy with respect to the current value function. Thus, both processes stabilize only when a policy has been found that is greedy with respect to its own evaluation function. This implies that the Bellman optimality equation holds, and thus that the policy and the value function are optimal.Pros and Cons of Dynamic ProgrammingDP is sometimes thought to be of limited applicability because of the curse of dimensionality, the fact that the number of states often grows exponentially with the numberof state variables. Large state sets do create diculties, but these are inherent difficulties of the problem, not of DP as a solution method. In fact, DP is comparatively better suited to handling large state spaces than competing methods such as direct search and linear programming.But DP methods assume we know the dynamics of the environment which is one of the biggest limiting factors for their direct use in many cases. In the upcoming parts we will loot at methods that tackle this issue."
  },
  {
    "title": "Deep Reinforcement Learning - Part 0 - Getting Started",
    "url": "/posts/DRL-0/",
    "categories": "Resources, Deep Reinforcement Learning",
    "tags": "mdp, optimal value, bellman",
    "date": "2023-01-18 13:13:20 +0530",
    "snippet": "Getting into deep reinforcement learning is a tough but rewarding journey. The primary reason it is so difficult for beginners is due to the shear number of fields involved. Deep Reinforcement Learning for Robotics can be broken down into three almost seperate parts:  Deep Learning (Neural Networks)  Reinforcement Learning Algorithm  Choice of State Space, Action Space and Reward FunctionsHow To Get StartedThere are a finite number of ways to get started with DRL, but I believe that the right curriculum (learning xD) can make you go a long way.RL Agents pretty much wither and die when left in environments with sparse rewards. One of the easiest solution for both RL Agents and beginners trying to learn DRL is to have a dense reward function that introduces a lot of fun and rewards at every timestep in the journey.Target-AudiancePeople who have prior experience with python, numpy(preferably) and basic understanding of probability and statistics. Added benefit if you are comfortable with basic calculus.You can learn the maths and numpy on the go as and when it is required but a decent understanding of python datatypes, object-oriented-programming is expected.End-GoalThe end-goal for this curriculum is to equip you with the skills required to read through research papers and reimplement/modify them, understand opensource projects and ultimately help you get started with research in DRL.If this is not what your looking for then this probably isn’t the right curriculum for you.CurriculumThe curriculum laid out below is my opinon and it might or might not be the best way for you to get into DRL, so please use it accordingly.System SetupPlease create a virtualenv and switch to python 3.8 for the entire series. Linux(Ubuntu or any other distro) is the recommended OS, while some of the code and packages might work in windows, I will not be helping with any windows debugging.Phase One1. Get started with gymnasiumPrerequisites: pythonNote: If you are an existing user of gym please refer to the migration guide to gymnasium here.      Explore the structure of gymnasium-api. Render a couple of environments until your comfortable with the syntax and have a general idea of what is happening in the code.        Progress Check: Assignmnet-1 | Solution-1  2. Play around with rewards, states and actionsPrerequisites: python, gymnasium      Get started with a 3rd party RL Library (SB3, CleanRL, RLlib or any other implementation of your choice) and a robust RL algorithm like PPO and focus on changing the rewards, states and actions in the basic environments to solve them. Gain an intution of how the RL framework works.        For robotics in paticular, try the MuJoCo environments like Ant or check here and here for other available options (This is not an exhaustive list).        What happens when you use torque instead of position in the action space ? What happens when you given a combination of negative and postive rewards ? What are termination conditions ?        Believe it or not, you are already in a position to replicate a couple of basic DRL papers. Search for papers that are related to blind locomotion in quadrupeds or robotic manipulators (for example), you should be able to comfortably work with any paper that involve changes to only the state, action and rewards.        Try importing different robot models into MuJoCo or any other physics engine of your choice and getting them to work or alternativly use one of the above listed rl-envs. Here here a couple of papers that you can implement along with a link to my implementation, feel free to try it on your own first or tinker around with my code directly:          Fu, Z., Kumar, A., Malik, J., &amp;amp; Pathak, D. (2021). Minimizing Energy Consumption Leads to the Emergence of Gaits in Legged Robots. doi:10.48550/ARXIV.2111.01674              Franceschetti, A., Tosello, E., Castaman, N., &amp;amp; Ghidoni, S. (2020). Robotic Arm Control and Task Training through Deep Reinforcement Learning. doi:10.48550/ARXIV.2005.02632              Michel Aractingi, Pierre-Alexandre Léziart, Thomas Flayols, Julien Perez, Tomi Silander, et al.. Controlling the Solo12 Quadruped Robot with Deep Reinforcement Learning. 2022. ⟨hal-03761331⟩              Fang-I Hsiao, Cheng-Min Chiang, Alvin Hou, et al.. Reinforcement Learning Based Quadcopter Controller            You can even try ideas like curriculum learning, dynamic goal generation and other ideas that vary the difficult of the training as per the agents performance.        Progress Check: Assignmnet-2 | Solution-2  3. Learn Tabular Reinforcement Learning MethodsPrerequisites: python, gymnasium, numpyNOTE: You can continue to explore ideas and research papers from step 2 in parallel.      Learn about the basics/fundamentals of reinforcement learning mainly: K-Arm Bandits, MDP, Monte-Carlo Methods, Temporal Difference Methods, Bootstrapping        Refer to the Sources &amp;amp; References for links to external resources like video lectures.        Refer to the following sections of the blog series for code and theory:          Part 1 - Multi-Arm Bandits              Part 2 - Finite MDP              Part 3 - Dynamic Programming              Part 4 - Monte Carlo and Temporal Difference Methods            Solve some of the basic low dimenssional problems from the gym environments like toy-text problems        Progress Check: Assignmnet-3 | Solution-3  Phase Two4. Deep Learning FrameworkPrerequisites: python, numpy      Pick a library of your choice for learning Neural Networks, this guide will be based on PyTorch. (Other options like TensorFlow exist, pick whatever works best for you.)        Learn Deep Learning using PyTorch. In paticular try a couple of basic projects till your comfortable with the following ideas: Loss Functions, Activation Functions, PyTorch Syntax, MLP, CNN, RNN, LSTM, GAN, Autoencoders, Weight Initializations, Dropout, Optimizers.        Refer to the Sources &amp;amp; References for links to external resources for learning.        Do a couple of pure Deep-Learning projects like binary/multi-class classification, De-noising Images and so on..        Try going through some of the classic papers in DL that laid the foundation for modern DL. Here is a link to my collection of must read classics. Here are links to an external collection that is more exhaustive spinning-openai, awesome-deep-rl, awesome-deep-reinforcement-learning        Progress Check: Assignmnet-4 | Solution-4  5. Apply Deep Learning to RLPrerequisites: python, numpy, rl-library-of-your-choice      Make small changes to the exisiting neural networks from your prior projects in Section 2. Play around with rewards, states and actions which were based on 3rd party RL libraries.        Change the number of layers, the type of NNet, the activation functon, maybe add a CNN and take camera input in the state.        Upgrade projects you worked on earlier like quadruped/robotics arm by adding camera inputs to the state space or change the NNet type to RNNs or LSTMs and check how the performace of the agent changes.        Progress Check: Assignmnet-5 | Solution-5  6. Approx. Methods in RLPrerequisites: python, numpy, PyTorch      Learn Deep Q-Learning, Policy Gradient, Actor-Critic Methods and other algorithms and implement them.        Refer to the following sections of the blog series:          Part 5 - Deep SARSA and Q-Learning              Part 6 - REINFORCE, AC, DDPG, TD3, SAC              Part 7 - A2C, PPO, TRPO, GAE, A3C            You should now be able to implement a good number of research papers, explore ideas like HER, PER, World Models and other concepts.        Progress Check: Assignmnet-6 | Solution-6  Where Does This Blog Fit in ?Some drawbacks of the existing resources for DRL:      Most of them only focus on the theory or the code but not both. A majority of the courses that cover the theory in great detail do not have any coding components making it very difficult to implement any learning. The courses which are coding centric only focus on the code and skip most of the theory and give a vague intution about the proof or the derivation for the formulas used.        Many courses use their own custom environments for teaching (ahm ahm Coursera Specialization) while this can make learning/teaching easy. Some use jupyter notebooks for teaching, most if not all the RL libraries and opensource projects in the internet use argparse and write their code in modular file structures. Once you step outside the course sandbox it becomes very difficult to switch or even follow other projects.        A good majority of courses are topic specific aka they only teach something with limits scope or prespective in mind. For example, there are tons of Deep Learning courses but there usually isn’t a deep learning for reinforcement learning course. So, you end up learning a lot more than what is needed and the course usually might focus on things that are not really required for DRL.  The primary goal for this blog series is to bridge the gap between theory and code in Deep Reinforcement Learning.This blog isn’t a one stop solution and will not teach you DRL from start to finish, you will still need to learn a good portion of the curriculum from other resources. This blog is a sort of an extended cheatsheet for people to refer to when they are learning/implementing DRL via code. It contains a mix of theory and code that build on top of each other.Structure of the blog  Part 0 - Getting Started  Part 1 - Multi-Arm Bandits  Part 2 - Finite MDP | Prerequisites: python, gymnasium, numpy  Part 3 - Dynamic Programming | Prerequisites: python, gymnasium, numpy  Part 4 - Monte Carlo, Temporal Difference &amp;amp; Bootstrapping Methods | Prerequisites: Python, gymnasium, numpy  Part 5 - Deep SARSA and Q-Learning | Prerequisites: python, gymnasium, numpy, torch  Part 6 - REINFORCE, AC, DDPG, TD3, SAC  Part 7 - A2C, PPO, TRPO, GAE, A3C  TBA (HER, PER, Distillation)Sources &amp;amp; ReferencesThis section contains a collection of all the various sources for this blog series (in no paticular order):  Sutton, R. S., Barto, A. G. (2018 ). Reinforcement Learning: An Introduction. The MIT Press.  David Silver (2015). Lectures on Reinforcement Learning  Udemy Course Reinforcement Learning beginner to master - AI in Python  Udemy Course Modern Reinforcement Learning: Deep Q Learning in PyTorch  Chris G. Willcocks - Durham University Reinforcement Learning Lectures  (My repo) drl-algorithms  Pieter Abbeel Foundations of Deep RL  Weng, L. (2018, February 19). A (long) peek into reinforcement learning. Lil’Log  Aditya Chopra. (2022). Introduction to Concepts in Reinforcement LearningThis section contains a collection of various references which are required to learn DRL and have been mentioned in the curriculum but have not been covered in this blog series:  Udemy Course PyTorch for Deep Learning in 2023: Zero to Mastery  Udemy Course A deep understanding of deep learning (with Python intro)This section contains other references that I have not used in this blog series but are in general useful:  Coursera Reinforcement Learning Specialization  HuggingFace Deep Reinforcement Learning Course  Professor Emma Brunskill, Stanford University Stanford CS234: Reinforcement Learning | Winter 2019  DeepMind x UCL RL Lecture Series  RAIL CS285: Deep Reinforcement Learning Series UC Berkeley"
  },
  {
    "title": "Energy Minimization based Deep Reinforcement Learning Policies for Quadrupeds",
    "url": "/posts/Energy-DRL/",
    "categories": "Projects, Quadrupeds",
    "tags": "mdp, optimal value, bellman",
    "date": "2022-12-11 13:13:20 +0530",
    "snippet": "The following work has been done during my time at the CNRS-AIST JRL (Joint Robotics Laboratory), IRL, Japan for my undergraduate thesis under the supervision of Dr. Mitsuharu Morisawa with support from Rohan Singh.  Note: Detailed results and video clips can be found in the Results section below.ObjectiveThe primary objective of this work was to create a deep reinforcement learning based policy for quadruped locomotion with emphasis on minimal hand tuning for deployment and easy sim-to-real transfer which was to be used as a baseline policy in our future work.This has been accomplished using an energy minimization approach for the reward function along with other training specifics like curriculum learning.Hardware &amp;amp; SoftwareWith the core belief of working towards a controller that can be deployed in real-world settings, it is extremely crucial that the entire framework of both software and hardware be scale-able and economically viable. We therefore went ahead with a commercially available quadrupedplatform rather than creating our own quadrupedal platform that might make the results more difficult to verify and the solution equally harder to be deployed in a commercial scale. Similar decisions have been taken wherever crucial decisions had to be taken.AlienGoThe AlienGo quadrupedal platform (can be seen in the figure below) by Unitree Robotics first launched in 2018 was our choice for this research study as it strikes the perfect balance between economical cost, features and capabilities. Further, quadruped robots from Unitree Robotics have been one of the most common choice among research labs across the globe. Some select parameters of the robot are listed below:  Body size: 650x310x500mm (when standing)  Body weight: 23kg  Driving method: Servo Motor  Degree of Freedom: 12  Structure/placement design: Unique design (patented)  Body IMU: 1 unit  Foot force sensor: 4 units (1 per foot)  Depth sensor: 2 units  Self-position estimation camera: 1 unitMuJoCoMuJoCo has been our choice for simulation as it is a free and open source physics engine that is widely used in the industry for research and development in robotics, biomechanics, graphics and animation. Also, we have observed that it is able to simulate contact dynamics more accurately and was also faster in most of our use cases when compared to other available options. We have also used Ray an open-source compute framework for parallelization of our training.Training PlatformOur primary training platform is the GDEP Deep Learning BOX some select system parameters are mentioned below:  CPU: AMD Ryzen Threadripper PRO 5975WX  Num. Cores: 32  RAM: 128GB  GPU: Nvidia RTX A6000 x 2RL FrameworkState SpaceAs stated in Jie Tan et al. Sim-to-Real: Learning Agile Locomotion For Quadruped Robots the choice of state space has a direct impact on the sim to real transfer, we note that this can be primarily summarized as the fewer dimensions in the state space the easier it is to do a sim to real transfer as the noise and drift increase with an increase in the number of parameters being included in the state space. Many of the recent papers on quadruped locomotion therefore try to avoid using parameters that are noisy or tend to drift such as yaw from the IMU and force values from the foot sensors. While a few papers use methods like supervised learning and estimation techniques to counter the noise and drift in sensor data we decided to eliminate the use of such parameters all together as it didn’t result in any drastic change in the performance of learning policy. Our final state space has 230(46x5) dimensions and its breakdown is listed below:  $[\\omega_x,\\omega_y,\\omega_z]$ - root angular velocity in the local frame  $[\\theta]$ - joint angles  $[\\dot{\\theta}]$ - joint velocities  $[c]$ - binary foot contacts  $[v_x^g,v_y^g,\\omega_z^g]$ - goal velocity  $[a_{t-1}]$ - previous actions  $[s_0,s_1,s_2,s_3]$ - history of the previous four statesThe goal velocity consists of three components linear velocity in x and y axis along with angular velocity along the z axis, we have discarded the roll, pitch, yaw and linear velocities that are usually included in the state space for reasons mentioned above. Further, although aliengo has force sensors that can give the magnitude of the force we decided to use a threshold and use binary representation for foot contacts as there is significant noise and drift in the readings.Action SpaceAs stated in Michiel van de Panne et al. “Learning locomotion skills using DeepRL: does the choice of action space matter? ”, the choice of action space directly effects the learning speed and hence we went ahead with joint angles as the action space representation, further to strongly center all our gait from the neutral standing pose of the robot. The policy outputs are added around the joint position values of the neutral standing pose joint angles before being fed into a low-gain PD controller that outputs the final joint torque values. The final control scheme can be seen hereLearning AlgorithmGiven the continuous nature of the state and action space Approx methods are essential. We went ahead with Proximal Policy Optimization Algorithm that is based on the Actor-Critic Framework as it do not require extensive hyperparamter tuning and are in general is quite stable. We use a Multi Layered Perceptron architecture with 2 hidden layers of size 256 units each and ReLU activation to represent both the actor and critic networks.HyperparametersThe hyperparameters were taken from standard implementations which are typically the same across many of the papers. The values have been listed below:  Parallel Instances: 32  Minibatch size: 512  Evaluation freq: 25  Adam learning rate: 1e-4  Adam epsilon: 1e-5  Generalized advantage estimate discount: 0.95  Gamma: 0.99  Anneal rate for standard deviation: 1.0  Clipping parameter for PPO surrogate loss: 0.2  Epochs: 3  Max episode horizon: 400Reward FunctionThe goal for any reinforcement learning policy is to maximize the total reward/expected reward collected. While the state and action space definitely effect the learning rate and stability of the policy, the reward function defines the very nature of the learning policy. The wide variety of literature available on locomotion policies for quadrupeds while almost the same with respect tothe state and action space has diversity mostly due to the choice of the reward function.\\[\\begin{aligned}\\text{Total Reward} &amp;amp;= \\text{Energy Cost} + \\text{Survival Reward} + \\text{Goal Velocity Cost} \\\\\\text{Energy Cost} &amp;amp;= C_1\\tau\\omega \\\\\\text{Survival Reward} &amp;amp;= C_2|v_x^g| + C_3|v_y^g| + C_4|\\omega_z^g| \\\\\\text{Goal Velocity Cost} &amp;amp;= -C_2|v_x-v_x^g| - C_3|v_y-v_y^g| - C_4|\\omega_z - \\omega_z^g| \\\\\\end{aligned}\\]Where $C_1,C_2,C_3,C_4$ are constants that have to be picked.We believe that the primary reason reinforcement learning based policies generalize poorly is due to the excessive number of artificial costs added to the reward function for achieving locomotion. Inspired by Zipeng Fu et al. Minimizing Energy Consumption Leads to the Emergence of Gaits in Legged Robots, we base our reward function on the principle of energy minimization. Another added benefit of energy minimization based policy is the fact that different goal velocity commands result in different gaits. As explained in Christopher L. Vaughan et al. “Froude and the contribution of naval architecture to our understanding of bipedal locomotion.”, this is consistent with how animalsbehave and is because a particular gait is only energy efficient for a particular range of goal velocities.Curriculum LearningCurriculum learning is a method of training reinforcement learning (RL) agents in which the difficulty of tasks or environments is gradually increased over time. This approach is based on the idea that starting with simpler tasks and gradually increasing the complexity can help theagent learn more efficiently. The agent can focus on mastering basic skills needed to solve the initial tasks before attempting to tackle more complex ones. There are two ways in which curriculum learning is usually implemented. One is to use a pre-defined set of tasks or environments that are ordered by increasing difficulty. The agent is trained on these tasks in a specific order, with the difficulty of the tasks increasing as the agent progresses through the curriculum. Another approach is to use a dynamic curriculum, where the difficulty of the tasks is adjusted based on the agent’s performance. For instance, if the agent struggles with a particular task, the difficulty of that task may be reduced, while the difficulty of easier tasks may be increased to provide more challenge. We use Curriculum learning in a variety of ways to tackle a varity of issues as discussed below.Cost CurriculumThe high energy cost with low reward for smaller values of goal velocity make it extremely difficult for the agent to learn walking at low goal speeds as it settles in a very attractive local minima of standing still and not moving at all to reduce the cost associated with energy ratherthan to learn how to walk. Using a Cost Curriculum, we first let the agent walk at the required low speed with almost zero energy cost and once the agent learns a reasonable gait we slowly increase the cost to its original value so that the gait is fine tuned to be energy efficient.Terrain CurriculumFor increasing robustness against external perturbation and for better adaptability to real world use cases where the ground is not plane and uniform, it is useful to train the agent in uneven terrain during simulation. But introducing difficult terrain from the beginning of the training might hinder the learning and in some cases the agent might never completely solve the task as the difficulty is too high. Using terrain curriculum enables us to start with a flat plain initiallyand gradually increase the difficult of the terrain to make sure the learning rate is not too difficult that the agent makes no progress at all. We train the agent across two different terrains (Plainand Triangle) and test the learnt policy in 2 additional environments (slope and rough).Velocity CurriculumTraining the agent for a single goal velocity while might result in faster training speed, having the ability to smoothly transition between various goal speeds is often crucial and this is especiallyimportant when we want the policy to adapt to any combination of linear and angular velocity given during evaluation by the user. Therefore, using a velocity curriculum enables us to randomize and cover the whole input velocity domain systematically.Terminal ConditionsTermination conditions enable faster learning as they help the agent only explore states which are useful to the task by stopping the episode as soon as the agent reaches a state from which it cannot recover and any experience gained by the agent from that state onwards does not helpthe agent learn or get better at solving the task at hand. We use two termination conditions to help increase the training speed, both of which are discussed belowMinimum Trunk HeightThis condition ensures that the Center of Mass of the trunk is above 30cm from the ground plane as any kind of walking gait should ensure that the trunk height isn’t too low from the ground. This enable the agent to learn how to stand from a very early stage in the trainingspeeding up the overall learning.Bad ContactsThis condition ensures that the only points of contact the agent has with the ground plane are through the feet and no other parts of the agent are in contact with the ground plane. This minimizes the probability of the agent learning gaits or behaviours which result in collision between the agents body and the ground plane minimizing damage to agent body and other mechanical parts during deployment.Sim to RealSim-to-real transfer refers to the problem of transferring a reinforcement learning agent that has been trained in a simulated environment to a real-world environment. This is a challengingproblem because the simulated environment is typically different from the real-world environment in many ways, such as the dynamics of the system, the sensors and actuators, and the noise anduncertainty present in the system.We employ a mix of domain randomization and system identification for sim-to-real transfer.ResultsThe energy minimization based policy is able to adjust its gait to the most optimal gait based on the given goal velocity. Traditional policies that do not use energy minimization are only able to exhibit asingle gait.All the below videos/picture frames are from a single policy with no changes made other than the goal speed.GaitsWalkGenerating walking gait at low-speed required the use of curriculum learning as the agent found an attractive local minima where It would just stand still without moving to avoid energy costsat low speeds. Furthermore, curriculum learning was used as the primary sim to real transfer technique along with domain randomization.Terrain curriculum in particular resulted in better foot-clearance and made the agent robust to external perturbations enabling the robot to walk on extremely difficult terrain.TrotThis is one of the most commonly generated gait for use in legged locomotion, while other methods are able to generate trotting gait we believe that our method enables us to use lessenergy and torque to reach the same target velocities.GallopWhile we see a Galloping gait emerge at goal speeds greater than 1.65 m/s in simulation, we need to test if the robot hardware can physically achieve this speed and therefore exhibit thegallop gait. The other possible alternative is to use a much lower energy cost to make the agentexhibit the gallop gait at a lower goal velocity.Directional ControlUsing velocity curriculum described above, the agent is trained using a random goal velocity vector that consists of linear and angular velocity components. This enables the agentto learn not only how to walk but also how to bank and turn in the process.Emergence of Asymmetrical GaitTraining in extremely uneven terrain leads to the emergence of asymmetrical gait that maintainsa low center of gravity and shows almost a crab like walking behaviour which is persistent evenwhen the policy is deployed on a smoother terrain. The results of the training are labelled asCP2 and have been described in the latter sections.Adaptation to Unseen TerrainWhile the agent has been trained in the triangle terrain, the learnt policy is able to successfullywalk on new terrain that it has not seen during training. The base-policy is referred to as BP,the base policy is then subjected to two different curriculum resulting in policies CP1 and CP2.While both CP1 and CP2 are trained in the Triangle Terrain 1 &amp;amp; 2 the maximum height of the triangular peaks for CP2 is 0.12m (Triangle Terrain 2) while it is 0.10m for CP1(Triangle Terrain 1).All the three curriculum’s have been tested on unseen terrains rough with maximum peak height of 0.12m and slopes with max slope height of 0.8m and slope of 32 degrees. The results are asfollows:InferenceWhile CP2 has a clear advantage when deployed in Triangle Terrain 2, it performs almost as good as or slightly worse than CP1 in all the other test cases. Furthermore, it is clearly visible that BP isn’t suitable for most of the testing environment as it flat-lines pretty early. While CP2 learns a more stable gait pattern it is slower and requires lot more movement by the agent which results in CP1 gaining a few points over it as CP1 can quickly cover the velocity cost.ConclusionMost of the current reinforcement learning based policies use highly constrained rewards due towhich the locomotion policy developed doesn’t use the entire solution space. Energy minimizationbased policy is able to adjust its gait to the most optimal gait based on the goal velocity.Traditional policies that do not use energy minimization are only able to exhibit a single gait.Current LimitationsThe current implementation of the policy although exhibits different gaits when trained atdifferent goal velocities, it fails to learn more than one gait during a single training run. Webelieve this is due to the difference in the weights of the network for different gaits. Also, whiletraining in extremely unstructured environments leads to the emergence of asymmetrical gaitthat is extremely stable, the policy seems to forget the older gait and tends to use this gait evenwhen deployed later on plain terrain"
  },
  {
    "title": "Deep Reinforcement Learning - Part 2 - Finite MDP",
    "url": "/posts/DRL-2/",
    "categories": "Resources, Deep Reinforcement Learning",
    "tags": "mdp, optimal value, bellman",
    "date": "2022-11-16 13:13:20 +0530",
    "snippet": "This post is part of the Deep Reinforcement Learning Blog Series.  For a detailed introduction about the series, the curicullum, sources and references used, please check Part 0 - Getting Started.Structure of the blog  Part 0 - Getting Started  Part 1 - Multi-Arm Bandits  Part 2 - Finite MDP | Prerequisites: python, gymnasium, numpy  Part 3 - Dynamic Programming | Prerequisites: python, gymnasium, numpy  Part 4 - Monte Carlo, Temporal Difference &amp;amp; Bootstrapping Methods | Prerequisites: Python, gymnasium, numpy  Part 5 - Deep SARSA and Q-Learning | Prerequisites: python, gymnasium, numpy, torch  Part 6 - REINFORCE, AC, DDPG, TD3, SAC  Part 7 - A2C, PPO, TRPO, GAE, A3C  TBA (HER, PER, Distillation)Finite MDPWe will now consider problems that involve evaluativefeedback, as in bandits, but also an associative aspect—choosing different actions in different situations.MDPs are a classical formalization of sequential decision making,where actions inﬂuence not just immediate rewards, but also subsequent situations, or states, and through those future rewards. Thus MDPs involve delayed reward and the need to trade off immediate and delayed reward.In bandit problems we estimated the value $q_{\\star}(a)$ of each action $a$, in MDPs we estimate the value $q_\\star(s, a)$ of each action $a$ in each state $s$, or we estimate the value $v_\\star(s)$ of each state given optimal action selections. These state-dependent quantities are essential to accurately assigning credit for long-term consequences to individual action selections.MDP FrameworkIn the above figure, the Agent is the learner and decision maker. Everything other than the agent is called the environmnet.The agent and the environmnet interact with each other at every discrete timestep $t=0,1,2,..$ at each timestep the agent receives some representation of the environment’s state $S_t \\in \\mathit{S}$, and on that basis selects an action $A_t \\in \\mathit{A}(s)$ and one timestep later the environment responds to this action and presents a new state $S_{t+1}$ to the agent along with a numerical value called reward $R_{t+1} \\in  \\mathit{R} \\subset \\mathbb{R}$ which the agent seeks to maximize over time through its choice of actions.Taking the Cliff-Walking environment as an example, here is how we can access information about the state and observation space in gymnasium.import gymnasium as gymenv = gym.make(&quot;CliffWalking-v0&quot;, render_mode=&quot;human&quot;)observation, info = env.reset(seed=42)print(f&quot;The initial state after reset is : {observation}&quot;)print(f&quot;The State space is of the type: {env.observation_space}&quot;)print(f&quot;The Action space is of the type: {env.action_space}&quot;)env.close()The output for the above program is as follows:The initial state after reset is : 36The State space is of the type: Discrete(48)The Action space is of the type: Discrete(4)The MDP and agent together thereby give rise to a sequence or trajectory that begins like this:\\[S_0,A_0,R_1,S_1,A_1,R_2,...\\]The following is the python implementation for generating a trajectory for N=5 steps in the Cliff Walking Envimport gymnasium as gymenv = gym.make(&quot;CliffWalking-v0&quot;, render_mode=&quot;human&quot;)observation, info = env.reset(seed=42)trajectory = []for _ in range(5):    action = env.action_space.sample()    new_observation,reward,terminated,truncated,info = env.step(action=action)    trajectory.append([observation,action,reward])    observation=new_observation        if terminated or truncated:        observation, info = env.reset()env.close()print(f&quot;The Trajectory for 5 steps is: {trajectory}&quot;)The output of the above program is as follows:The Trajectory for 5 steps is: [[36, 3, -1], [36, 3, -1], [36, 3, -1], [36, 2, -1], [36, 0, -1]]The above code can be slightly modified to generate the trajectory for an entire episode as follows:import gymnasium as gymenv = gym.make(&quot;CliffWalking-v0&quot;,render_mode=&quot;rgb_array&quot;)observation,info = env.reset(seed=42)terminated = Falsetrajectory = []while not terminated:    action = env.action_space.sample()    new_observation,reward,terminated,truncated,info = env.step(action=action)    trajectory.append([observation,action,reward])    observation = new_observationenv.close()print(f&quot;Trajectory for entire episode: {trajectory}&quot;)In a ﬁnite MDP, the sets of states, actions, and rewards ($\\mathit{S}$, $\\mathit{A}$, and $\\mathit{R}$) all have a ﬁnite number of elements. In this case, the random variables $R_t$ and $S_t$ have well deﬁneddiscrete probability distributions dependent only on the preceding state and action. That is, for particular values of these random variables, $s’ \\in \\mathit{S}$ and $r \\in \\mathit{R}$, there is a probability of those values occurring at time t, given particular values of the preceding state and action:\\[p(s&#39;,r|s,a) \\ \\dot{=} \\ \\text{Pr}\\{S_t=s&#39;,R_t=r \\ | \\  S_{t-1}=s,A_{t-1}=a \\}\\]Further since p speciﬁes a probability distribution for each choice of sand a,\\[\\sum_{s&#39;\\in \\mathit{S}}\\sum_{r \\in \\mathit{R}}p(s&#39;,r|s,a)=1, \\text{for all } s \\in \\mathit{S}, a \\in \\mathit{A}(s)\\]In a Markov decision process, the probabilities given by $p$ completely characterize the environment’s dynamics. That is, the probability of each possible value for $S_t$ and $R_t$ depends on the immediately preceding state and action, $S_{t-1}$ and $A_{t-1}$, and, given them,not at all on earlier states and actions. Therefore the state must include information about all aspects of the past agent–environment interaction that make a difference for the future. If itdoes, then the state is said to have the Markov property.From the four-argument dynamics function, $p$, one can compute anything else one mightwant to know about the environment, such asState-Transition ProbabilitiesA three-argument function denoting the probability of reaching state $s’$ when action $a$ is taken from state $s$.\\[p(s&#39;|s,a) \\ \\dot{=} \\ \\text{Pr}\\{S_t=s&#39;|S_{t-1}=s,A_{t-1}=a \\} = \\sum_{r \\in \\mathit{R}}p(s&#39;,r|s,a)\\]Expected Reward for State-ActionA two-argument function defining the expected rewards for a state-action pair.\\[r(s,a) \\ \\dot{=} \\ \\mathbb{E}[R_t|S_{t-1}=s,A_{t-1}=a] = \\sum_{r \\in \\mathit{R}} \\left[ r\\sum_{s&#39; \\in \\mathit{S}} p(s&#39;,r|s,a)\\right]\\]Expected Reward for State-Action-StateA Three argument function defining the expected rewards for a state-action-state pair.\\[\\begin{align*}r(s,a,s&#39;) \\ \\dot{=} \\ \\mathbb{E}[R_t|S_{t-1}=s,A_{t-1}=a,S_t=s&#39;] &amp;amp;= \\sum_{r \\in \\mathit{R}}r\\dfrac{p(s&#39;,r|s,a)}{p(s&#39;|s,a)} \\\\&amp;amp;= \\sum_{r \\in \\mathit{R}}r\\dfrac{p(s&#39;|s,a).p(r|s,a,s&#39;)}{p(s&#39;|s,a)} \\\\&amp;amp;= \\sum_{r \\in \\mathit{R}}r.p(r|s,a,s&#39;) \\\\ \\end{align*}\\]Goals &amp;amp; RewardsIn reinforcement learning, the purpose or goal of the agent is formalized in terms of a special signal, called the reward, passing from the environment to the agent. At each time step, the reward is a simple number, $R_t \\in \\mathit{R}$. Informally, the agent’s goal is to maximize the total amount of reward it receives. This means maximizing not immediate reward, but cumulative reward in the long run.The reward signal is your way of communicating to the agent what you want achieved, not how you want it achieved..  The Reward HypothesisThat all of what we mean by goals and purposes can be well thought of asthe maximization of the expected value of the cumulative sum of a received scalar signal (called reward).Returns &amp;amp; EpisodesTo formally define how we wish to maximize the cumulative reward, we seek to maximize the expected return, where the return, denoted $G_t$,isdeﬁned as some speciﬁc function of the reward sequence. In the simplest case the return is the sum of the rewards:\\[G_t \\ \\dot{=}\\ R_{t+1} + R_{t+2} + R_{t+3}+...+R_T\\]Where T is the final timestep, this approach is applicable when there is a natural notion of ﬁnal time step, that is, when the agent–environment interaction breaks naturally into subsequences, which we call episodes. Each episode ends in a special state called the terminal state, followed by a reset to a standard starting state or to asample from a standard distribution of starting states. Even if you think of episodes as ending in different ways, such as winning and losing a game, the next episode begins independently of how the previous one ended. Thus the episodes can all be considered toend in the same terminal state, with different rewards for the different outcomes.Tasks with episodes of this kind are called episodic tasks. In episodic tasks we sometimes need to distinguish the set of all nonterminal states, denoted $S$, from the set of all states plusthe terminal state, denoted $S^+$. The time of termination, $T$, is a random variable that normally varies from episode to episode.On the other hand, in many cases the agent–environment interaction does not break naturally into identiﬁable episodes, but goes on continually without limit. For example,this would be the natural way to formulate an on-going process-control task, or an application to a robot with a long life span. We call these continuing tasks. The return formulation discussed above is problematic for continuing tasks because the ﬁnal time step would be $T = \\infty$, and the return, which is what we are trying to maximize, could easily be inﬁnite.The additional concept that we need is that of discounting. According to this approach, the agent tries to select actions so that the sum of the discounted rewards it receives over the future is maximized. In particular, it chooses $A_t$ to maximize the expected discountedreturn:\\[G_t \\ \\dot{=}\\ R_{t+1} + \\gamma{R_{t+2}} + \\gamma^2{R_{t+3}}+... = \\sum_{k=0}^{\\infty}\\gamma^kR_{t+k+1}\\]Where $\\gamma$ is called the discount factor and lies between $[0,1]$. The above formula can also be written recursivly as,\\[G_t \\ \\dot{=} R_{t+1} + \\gamma G_{t+1}\\]Unified Notation for TasksWe need one other convention to obtain a single notation that covers both episodic and continuing tasks. We have deﬁned the return as a sum over a ﬁnite number of terms in one case and as a sum over an inﬁnite number of terms in the other. These two can be uniﬁed by considering episode termination to be the entering of a special absorbing state that transitions only to itself and that generates only rewards of zero. For example, consider the state transition diagram:Here the solid square represents the special absorbing state corresponding to the end of anepisode. Starting from $S_0$, we get the reward sequence +1, +1, +1, 0, 0, 0,. …Summing these, we get the same return whether we sum over the ﬁrst $T$ rewards (here $T$ = 3) or over the full inﬁnite sequence. This remains true even if we introduce discounting.We can therefore write the expected return as,\\[G_{t}  \\dot{=}  \\sum_{k=t+1}^{T}\\gamma^{k-t-1}R_k\\]Where there is a possibilty that $T = \\infty$ or $\\gamma = 1$ (but not both).The above formula can be used in code as follows:import gymnasium as gym env = gym.make(&quot;CliffWalking-v0&quot;,render_mode=&quot;rgb_array&quot;)terminated = FalseG = 0 # Return observation, info = env.reset(seed=42)counter = 0gamma = 1while not terminated:    action = env.action_space.sample()    observation,reward,terminated,truncated,info = env.step(action=action)    G += reward*(gamma**counter)    counter +=1env.close()print(f&quot;The episode terminated after {counter} steps with Return(G) {G} for gamma {gamma}&quot;)The output for the above code with various values of gamma is as follows:The episode terminated after 853 steps with Return(G) -8575 for gamma 1The episode terminated after 1391 steps with Return(G) -1445.9837656301004 for gamma 0.99The episode terminated after 4516 steps with Return(G) -1 for gamma 0Policies &amp;amp; Value FunctionsAlmost all reinforcement learning algorithms involve estimating value functions— which are functions of states (or of state–action pairs) that estimate how good it is for the agent to be in a given state (or how good it is to perform a given action in a given state).The notion of “how good” here is deﬁned in terms of future rewards that can be expected, or, to be precise, in terms of expected return. Of course the rewards the agent can expect to receive in the future depend on what actions it will take. Accordingly, value functionsare deﬁned with respect to particular ways of acting, called policies.Formally, a policy is a mapping from states to probabilities of selecting each possible action. If the agent is following policy $\\pi$ at time $t$,then $\\pi(a\\mid s)$ is the probability that $A_t = a$ if $S_t = s$. Like $p,\\pi$ is an ordinary function.The $\\mid$ in the middle of $\\pi(a\\mid s)$ merely reminds us that it deﬁnes a probability distribution over $a \\in \\mathit{A}(s)$ for each $s \\in \\mathit{S}$. Reinforcement learning methods specify how the agent’s policy is changed as a result of its experience.State Value FunctionThe value function of a state $s$ under a policy $\\pi$, denoted by $v_\\pi(s)$, is the expected return when starting in $s$ and following $\\pi$ thereafter. For MDPs, we can deﬁne $v_\\pi$ formally as\\[\\begin{align*}v_\\pi(s) \\ &amp;amp;\\dot{=} \\ \\mathbb{E}_\\pi[G_t|S_t=s] = \\mathbb{E}\\left[ \\sum_{k=0}^{\\infty}\\gamma^k R_{k+t+1} | S_t=s\\right] \\text{ for all } s \\in \\mathit{S}\\\\v_\\pi(s) \\ &amp;amp;\\dot{=} \\ \\mathbb{E}_\\pi[R_{t+1} + \\gamma G_{t+1}|S_t=s]\\\\v_\\pi(s) \\ &amp;amp;\\dot{=} \\ \\mathbb{E}_\\pi[R_{t+1} + \\gamma v_\\pi({S_{t+1}})|S_t=s]\\end{align*}\\]where $\\mathbb{E}[.]$ denotes the expected value of the variable given that the agent follows the policy $\\pi$. Also, the value of the terminal state is considered to be zero.Action Value FunctionThe value of taking action $a$ in state $s$ under a policy $\\pi$, denoted by $q_\\pi(s, a)$, as the expected return starting from $s$, taking the action $a$, and thereafter following policy $\\pi$:\\[\\begin{align*}q_\\pi(s,a) \\ &amp;amp;\\dot{=} \\ \\mathbb{E}_\\pi[G_t|S_t=s,A_t=a] = \\mathbb{E}\\left[ \\sum_{k=0}^{\\infty}\\gamma^k R_{k+t+1} | S_t=s, A_t=a\\right]\\\\q_\\pi(s,a) \\ &amp;amp;\\dot{=} \\ \\mathbb{E}_\\pi[R_{t+1} + \\gamma G_{t+1}|S_t=s,A_t=a]\\\\q_\\pi(s,a) \\ &amp;amp;\\dot{=} \\ \\mathbb{E}_\\pi[R_{t+1} + \\gamma q_\\pi{(S_{t+1})}|S_t=s,A_t=a]\\\\\\end{align*}\\]Bellman Value Equations\\[v_\\pi(s) = \\sum_a \\pi(a\\mid s)q_\\pi(s,a) \\tag{2.1}\\]\\[q_\\pi(s,a) = \\sum_{s&#39;,r} p(s&#39;,r\\mid s,a)(r + \\gamma v_\\pi(s&#39;)) \\tag{2.2}\\]Substuting equation (2.2) in (2.1) we get the following recursive relation in terms of $v_\\pi.$\\[v_\\pi(s) = \\sum_{a}\\pi(a\\mid s)\\sum_{s&#39;,r} p(s&#39;,r\\mid a,s)[r+\\gamma v_\\pi(s&#39;)] \\tag{2.2}\\]Substuting equation (2.1) in (2.2) we get the following recursive relation in terms of $q_\\pi.$\\[q_\\pi(a,s) = \\sum_{s&#39;,r}p(s&#39;,r\\mid s,a)[r + \\gamma \\sum_a&#39; \\pi(a&#39;\\mid s&#39;)q_\\pi(s&#39;,a&#39;)] \\tag{2.3}\\]Optimal Policies &amp;amp; Value FunctionsThere is always at least one policy that is better than or equal to all other policies. This is an optimal policy. Although there may be more than one, we denote all the optimal policies by $\\pi_{\\star}$. They share the same state-value function, called the optimalstate-value function, denoted $v_\\star$.\\[v_\\star(s) = \\text{max}_a q_\\star (s,a) \\tag{2.4}\\]\\[q_\\star(s,a) = \\text{max}\\sum_{s&#39;,r} p(s&#39;,r\\mid s,a)(r + v_\\star(s&#39;)) \\tag{2.5}\\]Substuting equation (2.5) in (2.4) we get the following recursive relation in terms of $v_\\pi.$\\[v_\\star(s) = \\text{max}\\sum_{s&#39;,a}p(r,s&#39;\\mid s,a)[r + \\gamma v_\\star (s&#39;)] \\tag{2.6}\\]Substuting equation (2.4) in (2.5) we get the following recursive relation in terms of $q_\\pi.$\\[q_\\star(s,a) = \\sum p(s&#39;,r\\mid s,a) [r + \\gamma\\  \\text{max} \\ q_\\star(s&#39;,a&#39;)] \\tag{2.7}\\]"
  },
  {
    "title": "Deep Reinforcement Learning - Part 1 - Multi-Arm Bandits",
    "url": "/posts/DRL-1/",
    "categories": "Resources, Deep Reinforcement Learning",
    "tags": "bandits, sample average, weighted average, ucb, bias",
    "date": "2022-11-15 19:13:20 +0530",
    "snippet": "This post is part of the Deep Reinforcement Learning Blog Series.  For a detailed introduction about the series, the curicullum, sources and references used, please check Part 0 - Getting Started.Structure of the blog  Part 0 - Getting Started  Part 1 - Multi-Arm Bandits  Part 2 - Finite MDP | Prerequisites: python, gymnasium, numpy  Part 3 - Dynamic Programming | Prerequisites: python, gymnasium, numpy  Part 4 - Monte Carlo, Temporal Difference &amp;amp; Bootstrapping Methods | Prerequisites: Python, gymnasium, numpy  Part 5 - Deep SARSA and Q-Learning | Prerequisites: python, gymnasium, numpy, torch  Part 6 - REINFORCE, AC, DDPG, TD3, SAC  Part 7 - A2C, PPO, TRPO, GAE, A3C  TBA (HER, PER, Distillation)Reinforcement learning loosely refers to the area of machine learning where an agent is tasked with learning about the concequences of its actions and in that process also maximize the numerical reward signal collected over time.K-Arm BanditsIn this simplified setting we assume that the problem is non-associatve and therefore does not involve learning to act in more than one situation.  Consider the following problem, You are faced repeatedly with a choice among k different options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.Each of the actions has an expected/mean reward associated with it, called the value of the action ($A_t$). At any timestep $t$ the reward for choosing a paticular action is denoted by $R_t$. The value of an arbitrary action can then be represented as:\\[q_*(a) \\ \\dot{=} \\ \\mathbb{E}[R_t|A_t = a ]\\]The value of the action is defined as the expected reward associated with choosing the action.Given the values of all actions it is quite trivial to solve the k-arm bandit problem as one would choose the action with the highest value all the time. But in most close-to-real life senarios the action values are not given and need to be estimated first. Let $Q_t(a)$ denote the estimated value of the action $a$ at time $t$.We have two possible moves here,  Make our estimates better by sampling all actions aka Exploration  Choose the highest action value given the current estimates aka ExploitThe need to balance exploration and exploitation is a distinctive challenge that arises in reinforcement learning. Below mentioned are a few methods to do so..Action-value MethodsWe begin by looking more closely at methods for estimating the values of actions and for using the estimates to make action selection decisions, which we collectively call action-value methods.Sample AverageRecall that the value of an action is the mean reward recieved when the action is choosen, one approach therefore is to average all the rewards for an action. Given by\\[Q_{n+1}(a) \\ \\dot{=} \\ \\dfrac{R_1+R_2+..+R_{n}}{n}\\]The estimated value of an action after being selected n timesFor easier implementation can be written as a recursive formula as follows:\\[\\begin{align*}Q_{n+1}(a) &amp;amp;= \\dfrac{1}{n} \\left( \\sum_{i=1}^{i=n}{R_i} \\right)\\\\  &amp;amp;= \\dfrac{1}{n} \\left( R_n + (n-1)\\dfrac{1}{(n-1)}\\sum_{i=1}^{i=n-1}{R_i} \\right)\\\\&amp;amp;= \\dfrac{1}{n} \\left( R_n + (n-1)Q_n(a) \\right)\\\\&amp;amp;= \\dfrac{1}{n} \\left( R_n + nQ_n(a) - Q_n(a) \\right)\\\\&amp;amp;= Q_n(a) + \\dfrac{1}{n}\\left[ R_n - Q_n(a)\\right]\\end{align*}\\]The above update rule can be summarised as follows:\\[NewEstimate = OldEstimate + stepSize[Target-OldEstimate]\\]A few observations based on the above:  $[Target-OldEstimate]$ is the error in the estimate that we wish to minimize by taking a step towards the target.  The stepSize parameter is not constant (For e.g. in sample avg it was 1/n) and is often denoted by $\\alpha_t(a)$Given the estimates we can now focus on exploitation, a simple rather “greedy” method would be to choose the highest action value which can be represented as:\\[A_t = \\text{argmax}_{a}Q_t(a)\\]The expression $\\text{argmax}_{a}$ denotes that a is choosen such that $Q_t(a)$ is maximised with ties broken arbitrarily.The above method of action selection is quite weak as it does no exploration and therefore can be acting on false assumtions of the action values. A better approach would be to exploit most of the time but every now and then explore values as well. This is called the $\\varepsilon$-greedy methods where the decision to explore is based on a small probability $\\varepsilon$.Exponential Recency-Weighted AverageThe combination of $\\varepsilon$-greedy with sample averages works well for stationary problems where the reward probabilties do not change with time. But as stated at the start in most cases this assumption isn’t valid. Therefore the solution for problems that involve non-statinoary reward distributions is to give more weight to recent rewards when compared to old rewards and one possible way to achieve this is to use a constant stepSize parameter that lies between $[0,1]$.The estimated value from sample value method can be re-written as follows:\\[\\begin{align*}Q_{n+1}(a) &amp;amp;= Q_n(a) + \\alpha\\left[ R_n - Q_n(a)\\right]\\\\&amp;amp;= \\alpha{R_n}+ (1-\\alpha)Q_n\\\\&amp;amp;= (1-\\alpha)^nQ_1 + \\sum_{i=1}^n \\alpha(1-\\alpha)^{n-i}R_i\\end{align*}\\]The final step can be derived based on the derivation in the sample based method section and therefore has been cut-short.The above is called weighted-average because the sum of weights is equal to 1, infact the weights decrease exponentially and the above method is therefore called Exponential Recency-Weighted Average.Initial Values &amp;amp; BiasWhen an action is being selected for the first time in sample average method the denominator is 0, therfore a default value is assumed in such cases and in Exponential Recency-Weighted Average method, the value of $Q_2$ depends on the assumption $Q_1$ which again has to be assumed.Therefore these methods are biased by their initial estimates. For the sample-average methods, the bias disappears once all actions have been selected at least once, but for methods with constant stepSize, the bias is permanent, though decreasing over time as seen in the equation derived above.In practice this kind of bias is helpful when the initial values choosen are based on expert knowledge. Initial values can also be used to encourage exploration, instead of setting the initial values to 0 if we set them to a very high value aka “optimistic value” the agent tries each action and gets dissapointed but keeps on trying until all the values are sampled atleast once this method often reffered to as optimistic initial value makes sure there is exploration at the start even when used with pure greedy methods.Exponential Recency-Weighted Average without Initial BiasGiven that the Exponential Recency-Weighted Average while works on non-stationary problems but suffers from initial bias and the sample average methods are less effected by the initial bias but are not effective against non-stationary problems, there is a need for another method that can work well with non-stationary problems without any intial bias.One such method can be formulated with the use of a new stepSize parameter defined as\\[\\beta_n \\ \\dot{=} \\ \\dfrac{\\alpha}{\\bar{o}_n}\\]To process the nth reward for a particular action, where $\\alpha$&amp;lt;0 is the conventional stepSize and $\\bar{o}_n$ is the trace of one that starts at 0:\\(\\bar{o}_n = \\bar{o}_{n-1} + \\alpha(1-\\bar{o}_{n-1}), \\text{ for } n&amp;gt;0, \\text{ with } \\bar{o}_0 \\ \\dot{=} \\ 0\\)Upper-Conﬁdence-BoundIn the $\\varepsilon$-greedy methods, while randomly picking actions every once in a while to encourage exploration helps, it makes much more sense to pick these actions based on some guided hurestic rather than random-sampling.One possible alternative is to pick among the non-greedy actions is to opt for actons that have higher degree of uncertainty in their estimates, such a mechanism will therefore allow us to get better overall estimates for all action values.The idea of upper conﬁdence bound (UCB) action selection is that the square-root term is a measure of the uncertainty or variance in the estimate of a’s value. The quantity being max’ed over is thus a sort of upper bound on the possible true value of action a,withc determining the conﬁdence level.The action is therefore selected as follows:\\[A_t \\ \\dot{=} \\ \\text{argmax}_a \\left[Q_t(a) + c\\sqrt{\\dfrac{ln\\ t}{N_t(a)}} \\right]\\]Where, $N_t(a)$ denotes the number of times action $a$ has been selected prior to time $t$ and $c$ &amp;gt;0 controls the degree of exploration. When an action is being explored for the first time it is considered to be a maximizing action.Each time $a$ is selected the uncertainty is presumably reduced: $N_t(a)$ increments, and, as it appears in the denominator, the uncertainty term decreases. On the other hand, each time an action other than $a$ is selected, $t$ increases but $N_t(a)$ does not; because $t$ appears in the numerator, the uncertainty estimate increases.The use of the natural logarithm means that the increases get smaller over time, but areunbounded; all actions will eventually be selected, but actions with lower value estimates,or that have already been selected frequently, will be selected with decreasing frequencyover time.Gradient-BanditSo far we have considered methods that estimate action values and usethose estimates to select actions. This is often a good approach, but it is not the only one possible.In this section we consider learning a numerical preference for each action $a$, which we denote as $H_t(a) \\in \\mathbb{R}$. The larger the preference, the more often that action is taken, but the preference has no interpretation in terms of reward. Only the relativepreference of one action over another is important; if we add 1000 to all the action preferences there is no effect on the action probabilities, which are determined accordingto a soft-max distribution (i.e., Gibbs or Boltzmann distribution) as follows:\\[\\text{Pr}\\{A_t=a\\} \\ \\dot{=} \\ \\dfrac{e^{H_t(a)}}{\\sum_{b=1}^{k}e^{H_t(b)}}  \\ \\dot{=} \\ \\pi_t(a)\\]Where $\\pi_t(a)$ is the probability of choosing action $a$ at time $t$. Initially all actions have the same preference, therefore $H_1(a)=0$.There is a natural learning algorithm for soft-max action preferences based on the ideaof stochastic gradient ascent. On each step, after selecting action $A_t$ and receiving thereward $R_t$, the action preferences are updated by:\\[\\begin{align*}H_{t+1}(A_t) \\ &amp;amp;\\dot{=} \\ H_t(A_t) + \\alpha(R_t - \\bar{R}_t)(1-\\pi_t(A_t)) \\\\H_{t+1}(a) \\ &amp;amp;\\dot{=} \\ H_t(a) - \\alpha(R_t - \\bar{R}_t)\\pi_t(a) \\text{ for all } a\\neq A_t \\\\\\end{align*}\\]where $\\alpha &amp;gt; 0$ is a step-size parameter, and $\\bar{R}_t \\in \\mathbb{R}$ is the average of the rewards up to butnot including time t (with $\\bar{R}_1 = R_1$), which can be computed incrementally.  The $\\bar{R}_t$ term serves as abaseline with which the reward is compared. If the reward is higher than the baseline, then the probability of taking $A_t$ in the future is increased, and if the reward is below baseline, then the probability is decreased. The non-selected actions move in the oppositedirection.Convergence ConditionsNot all stepSize values guarentee that the expected action value converges to the true values. A well-known result in stochastic approximation theory gives us the conditions required toassure convergence with probability 1:  The ﬁrst condition is required to guarantee that the steps are large enough to eventually overcome any initial conditions or random ﬂuctuations.  The second condition guarantees that eventually the steps become small enough to assure convergence.These can be expressed as follows:\\[\\begin{align*}\\sum_{n=1}^{\\infty}\\alpha_n(a) &amp;amp;= \\infty\\\\\\sum_{n=1}^{\\infty}\\alpha_n^2(a) &amp;amp;&amp;lt; \\infty\\\\\\end{align*}\\]Associative SearchIn the above discussed methods the learner either tries to ﬁnd a single best action when the task is stationary, or tries to track the best action as it changes over time when the task is nonstationary. However, in a general reinforcement learning task there is more than one situation, and the goal is to learn a policy: a mapping from situations to the actions that are best in those situations. To set the stage for the full problem, we brieﬂy discuss the simplest way in which nonassociative tasks extend to the associative setting.As an example, suppose there are several different k-armed bandit tasks, and that on each step you confront one of these chosen at random. Thus, the bandit task changes randomly from step to step. If the probabilities with which each task is selected for youdo not change over time, this would appear as a single stationary k-armed bandit task, and you could use one of the methods described in this chapter. Now suppose, however, that when a bandit task is selected for you, you are given some distinctive clue about its identity (but not its action values). Maybe you are facing an actual slot machine that changes the color of its display as it changes its action values. Now you can learn a policy associating each task, signaled by the color you see, with the best action to take when facing that task—for instance, if red, select arm 1; if green, select arm 2. With the right policy you can usually do much better than you could in the absence of any information distinguishing one bandit task from another.This is an example of an associative search task, so called because it involves both trial-and-error learning to search for the best actions, and association of these actions with the situations in which they are best.Associative search tasks are intermediate between the k-armed bandit problem and the full reinforcement learning problem. They are like the full reinforcement learning problem in that they involve learning a policy, but they are also like our version of the k-armed bandit problem in that each action affects only the immediate reward. If actions are allowed to affect the next situation as well as the reward, then we have the full reinforcement learning problem."
  },
  {
    "title": "Autonomus Ground Vehicle",
    "url": "/posts/AGV/",
    "categories": "Projects, Autonomous Robots",
    "tags": "ws2812b, esp32, mic, bluetooth",
    "date": "2022-01-05 19:13:20 +0530",
    "snippet": "An Autonomous Ground Vechile for exploring concepts like localization, path planning, control in ROS.Hardware  NVIDIA Jetson Nano  Arduino Mega  Buck Converter  RPLIDAR A8  L298N Motor Driver Module x2  100 RPM Geared Motors + Encoders x4  2200 mAh 3S 35C Lipo x1Layout  Solid bi-directional green lines represent lines that transfer both power and data.  Dark red lines represent 12V power transfer lines  Light red lines represent 5V power transfer lines  dashed blue lines represent data transfer lines  Red squares represnt power supply/distribution components  Blue squares represent inputs/sensors  Yellow squares represent computational units  Purple squares represent output units/actuators.ResultsPreliminary scan dataThis is an ongoing project, documentation will be updated soon.ROS Node TreeSLAMPath Planning"
  },
  {
    "title": "URDF",
    "url": "/posts/ROS-URDF/",
    "categories": "Resources, Robotics Theory",
    "tags": "ws2812b, esp32, mic, bluetooth",
    "date": "2022-01-02 19:13:20 +0530",
    "snippet": "The URDF file format is a way to represent robots and their components in ROS. It uses a modified verion of the XML syntax.Structure of URDFgeometry typesbox&amp;lt;box size=&quot;0.6 0.1 0.2&quot;/&amp;gt;  size attribute contains the three side lengths of the box. The origin of the box is in its center.    cylinder  &amp;lt;cylinder length=&quot;0.6&quot; radius=&quot;0.2&quot;/&amp;gt;  Specify the radius and length. The origin of the cylinder is in its center.    sphere  &amp;lt;sphere radius=&quot;3.0&quot;/&amp;gt;  Specify the radius. The origin of the sphere is in its center.    mesh  &amp;lt;mesh filename=&quot;package://robot_description/meshes/base_link_simple.DAE&quot;/&amp;gt;  A trimesh element specified by a filename, and an optional scale that scales the mesh’s axis-aligned-bounding-box. Any geometry format is acceptable but specific application compatibility is dependent on implementation.  The recommended format for best texture and color support is Collada .dae files. The mesh file is not transferred between machines referencing the same model. It must be a local file. Prefix the filename with package:/// to make the path to the mesh file relative to the package .    joint types    fixed    &amp;lt;joint name=&quot;base_to_right_leg&quot; type=&quot;fixed&quot;&amp;gt;  &amp;lt;parent link=&quot;base_link&quot;/&amp;gt;  &amp;lt;child link=&quot;right_leg&quot;/&amp;gt;  &amp;lt;origin xyz=&quot;0 -0.22 0.25&quot;/&amp;gt;&amp;lt;/joint&amp;gt;      continuous  &amp;lt;joint name=&quot;head_swivel&quot; type=&quot;continuous&quot;&amp;gt;    &amp;lt;parent link=&quot;base_link&quot;/&amp;gt;    &amp;lt;child link=&quot;head&quot;/&amp;gt;    &amp;lt;axis xyz=&quot;0 0 1&quot;/&amp;gt;    &amp;lt;origin xyz=&quot;0 0 0.3&quot;/&amp;gt;  &amp;lt;/joint&amp;gt;The connection between the body and the head is a continuous joint, meaning that it can take on any angle from negative infinity to positive infinity. The wheels are also modeled like this, so that they can roll in both directions forever.The only additional information we have to add is the axis of rotation, here specified by an xyz triplet, which specifies a vector around which the head will rotate. Since we want it to go around the z axis, we specify the vector “0 0 1”.revolute  &amp;lt;joint name=&quot;left_gripper_joint&quot; type=&quot;revolute&quot;&amp;gt;    &amp;lt;axis xyz=&quot;0 0 1&quot;/&amp;gt;    &amp;lt;limit effort=&quot;1000.0&quot; lower=&quot;0.0&quot; upper=&quot;0.548&quot; velocity=&quot;0.5&quot;/&amp;gt;    &amp;lt;origin rpy=&quot;0 0 0&quot; xyz=&quot;0.2 0.01 0&quot;/&amp;gt;    &amp;lt;parent link=&quot;gripper_pole&quot;/&amp;gt;    &amp;lt;child link=&quot;left_gripper&quot;/&amp;gt;  &amp;lt;/joint&amp;gt;Both the right and the left gripper joints are modeled as revolute joints. This means that they rotate in the same way that the continuous joints do, but they have strict limits. Hence, we must include the limit tag specifying the upper and lower limits of the joint (in radians). We also must specify a maximum velocity and effort for this joint but the actual values don’t matter for our purposes here.prismatic  &amp;lt;joint name=&quot;gripper_extension&quot; type=&quot;prismatic&quot;&amp;gt;    &amp;lt;parent link=&quot;base_link&quot;/&amp;gt;    &amp;lt;child link=&quot;gripper_pole&quot;/&amp;gt;    &amp;lt;limit effort=&quot;1000.0&quot; lower=&quot;-0.38&quot; upper=&quot;0&quot; velocity=&quot;0.5&quot;/&amp;gt;    &amp;lt;origin rpy=&quot;0 0 0&quot; xyz=&quot;0.19 0 0.2&quot;/&amp;gt;  &amp;lt;/joint&amp;gt;The gripper arm is a different kind of joint, namely a prismatic joint. This means that it moves along an axis, not around it. This translational movement is what allows our robot model to extend and retract its gripper arm.The limits of the prismatic arm are specified in the same way as a revolute joint, except that the units are meters, not radians.&amp;lt;?xml version=&quot;1.0&quot;?&amp;gt;&amp;lt;robot name=&quot;multipleshapes&quot;&amp;gt;	&amp;lt;link name=&quot;dummy&quot;&amp;gt;	&amp;lt;/link&amp;gt;  &amp;lt;link name=&quot;base_link&quot;&amp;gt;    &amp;lt;visual&amp;gt;      &amp;lt;geometry&amp;gt;        &amp;lt;cylinder length=&quot;0.6&quot; radius=&quot;0.2&quot;/&amp;gt;      &amp;lt;/geometry&amp;gt;      &amp;lt;material name=&quot;blue&quot;&amp;gt;        &amp;lt;color rgba=&quot;0 0 .8 1&quot;/&amp;gt;      &amp;lt;/material&amp;gt;    &amp;lt;/visual&amp;gt;    &amp;lt;collision&amp;gt;      &amp;lt;geometry&amp;gt;        &amp;lt;cylinder length=&quot;0.6&quot; radius=&quot;0.2&quot;/&amp;gt;      &amp;lt;/geometry&amp;gt;    &amp;lt;/collision&amp;gt;    &amp;lt;inertial&amp;gt;      &amp;lt;mass value=&quot;10&quot;/&amp;gt;      &amp;lt;inertia ixx=&quot;0.4&quot; ixy=&quot;0.0&quot; ixz=&quot;0.0&quot; iyy=&quot;0.4&quot; iyz=&quot;0.0&quot; izz=&quot;0.2&quot;/&amp;gt;    &amp;lt;/inertial&amp;gt;  &amp;lt;/link&amp;gt;  &amp;lt;link name=&quot;right_leg&quot;&amp;gt;    &amp;lt;visual&amp;gt;      &amp;lt;geometry&amp;gt;        &amp;lt;box size=&quot;0.6 0.1 0.2&quot;/&amp;gt;      &amp;lt;/geometry&amp;gt;      &amp;lt;origin rpy=&quot;0 1.57075 0&quot; xyz=&quot;0 0 -0.3&quot;/&amp;gt;    &amp;lt;/visual&amp;gt;  &amp;lt;/link&amp;gt;  &amp;lt;joint name=&quot;base_to_right_leg&quot; type=&quot;fixed&quot;&amp;gt;    &amp;lt;parent link=&quot;base_link&quot;/&amp;gt;    &amp;lt;child link=&quot;right_leg&quot;/&amp;gt;    &amp;lt;origin xyz=&quot;0 -0.22 0.25&quot;/&amp;gt;  &amp;lt;/joint&amp;gt;    &amp;lt;joint name=&quot;dummy_joint&quot; type=&quot;fixed&quot;&amp;gt;	 &amp;lt;parent link=&quot;dummy&quot;/&amp;gt;	 &amp;lt;child link=&quot;base_link&quot;/&amp;gt;  &amp;lt;/joint&amp;gt;&amp;lt;/robot&amp;gt;The above code is a modified example from urdf_tutorial package, instructions on how to download the tutorials can be found in the ros urdf wiki.The output of the above code is as follows:Summary of the above code  The fixed frame is the transform frame where the center of the grid is located. Here, it’s a frame defined by our one link, base_link.  The visual element (the cylinder) has its origin at the center of its geometry as a default. Hence, half the cylinder is below the grid.  The joint is defined in terms of a parent and a child. URDF is ultimately a tree structure with one root link. This means that the leg’s position is dependent on the base_link’s position.  Let’s start by examining the joint’s origin. It is defined in terms of the parent’s reference frame. So we are -0.22 meters in the y direction (to our left, but to the right relative to the axes) and 0.25 meters in the z direction (up). This means that the origin for the child link will be up and to the right, regardless of the child link’s visual origin tag. Since we didn’t specify a rpy (roll pitch yaw) attribute, the child frame will be default have the same orientation as the parent frame.  Now, looking at the leg’s visual origin, it has both a xyz and rpy offset. This defines where the center of the visual element should be, relative to its origin. Since we want the leg to attach at the top, we offset the origin down by setting the z offset to be -0.3 meters. And since we want the long part of the leg to be parallel to the z axis, we rotate the visual part PI/2 around the Y axis.  The collision element is a direct subelement of the link object, at the same level as the visual tag.  The collision element defines its shape the same way the visual element does, with a geometry tag. The format for the geometry tag is exactly the same here as with the visual.  You can also specify an origin in the same way as a subelement of the collision tag (as with the visual).  The dummy link and joint are added to avoid the following warning given by KDL          The root link base_link has an inertia specified in the URDF, but KDL does not support a root link with an inertia. As a workaround, you can add an extra dummy link to your URDF.      The meshes can be imported in a number of different formats. STL is fairly common, but the engine also supports DAE, which can have its own color data, meaning you don’t have to specify the color/material. Often these are in separate files. These meshes reference the .tif files also in the meshes folder.Xacro + URDFThe Xacro package can be used to simplify URDF, in paticular it can be used to deal with the followingyou can use the xacro package to make your life simpler. It does three things that are very helpful.  Constants  Simple Math  MacrosUsing Xacrovia .xacro&amp;lt;?xml version=&quot;1.0&quot;?&amp;gt;&amp;lt;robot xmlns:xacro=&quot;http://www.ros.org/wiki/xacro&quot; name=&quot;firefighter&quot;&amp;gt;  At the top of the URDF file, you must specify a namespace in order for the file to parse properly. For example, these are the first two lines of a valid xacro file:via launch files&amp;lt;param name=&quot;robot_description&quot;  command=&quot;xacro --inorder &#39;$(find pr2_description)/robots/pr2.urdf.xacro&#39;&quot; /&amp;gt;  You can also automatically generate the urdf in a launch file.Constants&amp;lt;xacro:property name=&quot;width&quot; value=&quot;0.2&quot; /&amp;gt;&amp;lt;xacro:property name=&quot;bodylen&quot; value=&quot;0.6&quot; /&amp;gt;&amp;lt;link name=&quot;base_link&quot;&amp;gt;    &amp;lt;visual&amp;gt;        &amp;lt;geometry&amp;gt;            &amp;lt;cylinder radius=&quot;${width}&quot; length=&quot;${bodylen}&quot;/&amp;gt;        &amp;lt;/geometry&amp;gt;        &amp;lt;material name=&quot;blue&quot;/&amp;gt;    &amp;lt;/visual&amp;gt;    &amp;lt;collision&amp;gt;        &amp;lt;geometry&amp;gt;            &amp;lt;cylinder radius=&quot;${width}&quot; length=&quot;${bodylen}&quot;/&amp;gt;        &amp;lt;/geometry&amp;gt;    &amp;lt;/collision&amp;gt;&amp;lt;/link&amp;gt;      The two values are specified in the first two lines. They can be defined just about anywhere (assuming valid XML), at any level, before or after they are used. Usually they go at the top.        Instead of specifying the actual radius in the geometry element, we use a dollar sign and curly brackets to signify the value.  &amp;lt;xacro:property name=”robotname” value=”marvin” /&amp;gt;&amp;lt;link name=”${robotname}s_leg” /&amp;gt;will be read as  &amp;lt;link name=”marvins_leg” /&amp;gt;MathsYou can build up arbitrarily complex expressions in the ${} construct using the four basic operations (+,-,*,/), the unary minus, and parenthesis.&amp;lt;cylinder radius=&quot;${wheeldiam/2}&quot; length=&quot;0.1&quot;/&amp;gt;&amp;lt;origin xyz=&quot;${reflect*(width+.02)} 0 0.25&quot; /&amp;gt;&amp;lt;box size=&quot;${cos(pi/6)}$&quot;&amp;gt;MacrosSimple MacroThe steup&amp;lt;xacro:macro name=&quot;default_origin&quot;&amp;gt;    &amp;lt;origin xyz=&quot;0 0 0&quot; rpy=&quot;0 0 0&quot;/&amp;gt;&amp;lt;/xacro:macro&amp;gt;The usage:&amp;lt;xacro:default_origin /&amp;gt;The output:&amp;lt;origin rpy=&quot;0 0 0&quot; xyz=&quot;0 0 0&quot;/&amp;gt;  Every instance of the &amp;lt;xacro:$NAME /&amp;gt; is replaced with the contents of the xacro:macro tag.  Note that even though its not exactly the same (the two attributes have switched order), the generated XML is equivalent.  If the xacro with a specified name is not found, it will not be expanded and will NOT generate an error.Macro with ParamtersThe setup:&amp;lt;xacro:macro name=&quot;blue_shape&quot; params=&quot;name *shape&quot;&amp;gt;    &amp;lt;link name=&quot;${name}&quot;&amp;gt;        &amp;lt;visual&amp;gt;            &amp;lt;geometry&amp;gt;                &amp;lt;xacro:insert_block name=&quot;shape&quot; /&amp;gt;            &amp;lt;/geometry&amp;gt;            &amp;lt;material name=&quot;blue&quot;/&amp;gt;        &amp;lt;/visual&amp;gt;        &amp;lt;collision&amp;gt;            &amp;lt;geometry&amp;gt;                &amp;lt;xacro:insert_block name=&quot;shape&quot; /&amp;gt;            &amp;lt;/geometry&amp;gt;        &amp;lt;/collision&amp;gt;    &amp;lt;/link&amp;gt;&amp;lt;/xacro:macro&amp;gt;The usage:&amp;lt;xacro:blue_shape name=&quot;base_link&quot;&amp;gt;songs    &amp;lt;cylinder radius=&quot;.42&quot; length=&quot;.01&quot; /&amp;gt;&amp;lt;/xacro:blue_shape&amp;gt;The result:    &amp;lt;link name=&quot;base_link&quot;&amp;gt;        &amp;lt;visual&amp;gt;            &amp;lt;geometry&amp;gt;                &amp;lt;cylinder radius=&quot;.42&quot; length=&quot;.01&quot; /&amp;gt;            &amp;lt;/geometry&amp;gt;            &amp;lt;material name=&quot;blue&quot;/&amp;gt;        &amp;lt;/visual&amp;gt;        &amp;lt;collision&amp;gt;            &amp;lt;geometry&amp;gt;                &amp;lt;cylinder radius=&quot;.42&quot; length=&quot;.01&quot;/&amp;gt;            &amp;lt;/geometry&amp;gt;        &amp;lt;/collision&amp;gt;    &amp;lt;/link&amp;gt;  To specify a block parameter, include an asterisk before its parameter name.  A block can be inserted using the insert_block commandThe setup:&amp;lt;xacro:macro name=&quot;pr2_caster&quot; params=&quot;suffix *origin **content **anothercontent&quot;&amp;gt;  &amp;lt;joint name=&quot;caster_${suffix}_joint&quot;&amp;gt;    &amp;lt;axis xyz=&quot;0 0 1&quot; /&amp;gt;  &amp;lt;/joint&amp;gt;  &amp;lt;link name=&quot;caster_${suffix}&quot;&amp;gt;    &amp;lt;xacro:insert_block name=&quot;origin&quot; /&amp;gt;    &amp;lt;xacro:insert_block name=&quot;content&quot; /&amp;gt;    &amp;lt;xacro:insert_block name=&quot;anothercontent&quot; /&amp;gt;  &amp;lt;/link&amp;gt;&amp;lt;/xacro:macro&amp;gt;The usage:&amp;lt;xacro:pr2_caster suffix=&quot;front_left&quot;&amp;gt;  &amp;lt;pose xyz=&quot;0 1 0&quot; rpy=&quot;0 0 0&quot; /&amp;gt;  &amp;lt;container&amp;gt;    &amp;lt;color name=&quot;yellow&quot;/&amp;gt;    &amp;lt;mass&amp;gt;0.1&amp;lt;/mass&amp;gt;  &amp;lt;/container&amp;gt;  &amp;lt;another&amp;gt;    &amp;lt;inertial&amp;gt;      &amp;lt;origin xyz=&quot;0 0 0.5&quot; rpy=&quot;0 0 0&quot;/&amp;gt;      &amp;lt;mass value=&quot;1&quot;/&amp;gt;      &amp;lt;inertia ixx=&quot;100&quot;  ixy=&quot;0&quot;  ixz=&quot;0&quot; iyy=&quot;100&quot; iyz=&quot;0&quot; izz=&quot;100&quot; /&amp;gt;    &amp;lt;/inertial&amp;gt;  &amp;lt;/another&amp;gt;&amp;lt;/xacro:pr2_caster&amp;gt;The result:&amp;lt;joint name=&quot;caster_front_left_joint&quot;&amp;gt;  &amp;lt;axis xyz=&quot;0 0 1&quot; /&amp;gt;&amp;lt;/joint&amp;gt;&amp;lt;link name=&quot;caster_front_left&quot;&amp;gt;  &amp;lt;pose xyz=&quot;0 1 0&quot; rpy=&quot;0 0 0&quot; /&amp;gt;  &amp;lt;color name=&quot;yellow&quot; /&amp;gt;  &amp;lt;mass&amp;gt;0.1&amp;lt;/mass&amp;gt;  &amp;lt;inertial&amp;gt;    &amp;lt;origin xyz=&quot;0 0 0.5&quot; rpy=&quot;0 0 0&quot;/&amp;gt;    &amp;lt;mass value=&quot;1&quot;/&amp;gt;    &amp;lt;inertia ixx=&quot;100&quot;  ixy=&quot;0&quot;  ixz=&quot;0&quot; iyy=&quot;100&quot; iyz=&quot;0&quot; izz=&quot;100&quot; /&amp;gt;  &amp;lt;/inertial&amp;gt;&amp;lt;/link&amp;gt;ROS ControlComing soon…"
  },
  {
    "title": "Lite Bar",
    "url": "/posts/LiteBar/",
    "categories": "Projects, DIY",
    "tags": "ws2812b, esp32, mic, bluetooth",
    "date": "2021-12-15 19:13:20 +0530",
    "snippet": "IntroductionA multipurpose indivudially addressable LED Strip based tubelight, that can be used to visualize music, time, smartphone notifications.Hardware  ESP32  1 Meter WS2812B Strip (144 LEDS per meter density)  Microphone Module  SMPS  RTC Module  LED Tubelight + holder  Basic power toolsOverview            Build details and an overview of the programming will be updated soon.Results"
  },
  {
    "title": "POV - Lightsaber",
    "url": "/posts/POV-lightsaber/",
    "categories": "Projects, DIY",
    "tags": "ws2812b, POV, imu",
    "date": "2021-12-10 19:13:20 +0530",
    "snippet": "A persistance of vision lightsaber heavly inspired by bitluni(original build).IntroductionA LED Lightsaber equipped with an onboard IMU and ESP32 to detect the current angle of the lightsaber and accordingly light up the LED’s to display images in the air as the saber is swung around.Hardware  ESP32  1 Meter WS2812B Strip (144 LEDS per meter density)  Buck Converter  3S 35C 2200mAh Lipo  MPU6050  PVC Pipe  Wooden strip  Basic power tools    Software    Given that the lightsaber had 144LEDs and each LED is supposed to represent the color of 1 pixel, we needed to scale images to 128x128 pixels before they could be used in the lightsaber  Since the pivot point of the lightsaber will be at the bottom center of the image, the side length of the image, given its diagonal size is fixed at 144, turns out to be 128, as shown in the figure below.The current angle of the lightsaber is calculated using the MPU6050. The LED colors’ corresponding values are picked by overlaying a straight line over the image array and extracting the pixels that coincide with the above-mentioned imaginary line.Results"
  },
  {
    "title": "Aerial Navigation in GPS Denied Environments",
    "url": "/posts/Orthomosaic-SLAM/",
    "categories": "Projects, DIY",
    "tags": "atmega328p, game, imu, touch",
    "date": "2021-10-18 19:13:20 +0530",
    "snippet": "A project on achieving realtime-orthomosaic SLAM for aerial navigation using a single downwards facing camera in outdoor GPS denied environments.Results              IntroductionTo carry out drone-based aerial surveying for generating orthomosaic maps on the fly, this project explores the image processing stack required to achieve the same using the most economical hardware and software footprint. The project explores corner and blob-based feature extraction techniques followed by brute force and KNN based feature matching methods which are later used to generate a homography matrix for stitching images passed through a cascaded image mixer to generate orthomosaic maps of a given dataset.Feature Detection &amp;amp; ExtractionWhile there is no universally accepted definition of “Feature” for a given image, it is often regarded as the information that is unique for the given image and thus helps us mathematically associate the image with its unique data properties.Image Features are small patches of unique raw data that can be potentially used to differentiate the given image from any other image, therefore, helping in tracking the similarity between given images. Image Features can be broken down into two major components:1) Keypoints2) DescriptorThey are explained in the subsequent sections.KeypointsKeypoints contain 2D patch data like position, scale, convergence area, and other properties of the local patch. Which we define as a distinctive point in an input image that is invariant to rotation, scale, and distortion. In practice, the key points are not perfectly invariant but they are a good approximation. [1]Example: Keypoints of a human hand.Descriptors &amp;amp; DetectorsA feature detector is an algorithm that takes an image and produces the positions (pixel coordinates) of important regions in the picture. A corner detector is an illustration of this, since it outputs the positions of corners in your picture but does not provide any other information about the features discovered.A feature descriptor is an algorithm that takes an image and generates feature descriptors/feature vectors from it.[2] Feature descriptors encapsulate important information into a sequence of numbers and serve as a numerical “fingerprint” that may be used to distinguish one feature from another. They highlight various keypoint properties in vector format (constant length). It might be their intensity in the direction of their most pronounced orientation, for example. It is giving a numerical description of the area of the image to which the keypoint is referenced.Important properties of descriptors      they should be independent of keypoint positionIf the same keypoint is extracted at different positions (e.g. because of translation) the descriptor should be the same. [3]    they should be robust against image transformationsVariations in contrast (for example, a photograph of the same location on a bright and overcast day) and changes in perspective are two examples (image of a building from center-right and center-left, we would still like to recognize it as the same building). Of all, no description is absolutely impervious to all modifications (nor against any single one if it is strong, e.g. big change in perspective). Different descriptors are meant to be resistant against various transformations, which are sometimes incompatible with the speed at which they are calculated. [3]  they should be scale independentThe scale should be considered in the descriptions. If the “prominent” component of one key point is a vertical line of 10px (within a circular area with a radius of 8px), and the prominent part of another is a vertical line of 5px (inside a circular area with a radius of 4px), the descriptions for both key points should be identical. [3]Available MethodsShi-Tomasi Corner DetectorExample: Using Good Features to Track to detect keypointsThe Shi-Tomasi Corner Detector is a modified version of the Harris Corner Detector with the primary change being in the way in which the score “R” is calculated making it more efficient than the standard Harris Corner Detector.Corners are considered as areas in photographs where a little alteration in position, results in a significant change in intensity in both the horizontal (X) and vertical (Y) axes.The Pseudocode for Shi-Tomasi Corner Detector:  Determine windows of small image patches that produce very large variations in intensity when moved in both X and Y directions (i.e. gradients).[4]  Compute score R for each such window.3, Apply a threshold to select and retrieve important corner points.Python Implementation of Shi-Tomasi Corner Detector:import cv2import numpy as npimg_1 = cv2.imread(&#39;images/admin_block.jpg&#39;)gray = cv2.cvtColor(img_1, cv2.COLOR_BGR2GRAY)cv2.namedWindow(&quot;output&quot;, cv2.WINDOW_NORMAL)corners = cv2.goodFeaturesToTrack(gray, 1000, 0.01, 1)corners = np.int0(corners)for i in corners:x, y = i.ravel()cv2.circle(img_1, (x, y), 3, (255, 0, 255), -1)cv2.imshow(&quot;output&quot;, img_1)cv2.waitKey(0)cv2.destroyAllWindows()OpenCV has implemented a function cv2.goodFeaturesToTrack() the parameters for this function are:  image - Input 8-bit or floating-point 32-bit, single-channel image  maxCorners - Maximum number of corners to detect. If the number of corners on an image is higher than the number of maxCorners the most pronounced corners are selected.  qualityLevel - Parameter characterizing the minimal accepted quality of image corners. All corners below the quality level are rejected. The parameter value is multiplied by the best corner quality measure, which is the minimal eigenvalue of the Harris function response.[5]  minDistance - minimal Euclidean distance between cornersScale Invariant Feature Transform (SIFT)Example: Using SIFT to detect keypointsSIFT, which stands for Scale-Invariant Feature Transform, was introduced in 2004 by D.Lowe of the University of British Columbia. This algorithm is robust against image scale variations and rotation invariances.The Pseudocode for SIFT:  Scale-space peak selection: Potential location for finding features.  Keypoint Localization: Accurately locating the feature keypoints.  Orientation Assignment: Assigning orientation to keypoints.  Keypoint descriptor: Describing the keypoints as a high dimensional vector.  Keypoint MatchingThe Python Implementation for SIFT:import cv2import numpy as npimg = cv2.imread&#39;(images/admin_block.jpg&#39;)img_gray = cv2.cvtColor(img, cv2.COLOR_BRG2GRAY)cv2.namedWindow(&quot;output&quot;, cv2.WINDOW_NORMAL)sift = cv2.SIFT_create()kypoints = sift.detect(img_gray, None)cv2.imshow(&quot;output&quot;, cv2.drawKeypoints(img, keypoints, None, (255, 0, 255)))cv2.waitKey(0)cv2.destroyAllWindows()Parameters for SIFT:  nfeatures - The number of best features to retain. The features are ranked by their scores (measured in SIFT algorithm as the local contrast)  nOctaveLayers - The number of layers in each octave. 3 is the value used in D. Lowe’s paper. The number of octaves is computed automatically from the image resolution.  contrastThreshold - The contrast threshold is used to filter out weak features in semi-uniform (low-contrast) regions. The larger the threshold, the fewer features are produced by the detector.Speeded Up Robust Features (SURF)The SURF (Speeded Up Robust Features) technique is a quick and robust approach for local, similarity invariant picture representation and comparison. The SURF approach’s prominent feature is its quick computing of operators using box filters, which enables real-time applications such as tracking and object identification.The Pseducode for SURF:  Feature Extraction          Hessian matrix-based interest points      Scale-space representation        Feature Description          Orientation Assignment      Descriptor Components      The python implementation of SURF:import cv2import numpy as npimg = cv2.imread&#39;(images/admin_block.jpg&#39;)img_gray = cv2.cvtColor(img, cv2.COLOR_BRG2GRAY)cv2.namedWindow(&quot;output&quot;, cv2.WINDOW_NORMAL)surf = cv2.xfeatures2d.SURF_create()kypoints = surf.detect(img_gray, None)cv2.imshow(&quot;output&quot;, cv2.drawKeypoints(img, keypoints, None, (255, 0, 255)))cv2.waitKey(0)cv2.destroyAllWindows()public static SURF create​(double hessianThreshold, int nOctaves, int nOctaveLayers, boolean extended, boolean upright)Parameters for SURF:  hessianThreshold - Threshold for hessian keypoint detector used in SURF.  nOctaves - Number of pyramid octaves the keypoint detector will use.  nOctaveLayers - Number of octave layers within each octave.  extended - Extended descriptor flag (true - use extended 128-element descriptors; false - use 64-element descriptors).  upright - Up-right or rotated features flag (true - do not compute the orientation of features; false - compute orientation).Oriented FAST and Rotated BRIEF (ORB)Example: Using ORB to detect keypointsORB is a combination of the FAST keypoint detector and the BRIEF descriptor, with some additional characteristics to boost performance. FAST stands for Features from Accelerated Segment Test, and it is used to find features in a picture. It also employs a pyramid to generate multiscale features.[2] It no longer computes the orientation and descriptors for the features, which is where BRIEF is used.ORB employs BRIEF descriptors, although the BRIEF performs badly when rotated. So what ORB does is rotate the BRIEF according to the keypoint orientation. The rotation matrix of the patch is calculated using the patch’s orientation, and the BRIEF is rotated to obtain the rotated version.The Pseudocode for ORB:[7]  Take the query image and convert it to grayscale.  Now Initialize the ORB detector and detect the keypoints in query image and scene.  Compute the descriptors belonging to both images.  Match the keypoints using Brute Force Matcher.  Show the matched images.The Python implementation of ORB:import cv2import numpy as npimg = cv2.imread&#39;(images/admin_block.jpg&#39;)img_gray = cv2.cvtColor(img, cv2.COLOR_BRG2GRAY)cv2.namedWindow(&quot;output&quot;, cv2.WINDOW_NORMAL)orb = cv2.ORB_create(nfeatures = 1000)kypoints_orb, descriptors = orb.detectAndCompute(img_1, None)cv2.imshow(&quot;output&quot;, cv2.drawKeypoints(img_1, keypoints_orb, None, (255, 0, 255)))cv2.waitKey(0)cv2.destroyAllWindows()Conclusion  One of the notable drawbacks of the Shi-Tomasi Corner Detector is the fact that it is not scale-invariant.  ORB is a more efficient feature extraction approach than SIFT or SURF in terms of computing cost, matching performance, and, most importantly, patents.  SIFT and SURF are patented, and you are required to pay for their use. However, ORB is not patented.    Feature Matching    Available Methods    Brute Force Matcher      The Brute Force Matcher is used to compare the characteristics of the first image to those of another image.It takes one of the first picture’s descriptors and matches it to all of the second image’s descriptors, then moves on to the second descriptor of the first image and matches it to all of the second image’s descriptors, and so on.Homography &amp;amp; TransformationA homography connects any two photographs of the same scene. It is a transformation that transfers the points in one picture to the points in the other. The two pictures can be captured by rotating the camera along its optical axis or by laying them on the same surface in space.The essence of the homography is the simple 3×3 matrix called the homography matrix.This matrix may be applied to any spot in the picture. For example, in the first image, if we choose a position A(x1,y1), we may use a homography matrix to translate this point A to the matching point B(x2,y2) in the second image.The python implementation of finding homography matrix:def wrap_images(self, image1, image2, H):    rows1, cols1 = image1.shape[:2]    rows2, cols2 = image2.shape[:2]    H = H    list_of_points_1 = np.float32(        [[0, 0], [0, rows1], [cols1, rows1], [cols1, 0]]).reshape(-1, 1, 2)    temp_points = np.float32(        [[0, 0], [0, rows2], [cols2, rows2], [cols2, 0]]).reshape(-1, 1, 2)    # When we have established a homography we need to warp perspective    # Change field of view    list_of_points_2 = cv2.perspectiveTransform(temp_points, H)    list_of_points = np.concatenate(        (list_of_points_1, list_of_points_2), axis=0)    [x_min, y_min] = np.int32(list_of_points.min(axis=0).ravel() - 0.5)    [x_max, y_max] = np.int32(list_of_points.max(axis=0).ravel() + 0.5)    translation_dist = [-x_min, -y_min]    H_translation = np.array([[1, 0, translation_dist[0]], [        0, 1, translation_dist[1]], [0, 0, 1]])    output_img = cv2.warpPerspective(        image2, H_translation.dot(H), (x_max-x_min, y_max-y_min))    output_img[translation_dist[1]:rows1+translation_dist[1],        translation_dist[0]:cols1+translation_dist[0]] = image1    return output_imgIt is important to realize that feature matching does not always provide perfect matches. As a result, the Random Sample Consensus (RANSAC) process is used as a parameter in the cv2.findHomography() method, making the algorithm robust to outliers. We can get reliable results using this strategy even if we have a large percentage of bad matches.Python Implementation of RANSAC:# Set minimum match conditionMIN_MATCH_COUNT = 2if len(good) &amp;gt; MIN_MATCH_COUNT:    # Convert keypoints to an argument for findHomography    src_pts = np.float32(    [keypoints1[m.queryIdx].pt for m in good]).reshape(-1, 1, 2)    dst_pts = np.float32(    [keypoints2[m.trainIdx].pt for m in good]).reshape(-1, 1, 2)    # Establish a homography    M, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)    result = self.wrap_images(image2, image1, M)    # cv2.imwrite(&#39;test4.jpg&#39;,result)    # cv2.imshow(&quot;output_image&quot;,result)    # cv2.waitKey(0)    # cv2.destroyAllWindows()    return resultelse:    print(&quot;Error&quot;)Image Processing PipelineDatasetUrban AreaThis dataset of the Thammasat University campus in Bangkok, Thailand was collected by a senseFly eBee X drone carrying a senseFly Aeria X photogrammetry camera.Technical Details|Feature|Value||–|–|Ground resolution|6 cm (3.36 in)/pxCoverage|2.1 sq. km (0.77 sq. mi)Flight height|285 m (935 ft)Number of images|443Forest AreaDroneMapper flew Greg 1 and 2 reservoirs on September 16th, 2019 using their Phantom 3 Advanced drone to collect imagery for precision digital elevation model (DEM) and orthomosaic generation of the site. GCP was surveyed with a Trimble 5800 and used during the image processing. Approximately 189 images were collected and processed via photogrammetry to yield the DEM, orthomosaic, and capacity report.[8]            Feature      Value                  Ground resolution      6 cm (3.36 in)/px              Coverage      2.1 sq. km (0.77 sq. mi)              Flight height      285 m (935 ft)              Number of images      189      Future WorkUse it for realtime localization in Gazebo."
  },
  {
    "title": "Co-Axial Bi-Copter",
    "url": "/posts/CoAxial-Bicopter/",
    "categories": "Projects, DIY",
    "tags": "atmega328p, game, imu, touch",
    "date": "2021-10-18 19:13:20 +0530",
    "snippet": "A project to explore control strategies for thrust vector control of anco-axial propeller with four control surfaces.Note: The project is in progress, further details/sections will be added as soon as the project is completed.Hardware  Arduino Nano 33 BLE Sense  2300kv BLDC  8 inch props  2500 mAh Lipo  45A ESC  CT6B  MG90 Mini Servo    Design    Initial Prototype          The initial prototype was made using 1000kv BLDC motors along with 13inch props, as these were the cheapest materials that were available during the pandemic. But the length and width of the support frame was very large which increases the complexity of getting the frame 3D printed.Final PrototypeThe final prototype had half the width of the initial prototype and each of the support frame elements were designed so that they can be finished in a single 3d print so that there are no strucutral issues during flight.     Monocopter V2  by  textzip  on SketchfabCurrent Work  Simulation in Gazebo/Webots  Writing code for the flight controller  Implementing MPC and other control algorithms"
  },
  {
    "title": "Quasi Direct Drive Actuator",
    "url": "/posts/QDD/",
    "categories": "Projects, DIY",
    "tags": "atmega328p, game, imu, touch",
    "date": "2021-10-16 19:13:20 +0530",
    "snippet": "An open-source Quasi Direct Drive Actuator for use in locomotion centric projects.Note: The project is ongoing, the blog post will be updated as the project progresses.Hardware  400kv BLDC  GEABN0.5-20-8-K-6  SI0.5-100  HK0408  SSG0.5-40B  SB6700ZZDesignEarly Sketches3D DesignsIn Figure: Cross-section view of the actuatorIn Figure: Exploded view of the actuatorIn Figure: Sketch of the outer casing for the actuatorIn Figure: Final actuator designMotion StudyDemoImplementationFuture Work[ ] Programming and Control"
  },
  {
    "title": "Plug-and-Play Oxygen Auto-flow Regulator for Low Flow Oxygen Therapy",
    "url": "/posts/COVID19/",
    "categories": "Projects, DIY",
    "tags": "atmega328p, game, imu, touch",
    "date": "2021-07-14 13:13:20 +0530",
    "snippet": "A portable, easy to interface device that intelligently controls the oxygen flow rate based on oxygen saturation and other vitals.This work has been published, please find the full details in the paper.  B. M, A. B. Pattaje, A. M. Krishna, B. J. Krishna, P. Chakraborty and A. M. Parimi, “Plug-and-Play Oxygen Auto-flow Regulator for Low Flow Oxygen Therapy: A Prototype Development,” 2022 IEEE International Instrumentation and Measurement Technology Conference (I2MTC), Ottawa, ON, Canada, 2022, pp. 1-6, doi: 10.1109/I2MTC48687.2022.9806550.Problem StatementExisting SolutionsDesignSchematicHardwareSoftwareFeaturesMethodologyThe device automatically regulates the oxygen flow rate supplied to the patient based on the current and past trends of the patient’s vitals(oxygen saturation levels). These vitals are sampled periodically through a discrete-time control algorithm that has been explained in the next section.The discrete-time control algorithm consists of two parts, a state machine that determines the minimum and maximum oxygen flow rate provided to the patient based only on their current oxygen saturation levels, and an Integral-Derivative control loop that calculates the exact flow rate within the range prescribed by the state machine taking into account the past trends of the oxygen saturation levels.The state machine divides the interval of flow rates from 2 LPM to 15 LPM (both inclusive) into four stages. Based on the current SpO2 reading of the patient, the state machine determines which stage to use to set the boundaries. The base flow rates for each stage are as tabulated below.            Base No.      Min. $SPO_2$ %      Max. $SPO_2$ %      $Base_{LPM}$                  1      87      99      1              2      80      87      2              3      70      80      5              4      0      70      10      The exact SpO2 readings are then processed using the formula given below:\\[Tot_{LPM} = min[(Base_{n}+I_{LPM}+D_{LPM}),Base_{n+1}]\\]There are three parts to this formula:Base $_{n}$ : The base flow rate of the $n$th base obtained from the state machine.D $_{LPM}$ : A derivative term, which increases its value when a negative trend is observed in SpO2 readings.I $_{LPM}$ : An integral term, which increases its value when a stationary trend is observed while the SpO2 readings are less than 90%.\\[\\begin{equation}    D_{LPM}=\\left(\\frac{|\\Delta SpO_{2}(t)|}{|\\Delta SpO_{2}(t)|+SpO_{2_{width}}(t)}\\right)\\times Base_{width}\\end{equation}\\\\\\]—\\(\\begin{align}    &amp;amp;\\Delta SpO_{2}(m) = SpO_{2}(m) - SpO_{2} (m-1)\\\\    &amp;amp;Base_{width}(t)= B_{n}(m)-B_{n-1}(m)\\\\    &amp;amp;SpO_{2_{width}}(m)=(SpO_{2}(m)-\\textrm{Min. $SpO_{2}$ of $B_{n}$})\\end{align}\\)Derivative Term:When \\(SPO_2(t-1) &amp;gt;SPO_2(t)\\)\\[Base_{width} =CurrentBase_{LPM} - (Current-1)Base_{LPM}\\]\\[width = |\\Delta SPO_2| + (SPO_2(t)-Min. SPO_2\\ of\\ Current Base)\\]\\[Derivative_{LPM}=\\dfrac {|\\Delta SPO2|} {width} Base_{width}\\]When the SOP2 value is stationary for a considerable amount of time, a counter is initialized that is used by the $Intergral_{LPM}$ to update its value.Integral Term:When \\(SPO_2(t-1) =SPO_2(t)\\)\\[stationary_{counter} += 1\\]\\[Base_{width}=Current Base_{LPM} - (Current-1) Base_{LPM}\\]\\[Integral_{LPM}=Base_{width}  (stationary_{counter}-1)/10\\]These three terms are summed up, and a saturation operation is performed, capping the flow rate at the base flow rate of the next higher stage.In this method, our device automatically adjusts the flow rate as required without any human supervision, thus helping avoid the wastage of precious O2 reserves. We have also provided a manual mode, using which the attendants or nurses can set the flow rate to a constant value and an option to vary the control algorithm parameters.CAD ModelClinical Testing"
  },
  {
    "title": "Dino Game Arduino Edition",
    "url": "/posts/Dino-Game-Arduino/",
    "categories": "Projects, DIY",
    "tags": "TFT, Dino, Arduino, game",
    "date": "2021-06-14 01:10:20 +0530",
    "snippet": "IntroductionI always had a thing for wearable electronics, a few months ago I got a cheap TFT display to play around with and the ultimate goal was to get familiar with all the electronics so that I can try to scale everything down and make a (hopefully portable) smartwatch.I stumbled upon the flappy bird game for arduino by mrt-prodz while looking for some good projects with the TFT display, after playing the game for a couple of times, I decided to build my own version of the classic “Dino Game” from chromium.All the code files and assets for the project can be found in the following repo:Hardware &amp;amp; SchematicsComponents  Arduino Uno  Push Button  TFT Display 1.8 Inches (128x160)  Buzzer  A few jumpersSchematicThe TFT display I got is based on the ST7735 Driver IC while it also had an SD card slot I ended up saving all the required data in flash as I didn’t have an SD card at hand. The wiring for the TFT was a bit of a pain but thanks to help from Tweaking4all I finally got it running.The other connections for the buzzer and the push button are quite easy and require no explanation.I did notice that the flappy bird game didn’t actually use an interrupt based trigger but rather had a normal digitalRead function. I decided to use the internal pull and the external interrupt triggered ISR for servicing the user input via the digital pin 2.The Display uses SPI communication, so I ended up using the SPI hardware pins of the ardunio uno for communicating with the display. Tinkercad doesn’t have a model for the TFT display, so I just labeled the connections and dropped them onto a breadboard.My final setup with everything jam-packed into a tiny breadboard, as I didn’t have anything bigger than this with me.Another view of my TFT display for anyone’s referenceSoftwareThe software stack can be broken down into the following sub-tasksMake a box that jumpsThis step was supposed to be fairly easy and people actually use the euler’s integration method to give an object some force (acceleration) and then compute the velocity and displacement numerically using a very small dt.\\[acceleration = force / mass\\]\\[Delta_{position} = velocity * dt\\]\\[Delta_{velocity} = acceleration * dt\\]I for some reason ended up using the equation of motion of a projectile under constant acceleration.\\[y = ut + \\dfrac{1}{2}at^2\\]Everytime the button was pushed the box ended up with an initial velocity of $30 m/s$, I played around with the value of acceleration due to gravity and the initial velocity to get the right jump response.Things ended up looking like this (PS: don’t mind the moving cactus in the background)Draw cactus and make it move aroundI used some basic shapes (rounded rectangles) to make a cactus like object and gave it a constant velocity in the -ve x direction to make it look like the dino is moving forward.I used the brush method that was discussed in the mrt-prodz post for animations, which basically meant I erased the old pixels by drawing the background color over them and then drew the required colors over the new pixels, therefore I had to change a few selected pixels in each frame rather than the whole 128x160 of them.Replace box with a dinoI next tried replacing the box with a dino(bitmap), it looked like the dino was skating over the floor, and was quite artificial.I decided to make the dino walk as the original game does that as well, after snipping the two walking frames from the sprite sheet, I tried a simple code where I switched between the frames to make it look like the dino is walking.Walking vs Skating DinoThe above method was a partial success as the entire dino image was now having a weird flicker due to the update speed and I realised that I was updating the whole dino bitmap but most of the body was stationary and only the legs were supposed to move.I ended up fixing this by breaking the 2 dino frames into 2 parts each (4 in total), where the top part of each frame was the body and the bottom part was the leg, luckily the animation was setup in such a way that the body of the dino never actually changed between the two frames and only the leg positions were changed, so I fixed the body and only updated the leg parts of the dino, this eliminated most or if not all the flicker and I ended up with a very smooth looking animation.Adding Clouds, Sounds, Score and other minor detailsI now added other small details like sounds, intro and game over frames. I also added the score display and other features. I wanted to give a cloudy background for the game as the original has as it not only helps in the aesthetics but also in showing that the dino is moving relative to the background all the time.I tired using clouds from bitmaps but there was a bit too much of a flicker in the clouds and there were soo small that I their features hardly mattered, so I ended up replacing the bitmaps with regular rounded rectangles which improved the game performance a lot.ResultsAfter fixing a few more bugs I finally had a working dino gameFuture UpgradesWhile I don’t have immediate plans for improvements, I do have a few features in mind that I wana implement.  Add Lives  Dino has powers (Fire breathing)  New obstacles (Birds?)  Remove the flicker due to jumps"
  },
  {
    "title": "Manipulator Workspace Plotter",
    "url": "/posts/Manipulator-Workspace-Plotter/",
    "categories": "Projects, Manipulators",
    "tags": "matlab, forward kinematics, workspace, transformation matirx, monte carlo",
    "date": "2021-05-18 23:42:20 +0530",
    "snippet": "IntroductionI wrote this small script to tinker with DH Parameters and wanted to visualize and understand how joint limits affect the manipulator workspace. While there are many ways of generating the workspace of a manipulator, this particular sketch uses forward kinematics to do so.The full code for the project can be found in the following repoMethodThe idea of plotting workspace from forward kinematics can be broken down into the following smaller tasks  Capture the DH Table from the user  Create a Homogenous Transformation Matrix ideally(Base to Tool).  Take in user inputs for joint limits.  Create a dataset of sorts that contains random possible combinations from the possible joint limits.  Plug these random points into the Transformation Matrix and plot the outputs.A few other points:  There is no need to create a dataset with “all” possible combinations for the joint angles, this method will be computationally expensive. We will instead be using monte carlo distribution to generate random points.  Angle limits for roll joint at the end of the tool tip need not be considered as change in roll values doesn’t change the tool tip position.The Code ExplaninedCapture the DH Table from the userThe first step would be to capture the DH-Parameters for a given manipulators, this can be represented using a “M x 4 matrix” where M is the number of axis for the manipulator.We will be creating a matrix using symbolic variables as it would make it easier later for us to solve the matrix multiplications using variables that can later be substituted for.%% Homogeneous Matrix BuildernoParams = input(&#39;Enter the number of axis: &#39;);T = [1,0,0,0;0,1,0,0;0,0,1,0;0,0,0,1];flg = [0,0,0,0];A = sym(&#39;A%d%d&#39;,[noParams,4]);noParams here is used to decide the number of rows for the DH parameter table.A is the DH-Table MatrixOn its own for an input of “5” to noParams, the A matrix would look like this:&amp;gt;&amp;gt; A A = [A11, A12, A13, A14][A21, A22, A23, A24][A31, A32, A33, A34][A41, A42, A43, A44][A51, A52, A53, A54]The names of the column and row elements are due to the “A%d%d” in the A = sym(&#39;A%d%d&#39;,[noParams,4]);Create a Homogenous Transformation Matrix ideally(Base to Tool).We will now be creating a Homogenous symbolic transformation matrix with the above generated blank DH paramter matrix.  Rotation of $L_{k-1}$ about $Z_{k-1}$ is denoted by $\\theta_k$.  Translation of $L_{k-1}$ about $Z_{k-1}$ is denoted by $d_k$.  Translation of $L_{k-1}$ about $X_{k-1}$ is denoted by $a_k$.  Rotation of $L_{k-1}$ about $X_{k-1}$ is denoted by $\\alpha_k$. Therefore, any homogenous Transformation matrix T can be expressed as the following:\\(T_{k-1}^k(\\theta_k,d_k,a_k,\\alpha_k) = R(\\theta_k,z)T(d_k,z)T(a_k,x)R(\\alpha_k,x)\\)\\(T_{base}^{tool} = T_{base}^1T_1^2T_2^3...T_{n-1}^{tool}\\)    for i=1:1:noParams  Rz = [cos(A(i,1)),-sin(A(i,1)),0,0;sin(A(i,1)),cos(A(i,1)),0,0;0,0,1,0;0,0,0,1];  Tz = [1,0,0,0;0,1,0,0;0,0,1,A(i,2);0,0,0,1];  Tx = [1,0,0,A(i,3);0,1,0,0;0,0,1,0;0,0,0,1];  Rx = [1,0,0,0;0,cos(A(i,4)),-sin(A(i,4)),0;0,sin(A(i,4)),cos(A(i,4)),0;0,0,0,1];  T = T*(Rz*Tz*Tx*Rx);end        The next step after generating the DH table would be to fill the empty Transformation Matrix with the acutal values for the manipulator, this is done via user input.  This part of the code isn’t that straightforward, we will be taking one DH parameter at a time and the user needs to enter the row and column number of the DH paramter they are about to enter. The syntax for user input is [position,value,1/0]. Where a 1 would mean its the end of the inputs series and there are no more DH paramters to input, on the other hand 0 can be used to let the program know that there are more DH paramters that need to be entered.NOTE:  Users only need to enter DH-Paramters that are constant like “link length” and other values that cannot change.while flg(4) == 0        flg=input(&#39;Enter data as [position,value,1/0]: &#39;); %constants filler        T = vpa(subs(T,A(flg(1),flg(2)),flg(3)),4);    end	disp(&#39;Tool to Base Transformation Matrix created&#39;);    disp(T);After the Homogenous Transformation matrix is created, we can extract the tool-tip cordinate equations and convert them into a function so that we can substitue numerical values into the variables to get the tool tip postion.Tpos=[T(1,4),T(2,4),T(3,4)];Tpost = matlabFunction(Tpos);matlabFunction is used to convert symbolic expressions into matlab function handles.Take in user inputs for joint limitsWe will now be taking in user inputs for joint limits and create a dataset with random combinations of joint angles using monte carlo.  lim contains all the random datasets generated.  N=67312 is the number of random combinations that will be generated, this number can be increased or decresed based on the needs, remember that the higher the number the more dense the workspace plot will turn out, but it will also increase the computation time.  Dataset = lower limit + (upper limit - lower limit)*random number between 0 and 1ch2=input(&#39;Press 1 if you want to plot the 3D workspace&#39;);    if ch2 == 1         N = 67312;        range=input(&#39;Enter upper and lower limits of each joint. EXCLUDE ROLL JOINTS&#39;);        lim(N,size(range,1))=0;        for i=1:size(range,1)            lim(:,i) = range(i,1) + (range(i,2)-range(i,1))*rand(N,1);        end    endCreating a dataset of pointsThere is nothing much to explain here, we are just iterating through all the possible combinations of numbers that we generated and are storing them in f%% Plot Points Collector    f(size(lim,1),3) = 0;    if size(range,1) == 2        for j=1:size(lim,1)            f(j,:)=Tpost(lim(j,1),lim(j,2));        end    elseif size(range,1) == 3        for j=1:size(lim,1)            f(j,:)=Tpost(lim(j,1),lim(j,2),lim(j,3));        end    elseif size(range,1) == 4        for j=1:size(lim,1)            f(j,:)=Tpost(lim(j,1),lim(j,2),lim(j,3),lim(j,4));        end    elseif size(range,1) == 5        for j=1:size(lim,1)            f(j,:)=Tpost(lim(j,1),lim(j,2),lim(j,3),lim(j,4),lim(j,5));        end    elseif size(range,1) == 6        for j=1:size(lim,1)            f(j,:)=Tpost(lim(j,1),lim(j,2),lim(j,3),lim(j,4),lim(j,5),lim(j,6));        end    elseif size(range,1) == 7        for j=1:size(lim,1)            f(j,:)=Tpost(lim(j,1),lim(j,2),lim(j,3),lim(j,4),lim(j,5),lim(j,6),lim(j,7));        end    elseif size(range,1) == 8        for j=1:size(lim,1)            f(j,:)=Tpost(lim(j,1),lim(j,2),lim(j,3),lim(j,4),lim(j,5),lim(j,6),lim(j,7),lim(j,8));        end    elseif size(range,1) == 9        for j=1:size(lim,1)            f(j,:)=Tpost(lim(j,1),lim(j,2),lim(j,3),lim(j,4),lim(j,5),lim(j,6),lim(j,7),lim(j,8),lim(j,9));        end    endPlotting PointsWhile the plot is 3D we will plot a few standard views as moving the plot around when it is extremly dense is a computationally heavy task.%% Workspace Plotter    disp(&#39;Starting Workspace Plotter&#39;);    for i=1:N        plot3(f(i,1),f(i,2),f(i,3),&#39;.&#39;)        hold on;    end    view(3);    title(&#39;Isometric view&#39;);    xlabel(&#39;x (m)&#39;);    ylabel(&#39;y (m)&#39;);    zlabel(&#39;z (m) &#39;);    view(2); % top view    title(&#39; Top view&#39;);    xlabel(&#39;x (m)&#39;);    ylabel(&#39;y (m)&#39;);    view([0 1 0]); % y-z plane    title(&#39;Side view, Y-Z&#39;);    ylabel(&#39;y (m)&#39;);    zlabel(&#39;z (m)&#39;);    disp(&#39;Plotting Complete&#39;);    NOTEThis version of the documentation is based on the DHPnP_v1.m which is based on the monte carlo method for random point sampling. The older version of the same project is based on a different approach and is significantly slower and more computational expenseive and memory intensive.ResultsA few images from both the versions which clearly shows the difference in the resulting workspace plots.          "
  },
  {
    "title": "TF101",
    "url": "/posts/ROS-TF101/",
    "categories": "Resources, ROSCheatSheet",
    "tags": "ros, tf",
    "date": "2021-05-04 16:17:25 +0530",
    "snippet": "Contents: Visulization of TF, Publishing, Broadcasting &amp;amp; Listening to TF, Static Transform, Joint and Robot State Publishers.A transform specifies how data expressed in a frame can be transformed into a different frame. For instance, if you detect an obstacle with the laser at 3 cm in the front, this means that it is 3 cm from the laser, but not from the center of the robot (that is usually called the /base_link). To know the distance from the center of the robot, you need to transform the 3 cm from the /laser_frame to the /base_link frame (which is actually what the Path Planning system needs to know, what is the distance from the center of the robot to the obstacle).First, we’ll define two frames (coordinate frames), one at the center of the laser and another one at the center of the robot. Usually, the center of the robot is placed at the rotational center of the robot. We’ll name the laser frame as base_laser and the robot frame as base_link.Now, we need to define a relationship (in terms of position and orientation) between the base_laser and the base_link. For instance, we know that the base_laser frame is at a distance of 20 cm in the y axis and 10 cm in the x axis referring the base_link frame. Then we’ll have to provide this relationship to the robot. This relationship between the position of the laser and the base of the robot is known in ROS as the TRANSFORM between the laser and the robot.Visulization of TFView_framesThe view_frames ROS node generates a diagram with the current TF tree of the system.rosrun tf view_framesrqt_tf_treerqt_tf_tree gives the same functionality as the view_frames, with an interesting extra: you can refresh and see changes without having to generate another PDF file each time.rosrun rqt_tf_tree rqt_tf_treeUsing echo /tfThere is a topic named /tf where all the TFs are published. The only problem is that ALL of them are published. In simple systems like this one, that isn’t a problem; but in massive systems, the quantity of data can be overwhelming. Therefore, the tf package gives a handy tool that filters which transform you are interested in and just shows you that one.rostopic echo -n1 /tfUsing tf_echoThe /tf topic only publishes the direct TFs, not all the transforms between all the frames. tf_echo does return the transforms between any connected frames to you.rosrun tf tf_echo [reference_frame] [target_frame]TF Publisher, Broadcaster, Listener, SubscriberTF Publisher#!/usr/bin/env python  import rospyimport tf2_rosfrom tf2_msgs.msg import TFMessagefrom geometry_msgs.msg import TransformStampedrospy.init_node(&#39;fixed_tf2_broadcaster&#39;)pub_tf = rospy.Publisher(&quot;/tf&quot;,TFMessage, queue_size=1)message = TFMessage()while not rospy.is_shutdown():     # Run this loop at about 10Hz    rospy.sleep(0.1)    t1 = TransformStamped()    t1.header.frame_id = &quot;turtle1&quot;    t1.header.stamp = rospy.Time.now()    t1.child_frame_id = &quot;carrot1&quot;    t1.transform.translation.x = 0.0    t1.transform.translation.y = 2.0    t1.transform.translation.z = 0.0    t1.transform.rotation.x = 0.0    t1.transform.rotation.y = 0.0    t1.transform.rotation.z = 0.0    t1.transform.rotation.w = 1.0    t2 = TransformStamped()    t2.header.frame_id = &quot;turtle1&quot;    t2.header.stamp = rospy.Time.now()    t2.child_frame_id = &quot;carrot2&quot;    t2.transform.translation.x = 0.0    t2.transform.translation.y = 4.0    t2.transform.translation.z = 0.0    t2.transform.rotation.x = 0.0    t2.transform.rotation.y = 0.0    t2.transform.rotation.z = 0.0    t2.transform.rotation.w = 1.0    message = [t1,t2]    pub_tf.publish(message)rospy.spin()TF Broadcaster Example - 1def handle_turtle_pose(pose_msg, robot_name):    br = tf2_ros.TransformBroadcaster()    br.sendTransform((pose_msg.position.x,pose_msg.position.y,pose_msg.position.z),                     (pose_msg.orientation.x,pose_msg.orientation.y,pose_msg.orientation.z,pose_msg.orientation.w),                     rospy.Time.now(),                     robot_name,                     &quot;/world&quot;)You have to publish each element of the position and orientation inside a parenthesis, otherwise it might not work.There is also a very important element, which is the rospy.Time.now(). This is because TF really depends on time to make everything work and be able to play with past messages. Then, state the name of the child-frame you want to assign that model (robot_name) and the parent-frame, which, in this case, is /world.TF Broadcaster Example - 2#!/usr/bin/env python  import rospyimport tfimport mathif __name__ == &#39;__main__&#39;:    rospy.init_node(&#39;my_moving_carrot_tf_broadcaster&#39;)    br = tf.TransformBroadcaster()    rate = rospy.Rate(5.0)    turning_speed_rate = 0.1    while not rospy.is_shutdown():        t = (rospy.Time.now().to_sec() * math.pi)*turning_speed_rate        # Map to only one turn maximum [0,2*pi)        rad_var = t % (2*math.pi)        br.sendTransform((1.0 * math.sin(rad_var), 1.0 * math.cos(rad_var), 0.0),                         (0.0, 0.0, 0.0, 1.0),                         rospy.Time.now(),                         &quot;moving_carrot&quot;,                         &quot;turtle2&quot;)        rate.sleep()TF Broadcaster from Pose for turtlesim_nodeNOTE: Requires setting of ROS params in launch file as shown below&amp;lt;launch&amp;gt;    &amp;lt;!--To get the Pose from turtle bot we have to launch turtlesim_node--&amp;gt;    &amp;lt;node pkg=&quot;turtlesim&quot; type=&quot;turtlesim_node&quot; name=&quot;sim&quot;/&amp;gt;    &amp;lt;node name=&quot;turtle1_tf2_broadcaster&quot; pkg=&quot;learning_tf2&quot; type=&quot;turtle_tf2_broadcaster.py&quot; respawn=&quot;false&quot; output=&quot;screen&quot; &amp;gt;        &amp;lt;param name=&quot;turtle&quot; type=&quot;string&quot; value=&quot;turtle1&quot; /&amp;gt;    &amp;lt;/node&amp;gt;&amp;lt;/launch&amp;gt;#! /usr/bin/env pythonimport rospyfrom geometry_msgs.msg import TransformStampedfrom turtlesim.msg import Poseimport tf_conversionsimport tf2_rosdef callback_function(pose):    global turtlename    br = tf2_ros.TransformBroadcaster()    t = TransformStamped()    t.header.stamp = rospy.Time.now()    t.header.frame_id = &#39;world&#39;    t.child_frame_id = turtlename    t.transform.translation.x = pose.x    t.transform.translation.y = pose.y    t.transform.translation.z = 0    q = tf_conversions.transformations.quaternion_from_euler(0,0,pose.theta)    t.transform.rotation.x = q[0]    t.transform.rotation.y = q[1]    t.transform.rotation.z = q[2]    t.transform.rotation.w = q[3]    br.sendTransform(t)if __name__== &#39;__main__&#39;:    rospy.init_node(&#39;tf2_broadcaster&#39;)    turtlename = rospy.get_param(&#39;~turtle&#39;)    rospy.Subscriber(&#39;/%s/pose&#39;% turtlename,Pose,callback_function)    rospy.spin()TF Listener#!/usr/bin/env python  import rospyimport tfimport mathimport tf2_rosfrom geometry_msgs.msg import Twistif __name__ == &#39;__main__&#39;:    rospy.init_node(&#39;tf2_turtle_listener&#39;)    tfBuffer = tf2_ros.Buffer()    tf2_ros.TransformListener(tfBuffer)    turtle_name = &quot;turtle2&quot;    rate = rospy.Rate(10.0)    while not rospy.is_shutdown():        try:            trans = tfBuffer.lookup_transform(turtle_name, &#39;turtle1&#39;, rospy.Time())            #print(trans)        except (tf.LookupException, tf.ConnectivityException, tf.ExtrapolationException):            rate.sleep()            continue        print(trans)        rate.sleep()NOTE: Using rospy.Time.now() in lookup_transform can cause errors. rospy.Time.now() &amp;gt; will ask for the frame with the current time, which might always not be avaiable immdeaditly. rospy.Time() &amp;gt; will ask for the latest frame that is available. To tackle this a time out argument can be given to the lookup_transform,trans = tfBuffer.lookup_transform(turtle_name, &#39;turtle1&#39;, rospy.Time.now(),rospy.Duration(1.0))Time TravelTo get the tranform between two objects from the past the following can be used.try:    past = rospy.Time.now() - rospy.Duration(5.0)    trans = tfBuffer.lookup_transform(turtle_name, &#39;carrot1&#39;, past, rospy.Duration(1.0))except (tf2_ros.LookupException, tf2_ros.ConnectivityException, tf2_ros.ExtrapolationException):To get transforms between objects from different timelines the following can be used.try:    past = rospy.Time.now() - rospy.Duration(5.0)    trans = tfBuffer.lookup_transform_full(target_frame=turtle_name,target_time=rospy.Time.now(), source_frame=&#39;carrot1&#39;,source_time=past,fixed_frame=&#39;world&#39;,timeout=rospy.Duration(1.0))except(tf2_ros.LookupException, tf2_ros.ConnectivityException, tf2_ros.ExtrapolationException):robot_state_publisherPublishing all the TF’s manually is very tedious and hence ROS provides nice tool called robot state publisher to automate the task for you. In essence, it takes a file describing the morphology of the robot (aka URDF file) as input and it automatically publishes the TF for you.&amp;lt;launch&amp;gt;  &amp;lt;!-- Load the URDF file in the param server variable robot_description if it wasn&#39;t loaded before --&amp;gt;  &amp;lt;param name=&quot;robot_description&quot; command=&quot;cat $(find pi_robot_pkg)/urdf/pi_robot_v2.urdf&quot;/&amp;gt;  &amp;lt;!-- Publish TF with robot_state_publisher --&amp;gt;  &amp;lt;node name=&quot;robot_state_publisher&quot; pkg=&quot;robot_state_publisher&quot; type=&quot;robot_state_publisher&quot;    respawn=&quot;false&quot; output=&quot;screen&quot;&amp;gt;    &amp;lt;remap from=&quot;/joint_states&quot; to=&quot;/pi_robot/joint_states&quot; /&amp;gt;  &amp;lt;/node&amp;gt;&amp;lt;/launch&amp;gt;joint_state_publisherThere are two ways to publish non-fixed joint framesPublish them directly&amp;lt;launch&amp;gt;  &amp;lt;param name=&quot;robot_description&quot; command=&quot;cat $(find pi_robot_pkg)/urdf/pi_robot_v2.urdf&quot; /&amp;gt;  &amp;lt;!-- send fake joint values --&amp;gt;  &amp;lt;node name=&quot;joint_state_publisher&quot; pkg=&quot;joint_state_publisher&quot; type=&quot;joint_state_publisher&quot;&amp;gt;    &amp;lt;param name=&quot;use_gui&quot; value=&quot;TRUE&quot;/&amp;gt;  &amp;lt;/node&amp;gt;  &amp;lt;!-- Combine joint values --&amp;gt;  &amp;lt;node name=&quot;robot_state_publisher&quot; pkg=&quot;robot_state_publisher&quot; type=&quot;state_publisher&quot;/&amp;gt;  &amp;lt;!-- Show in RVIZ   --&amp;gt;  &amp;lt;node name=&quot;rviz&quot; pkg=&quot;rviz&quot; type=&quot;rviz&quot; args=&quot;-d $(find pi_robot_pkg)/launch/pi_robot.rviz&quot;/&amp;gt;&amp;lt;/launch&amp;gt;NOTE: This only works in RVIZ, meaning the robot in gazebo won’t be effected by the joint state values.Publish them through Hardware/SimulationStep 1: Define a joint that isn’t fixed in your URDF File&amp;lt;joint name=&quot;left_shoulder_forward_joint&quot; type=&quot;revolute&quot;&amp;gt;    &amp;lt;parent link=&quot;left_shoulder_link&quot;/&amp;gt;    &amp;lt;child link=&quot;left_shoulder_forward_link&quot;/&amp;gt;    &amp;lt;origin xyz=&quot;0 0.025 0&quot; rpy=&quot;0 0 0&quot;/&amp;gt;    &amp;lt;limit lower=&quot;-1.57&quot; upper=&quot;1.57&quot; effort=&quot;10&quot; velocity=&quot;3&quot;/&amp;gt;    &amp;lt;axis xyz=&quot;0 0 1&quot;/&amp;gt;    &amp;lt;dynamics damping=&quot;0.7&quot;/&amp;gt;&amp;lt;/joint&amp;gt;Step 2: Define a Transmission for your joint in your URDF File&amp;lt;transmission name=&quot;tran4&quot;&amp;gt;    &amp;lt;type&amp;gt;transmission_interface/SimpleTransmission&amp;lt;/type&amp;gt;    &amp;lt;joint name=&quot;left_shoulder_forward_joint&quot;&amp;gt;        &amp;lt;hardwareInterface&amp;gt;EffortJointInterface&amp;lt;/hardwareInterface&amp;gt;    &amp;lt;/joint&amp;gt;    &amp;lt;actuator name=&quot;motor4&quot;&amp;gt;        &amp;lt;hardwareInterface&amp;gt;EffortJointInterface&amp;lt;/hardwareInterface&amp;gt;        &amp;lt;mechanicalReduction&amp;gt;1&amp;lt;/mechanicalReduction&amp;gt;    &amp;lt;/actuator&amp;gt;&amp;lt;/transmission&amp;gt;Step 3: Define the new transmission controller with the name xxx_position_controller (left_shoulder_forward_joint_position_controller) in a configuration yaml filepi_robot:  # Publish all joint states -----------------------------------  joint_state_controller:    type: joint_state_controller/JointStateController    publish_rate: 50  left_shoulder_forward_joint_position_controller:    type: effort_controllers/JointPositionController    joint: left_shoulder_forward_joint    pid: {p: 100.0, i: 0.01, d: 10.0}Step 4: Start everything through a launch file.&amp;lt;launch&amp;gt;  &amp;lt;!-- Load the URDF file in the param server variable robot_description if it wasn&#39;t loaded before --&amp;gt;  &amp;lt;param name=&quot;robot_description&quot; command=&quot;cat $(find pi_robot_pkg)/urdf/pi_robot_v2.urdf&quot; /&amp;gt;&amp;lt;!-- Load joint controller configurations from YAML file to parameter server --&amp;gt;  &amp;lt;rosparam file=&quot;$(find pi_robot_pkg)/config/pirobot_control.yaml&quot; command=&quot;load&quot;/&amp;gt;  &amp;lt;node name=&quot;controller_spawner&quot;        pkg=&quot;controller_manager&quot;        type=&quot;spawner&quot;        respawn=&quot;false&quot;        output=&quot;screen&quot;        ns=&quot;/pi_robot&quot;        args=&quot;head_pan_joint_position_controller              head_tilt_joint_position_controller              torso_joint_position_controller              left_shoulder_forward_joint_position_controller              right_shoulder_forward_joint_position_controller              left_shoulder_up_joint_position_controller              right_shoulder_up_joint_position_controller              left_elbow_joint_position_controller              right_elbow_joint_position_controller              left_wrist_joint_position_controller              right_wrist_joint_position_controller              joint_state_controller&quot;/&amp;gt;&amp;lt;node name=&quot;robot_state_publisher&quot; pkg=&quot;robot_state_publisher&quot; type=&quot;robot_state_publisher&quot;    respawn=&quot;false&quot; output=&quot;screen&quot;&amp;gt;    &amp;lt;remap from=&quot;/joint_states&quot; to=&quot;/pi_robot/joint_states&quot; /&amp;gt;  &amp;lt;/node&amp;gt; &amp;lt;!-- &amp;lt;node name=&quot;robot_state_publisher&quot; pkg=&quot;robot_state_publisher&quot; type=&quot;state_publisher&quot;/&amp;gt;--&amp;gt;&amp;lt;/launch&amp;gt;Static TransformsThere are some cases where changing the URDF is not advisable or it simply makes no sense to add it to the robot model.urdf. These are the cases where publishing a static transform, especially through the launch file or commands, is more convenient.Through Command LineThe following syntax can be used to publish a static transform via command line. rosrun tf static_transform_publisher x y z yaw pitch roll frame_id child_frame_id period_in_msThrough Launch filesThe following syntax can be used to publish a static transform via launch files.&amp;lt;launch&amp;gt;    &amp;lt;node pkg=&quot;tf&quot; type=&quot;static_transform_publisher&quot; name=&quot;name_of_node&quot;           args=&quot;x y z yaw pitch roll frame_id child_frame_id period_in_ms&quot;&amp;gt;    &amp;lt;/node&amp;gt;&amp;lt;/launch&amp;gt;Extra ContentSpawn a URDF Model in gazeboThis is an example of a launch file you could use to spawn URDF models into a Gazebo world:&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt;&amp;lt;launch&amp;gt;    &amp;lt;arg name=&quot;x&quot; default=&quot;0.0&quot; /&amp;gt;    &amp;lt;arg name=&quot;y&quot; default=&quot;0.0&quot; /&amp;gt;    &amp;lt;arg name=&quot;z&quot; default=&quot;0.0&quot; /&amp;gt;    &amp;lt;arg name=&quot;urdf_robot_file&quot; default=&quot;$(find your_pkg)/urdf/your_robot.urdf&quot; /&amp;gt;    &amp;lt;arg name=&quot;robot_name&quot; default=&quot;your_robot_model_name&quot; /&amp;gt;    &amp;lt;!-- Load the URDF into the ROS Parameter Server --&amp;gt;    &amp;lt;param name=&quot;robot_description&quot; command=&quot;cat $(arg urdf_robot_file)&quot; /&amp;gt;    &amp;lt;!-- Run a python script to the send a service call to gazebo_ros to spawn a URDF robot --&amp;gt;    &amp;lt;node name=&quot;urdf_spawner&quot; pkg=&quot;gazebo_ros&quot; type=&quot;spawn_model&quot; respawn=&quot;false&quot; output=&quot;screen&quot;    args=&quot;-urdf -x $(arg x) -y $(arg y) -z $(arg z)  -model $(arg robot_name) -param robot_description&quot;/&amp;gt;&amp;lt;/launch&amp;gt;Delete a model from gazeboTo get a list of all the current models in gazebo:osservice call /gazebo/get_world_properties &quot;{}&quot;To delete a paticular model from the above list:rosservice call /gazebo/delete_model &quot;model_name: &#39;your_robot_model_name&#39;&quot;"
  },
  {
    "title": "ROS Basics",
    "url": "/posts/ROS-Baiscs/",
    "categories": "Resources, ROSCheatSheet",
    "tags": "ros, topics, services, actions, packages, launch",
    "date": "2021-05-04 16:17:25 +0530",
    "snippet": "PackagesROS uses packages to orgnaize its program, you can think of a package as all the files that a specific ROS program needs to run sucessfully. A few common items inside a package:  launch - contains launch files  src - contains the source files  CMakeLists.txt - contains the rules for compilation  package.xml - package info and dependsYou can use roscd to go to any package in ROSroscd &amp;lt;package_name&amp;gt;Creating ROS PackagesAll packages must be created inside ~/catkin_ws/srcTo create a package the following synatx can be used:catkin_create_pkg &amp;lt;package_name&amp;gt; &amp;lt;package_depends&amp;gt;Compiling ROS PackagesOnce a package has been created, it needs to be compiled in order for it to work.catkin_make - Will compile the entire src directory and needs to be issued only in the catkin_ws directory.catkin_make --only-pkg-with-deps &amp;lt;package_name&amp;gt; - Will only compile the selected package.NodesNodes are individual ROS programs that together with other nodes form a working structure.To launch a node from the terminal:rosrun &amp;lt;package_name&amp;gt; &amp;lt;node_name&amp;gt; __name:=new_node_name __ns:=name_space topic:=new_topicTo get a list of nodes that are currently active:rosnode listTo get more information about a paticular node:rosnode info /node_nameLaunch FilesAll launch files are contained inside the &amp;lt;launch&amp;gt; tags. Launch files are used to “launch” nodes/programs in ROS. One of the major advantages of launch files is the fact that you can launch multiple nodes at once.&amp;lt;launch&amp;gt;      &amp;lt;include file=&quot;$(find package_where_launch_is)/launch/my_launch_file.launch&quot;/&amp;gt;      &amp;lt;node pkg=&quot;package_name&quot; type=&quot;script_name.py&quot; name=&quot;node_name&quot; output=&quot;screen&quot; respawn=&quot;false&quot; ns=&quot;w&quot;&amp;gt;        &amp;lt;remap from=&quot;/current_topic&quot; to=&quot;/needed_topic&quot;/&amp;gt;    &amp;lt;/node&amp;gt;&amp;lt;/launch&amp;gt;Parameter ServerA parameter server is a dictonary that ROS uses to store parameters. Any ROS program can access these parameters during runtime to customize itself accordingly. The following commands can be used to interact with the parameter server.rosparam list - Gets a list of parameter names that can be accessedrosparam get &amp;lt;parameter_name&amp;gt; - Gets the current value of that parameterrosparam set &amp;lt;parameter_name&amp;gt; &amp;lt;value&amp;gt; - Updates the parameter valueTopicsROS Nodes communicate with each other via communication channels called “Topics”, further messages are passed between nodes through topics. Every topic has a message type that it supports communication over. Nodes can subscribe to a topic to listen to data from other nodes or publish to topics to send data to other nodes which are subscribed to a given topic.To check for active topics:rostopic listFor more information about a topic:rostopic info /topic_nameTo publish to a topic at a rate of x Hz:rostopic pub -r x /topic_name /message_type argsTo Listen to a topic:rostopic echo /topic_nameSimple Publisher#! /usr/bin/env pythonimport rospyfrom std_msgs.msg import Stringrospy.init_node(&#39;node_name&#39;)pub = rospy.Publisher(&#39;/topic_name&#39;, String, latch=True, queue_size=10)r = rospy.Rate(10) # 10hzwhile not rospy.is_shutdown():   pub.publish(&quot;hello world&quot;)   r.sleep()Simple Subscriber#! /usr/bin/env pythonimport rospyfrom std_msgs.msg import Stringdef callback_function(data):    result = data.data    print(result)rospy.init_node(&#39;node_name&#39;)rospy.Subscriber(&#39;/topic_name&#39;,String,callback_function)rospy.spin()MessagesAs explained before data is sent via messages inside topics. ROS has a rich set of predefined messages that can be used.To find out more infomation about a paticular message:rosmsg show &amp;lt;package_name&amp;gt;/&amp;lt;message&amp;gt;You can get more information about what message type is used by a paticular node by using rosnode info. Messages can contain premitive datatypes or can be composed of multiple primitve datatypes. Here is an example rosmsg show geometry_msgs/PointStamped[geometry_msgs/PointStamped]:std_msgs/Header headeruint32 seq  time stamp  string frame_id  geometry_msgs/Point pointfloat64 x  float64 y  float64 z  Custom MessagesIn situations where standard messages don’t do the job, you can create your own message types using the following steps.  Create a msg folder in your package directory  Create a new file named YourMessage.msg inside the msg folder.      Age.msg should contain the message structure for example      Float32 Years     Float32 Months  Float32 Days        Edit the CMakeLists.txt - find_package()This is where all the packages required to COMPILE the messages of the topics, services, and actions go. In package.xml, you have to state them as build_depend.      find_package(catkin REQUIRED COMPONENTS       rospy       std_msgs       message_generation   # Add message_generation here, after the other packages  )        Edit the CMakeLists.txt - add_message_files() This function includes all of the messages of this package (in the msg folder) to be compiled. The file should look like this.      add_message_files(      FILES      Age.msg    ) # Dont Forget to UNCOMENT the parenthesis and add_message_files TOO        Edit the CMakeLists.txt - generate_messages() Here is where the packages needed for the messages compilation are imported.      generate_messages(      DEPENDENCIES      std_msgs  ) # Dont Forget to uncoment here TOO        Edit the CMakeLists.txt - catkin_package() State here all of the packages that will be needed by someone that executes something from your package. All of the packages stated here must be in the package.xml as exec_depend.      catkin_package(      CATKIN_DEPENDS rospy message_runtime   # This will NOT be the only thing here  )        Edit the package.xmlJust add these 3 lines to the package.xml file.      &amp;lt;build_depend&amp;gt;message_generation&amp;lt;/build_depend&amp;gt;   &amp;lt;build_export_depend&amp;gt;message_runtime&amp;lt;/build_export_depend&amp;gt;  &amp;lt;exec_depend&amp;gt;message_runtime&amp;lt;/exec_depend&amp;gt;        Compile      roscd; cd ..  catkin_make  source devel/setup.bash            Verification  rosmsg show Age      [topic_ex/Age]  float32 years     float32 months     float32 days        Using these in ROS Nodes    from package_name.msg import Age      ServicesMessages over topics are not the only way nodes communicate with each other, Services are a synchronous way of commincation where a ROS program calls a service, and waits until it receives a result from the service before continuing.To view what services are currently being offered:rosservice listFor more information about a service:rosservice info /service_nameThis will return you the name_of_the_package/Name_of_Service_messageTo call a service via terminal:rosservice call /service_name /message_type argumentsClientThis is what CALLS the functionality provided by the Service Server. That is, it CALLS the Service Server.#! /usr/bin/env pythonimport rospy# Import the service message used by the service /trajectory_by_namefrom trajectory_by_name_srv.srv import TrajByName, TrajByNameRequestimport sysrospy.init_node(&#39;service_client&#39;)rospy.wait_for_service(&#39;/trajectory_by_name&#39;)traj_by_name_service = rospy.ServiceProxy(&#39;/trajectory_by_name&#39;, TrajByName)traj_by_name_object = TrajByNameRequest()traj_by_name_object.traj_name = &quot;release_food&quot;result = traj_by_name_service(traj_by_name_object)print resultService MessagesROS Services are defined by srv files, which contains a request message and a response message. These are identical to the messages used with ROS Topics (see rospy message overview). rospy converts these srv files into Python source code and creates three classes that you need to be familiar with: service definitions, request messages, and response messages. The names of these classes come directly from the srv filename:my_package/srv/Foo.srv → my_package.srv.Foomy_package/srv/Foo.srv → my_package.srv.FooRequestmy_package/srv/Foo.srv → my_package.srv.FooResponseService message files have the extension .srv. Remember that Topic message files have the extension .msg. Service message files are defined inside a srv directory. Remember that Topic message files are defined inside a msg directory.To see the structure of a srv message:rossrv show name_of_the_package/Name_of_Service_messageService messages have TWO parts:REQUESTThe REQUEST is the part of the service message that defines HOW you will do a call to your service. This means, what variables you will have to pass to the Service Server so that it is able to complete its task.—RESPONSEThe RESPONSE is the part of the service message that defines HOW your service will respond after completing its functionality. If, for instance, it will return an string with a certaing message saying that everything went well, or if it will return nothing, etc…ServerThis is what PROVIDES the functionality. Whatever you want your Service to do, you have to place it in the Service Server.#! /usr/bin/env pythonimport rospyfrom std_srvs.srv import Empty, EmptyResponsedef my_callback(request):    print &quot;My_callback has been called&quot;    return EmptyResponse() # the service Response class, in this case EmptyResponserospy.init_node(&#39;service_server&#39;) my_service = rospy.Service(&#39;/my_service&#39;, Empty , my_callback) # create the Service called my_service with the defined callbackrospy.spin() # maintain the service open.Custom Service MessagesIn order to create a service message, you will have to follow the next steps  Create a srv folder in your package directory  Create a new file named MyCustomServiceMessage.srv inside the srv folder.  MyCustomServiceMessage.srv should contain the message structure for example      int32 duration    ---  bool success        Edit the CMakeLists.txt - find_package()This is where all the packages required to COMPILE the messages of the topics, services, and actions go. In package.xml, you have to state them as build_depend.    find_package(catkin REQUIRED COMPONENTS   rospy   std_msgs   message_generation   # Add message_generation here, after the other packages)        Edit the CMakeLists.txt - add_service_files() This function includes all of the service messages of this package (in the srv folder) to be compiled. The file should look like this.      add_service_files(      FILES      MyCustomServiceMessage.srv    ) # Dont Forget to UNCOMENT the parenthesis and add_message_files TOO        Edit the CMakeLists.txt - generate_messages() Here is where the packages needed for the service messages compilation are imported.      generate_messages(      DEPENDENCIES      std_msgs  ) # Dont Forget to uncoment here TOO        Edit the CMakeLists.txt - catkin_package() State here all of the packages that will be needed by someone that executes something from your package. All of the packages stated here must be in the package.xml as exec_depend.      catkin_package(      CATKIN_DEPENDS rospy message_runtime   # This will NOT be the only thing here  )        Edit the package.xmlJust add these 3 lines to the package.xml file.      &amp;lt;build_depend&amp;gt;message_generation&amp;lt;/build_depend&amp;gt;   &amp;lt;build_export_depend&amp;gt;message_runtime&amp;lt;/build_export_depend&amp;gt;  &amp;lt;exec_depend&amp;gt;message_runtime&amp;lt;/exec_depend&amp;gt;        Compile      roscd; cd ..  catkin_make  source devel/setup.bash        Verificationrossrv show package_name/MyCustomServiceMessage    [package_name/MyCustomServiceMessage]:   int32 duration---bool success            Using these in ROS Nodes      from package_name.srv import MyCustomServiceMessage, MyCustomServiceRequest/Responce      ActionsActions are like asynchronous calls to services, therefore are very similar to services. When you call an action, you are calling a functionality that another node is providing. Just the same as with services. The difference is that when your node calls a service, it must wait until the service finishes. When your node calls an action, it doesn’t necessarily have to wait for the action to complete.Hence, an action is an asynchronous call to another node’s functionality.The node that provides the functionality has to contain an action server. The action server allows other nodes to call that action functionality.The node that calls to the functionality has to contain an action client. The action client allows a node to connect to the action server of another node.When a robot provides an action, you will see that in the topics list. There are 5 topics with the same base name, and with the subtopics cancel, feedback, goal, result, and status.For example, here the name of the action server is “ardrone_action_server”:```bashuser ~ $ rostopic list....../ardrone_action_server/cancel/ardrone_action_server/feedback/ardrone_action_server/goal/ardrone_action_server/result/ardrone_action_server/status...``` ### Calling an Action Server Calling an action server means sending a message to it. In the same way as with topics and services, it all works by passing messages around.  The message of a topic is composed of a single part: the information the topic provides.  The message of a service has two parts: the request and the response.  The message of an action server is divided into three parts: the goal, the result, and the feedback.For example, here is the message structure of an action server:user ~ $ roscd ardrone_as/action; cat Ardrone.action#goal for the droneint32 nseconds  # the number of seconds the drone will be taking pictures---#resultsensor_msgs/CompressedImage[] allPictures # an array containing all the pictures taken along the nseconds---#feedbacksensor_msgs/CompressedImage lastImage  # the last image takenFeedbackDue to the fact that calling an action server does not interrupt your thread, action servers provide a message called the feedback. The feedback is a message that the action server generates every once in a while to indicate how the action is going (informing the caller of the status of the requested action). It is generated while the action is in progress.Working ExplainedSo, whenever an action server is called, the sequence of steps are as follows:1) When an action client calls an action server from a node, what actually happens is that the action client sends to the action server the goal requested through the /ardrone_action_server/goal topic.2) When the action server starts to execute the goal, it sends to the action client the feedback through the /ardrone_action_server/feedback topic.3) Finally, when the action server has finished the goal, it sends to the action client the result through the /ardrone_action_server/result topic.Sending Goals via terminalEach one of those three topics have their own type of message. The type of the message is built automatically by ROS from the .action file.For example, in the case of the ardrone_action_server the action file is called Ardrone.action.When you compile the package (with catkin_make), ROS will generate the following types of messages from the Ardrone.action file:  ArdroneActionGoal  ArdroneActionFeedback  ArdroneActionResult  ArdroneAction  ArdroneGoal  ArdroneResult  ArdroneFeedbackEach topic of the action server uses its associated type of message accordingly.To send goals via the terminal:rostopic pub /name_of_action_server/goal /type_of_the_message_used_by_the_topic parametersClient#! /usr/bin/env pythonimport rospyimport timeimport actionlibfrom ardrone_as.msg import ArdroneAction, ArdroneGoal, ArdroneResult, ArdroneFeedbacknImage = 1# definition of the feedback callback. This will be called when feedback# is received from the action server# it just prints a message indicating a new message has been receiveddef feedback_callback(feedback):	global nImage	print(&#39;[Feedback] image n.%d received&#39;%nImage)	nImage += 1# initializes the action client noderospy.init_node(&#39;drone_action_client&#39;)# create the connection to the action serverclient = actionlib.SimpleActionClient(&#39;/ardrone_action_server&#39;, ArdroneAction)# waits until the action server is up and runningclient.wait_for_server()# creates a goal to send to the action servergoal = ArdroneGoal()goal.nseconds = 10 # indicates, take pictures along 10 seconds# sends the goal to the action server, specifying which feedback function# to call when feedback receivedclient.send_goal(goal, feedback_cb=feedback_callback)# Uncomment these lines to test goal preemption:#time.sleep(3.0)#client.cancel_goal()  # would cancel the goal 3 seconds after starting# wait until the result is obtained# you can do other stuff here instead of waiting# and check for status from time to time # status = client.get_state()# check the client API link below for more infoclient.wait_for_result()print(&#39;[Result] State: %d&#39;%(client.get_state()))How to perform other tasks while the Action is in progressSo, the SimpleActionClient objects have two functions that can be used for knowing if the action that is being performed has finished, and how:1) wait_for_result(): This function is very simple. When called, it will wait until the action has finished and returns a true value. As you can see, it’s useless if you want to perform other tasks in parallel because the program will stop there until the action is finished.2) get_state(): This function is much more interesting. When called, it returns an integer that indicates in which state is the action that the SimpleActionClient object is connected to.  0 ==&amp;gt; PENDING  1 ==&amp;gt; ACTIVE  2 ==&amp;gt; DONE  3 ==&amp;gt; WARN  4 ==&amp;gt; ERRORThis allows you to create a while loop that checks if the value returned by get_state() is 2 or higher. If it is not, it means that the action is still in progress, so you can keep doing other things.Preempting a goalIt happens that you can cancel a goal previously sent to an action server prior to its completion. Cancelling a goal while it is being executed is called preempting a goalYou may need to preempt a goal for many reasons, like, for example, the robot went mad about your goal and it is safer to stop it prior to the robot doing some harm.In order to preempt a goal, you send the cancel_goal to the server through the client connection.client.cancel_goal()The axclientThe axclient is, basically, a GUI tool provided by the actionlib package, that allows you to interact with an Action Server in a very easy and visual way.The command used to launch the axclient is the following:rosrun actionlib axclient.py /name_of_action_serverServer#! /usr/bin/env pythonimport rospyimport timefrom std_msgs.msg import Emptyfrom actions_quiz.msg import CustomActionMsgAction, CustomActionMsgGoal, CustomActionMsgFeedbackimport actionlibclass DroneAction(object):    feedbackM = CustomActionMsgFeedback()    goalM = CustomActionMsgGoal()    emp = Empty()    def __init__(self):        self.success = True        self.pub_takeoff = rospy.Publisher(&#39;drone/takeoff&#39;,Empty,latch=True,queue_size=1)        self.pub_land = rospy.Publisher(&#39;drone/land&#39;,Empty,latch=True,queue_size=1)        self.server = actionlib.SimpleActionServer(&#39;/action_custom_msg_as&#39;,CustomActionMsgAction,self.goal_callback,False)        self.server.start()    def goal_callback(self,goal):        if self.server.is_preempt_requested():                rospy.loginfo(&#39;asked to quit&#39;)                self.server.set_preempted()                self.success = False        else :            if goal.goal == &#39;TAKEOFF&#39;:                self.pub_takeoff.publish(self.emp)            if goal.goal == &#39;LAND&#39;:                self.pub_land.publish(self.emp)            self.feedbackM.feedback = goal.goal            self.server.publish_feedback(self.feedbackM)            rospy.sleep(1)        if self.success:            self.result = self.emp            self.server.set_succeeded(self.result)if __name__ == &#39;__main__&#39;:  rospy.init_node(&#39;server_noder&#39;)  DroneAction()  rospy.spin()Once the server is launched you should be able to see the following:rostopic list | grep action_custom_msg_as    /action_custom_msg_as/cancel    /action_custom_msg_as/feedback    /action_custom_msg_as/goal    /action_custom_msg_as/result    /action_custom_msg_as/statusCustom Action MessagesIn order to create a actiob message, you will have to follow the next steps:  Create a action folder in your package directory  Create a new file named Name.action inside the action folder.  Name.action should contain the message structure for example      int32 duration #Goal     ---     bool success #Result     ---     int32 current_time #Feedback        Edit the CMakeLists.txt - find_package()This is where all the packages required to COMPILE the messages of the topics, services, and actions go. In package.xml, you have to state them as build_depend.      find_package(catkin REQUIRED COMPONENTS       rospy       actionlib_msgs  )        Edit the CMakeLists.txt - add_action_files() This function includes all of the action messages of this package (in the action folder) to be compiled. The file should look like this.      add_action_files(      FILES      Name.action    ) # Dont Forget to UNCOMENT the parenthesis            Edit the CMakeLists.txt - generate_messages()Here is where the packages needed for the service messages compilation are imported.      generate_messages(      DEPENDENCIES      actionlib_msgs  ) # Dont Forget to uncoment here TOO            Edit the CMakeLists.txt - catkin_package() State here all of the packages that will be needed by someone that executes something from your package. All of the packages stated here must be in the package.xml as exec_depend.      catkin_package(      CATKIN_DEPENDS rospy message_runtime   # This will NOT be the only thing here  )        Edit the package.xmlJust add these 3 lines to the package.xml file.      &amp;lt;build_depend&amp;gt;actionlib&amp;lt;/build_depend&amp;gt;  &amp;lt;build_depend&amp;gt;actionlib_msgs&amp;lt;/build_depend&amp;gt;  &amp;lt;build_export_depend&amp;gt;actionlib&amp;lt;/build_export_depend&amp;gt;  &amp;lt;build_export_depend&amp;gt;actionlib_msgs&amp;lt;/build_export_depend&amp;gt;  &amp;lt;exec_depend&amp;gt;actionlib&amp;lt;/exec_depend&amp;gt;  &amp;lt;exec_depend&amp;gt;actionlib_msgs&amp;lt;/exec_depend&amp;gt;        Compile      roscd; cd ..    catkin_make    source devel/setup.bash            Verification      rosmsg list | grep Name              my_custom_action_msg_pkg/NameAction       my_custom_action_msg_pkg/NameActionFeedback      my_custom_action_msg_pkg/NameActionGoal      my_custom_action_msg_pkg/NameActionResult      my_custom_action_msg_pkg/NameFeedback      my_custom_action_msg_pkg/NameGoal       my_custom_action_msg_pkg/NameResult        Using these in ROS Nodes    from my_custom_action_msg_pkg.msg import NameAction, NameGoal, NameFeedback, NameResult      "
  },
  {
    "title": "Autonomous Stair Climbing Robot",
    "url": "/posts/Stair-Climbing/",
    "categories": "Projects, Autonomous Robots",
    "tags": "atmega328p, ros, imu, touch, opencv",
    "date": "2020-09-15 21:05:20 +0530",
    "snippet": "The aim of this project is to “Design, Develop and Implement an Autonomous Stair Climbing Robot which can climb stairs, turn at the quarter-landings and deliver packages autonomously”.Team            Members      Sub-System                  Bandi Jai Krishna      CAD, Simulation              Utkarsh Jain      Simulation, Electronics              Abijith Y.L.      CAD, Simulation              Sriram Kodey      CAD, Image Processing              T. Vamsi Krishna      Image Processing      IntroductionIn an e-commerce scenario, the last mile delivery is the phase where the customer experiences / receives his desired ordered item. Sophisticated handling and delivery of the product ensures customer confidence.Robots can be used to assist the ‘Wishmasters’ in making deliveries ergonomic thus resulting in efficient delivery. This kind of automation can prove to be very helpful for delivery of large items such as refrigerators, Washing Machines etc. in multi-floor buildings where elevators aren’t available. They can also find applications in good movements within the warehouse.This project will act as a prototype for the above-mentioned vision and will be a huge step towards the automatization of e-commerce deliveries.MotivationThe motivation behind this project is to develop an autonomous stair climbing robot which will help improve Flipkart’s last-mile delivery’s speed and efficiency and subsequently increase the customer confidence in Flipkart.Design AlternativesIn the designing process, during the ideation phase, several different options and mechanisms were explored. The merits and limitations of each mechanism were considered  and three of the most suitable mechanisms were selected. These were modelled using Solidworks(CAD Modelling Software) and were simulated in the environment described in the problem statement. The results of these simulations were analysed and based on this analysis, the final mechanism/design was developed.R-HexThe first mechanism that was modelled and tested was based on the  R - Hex project developed by DARPA. This mechanism has been specifically designed to travel on varying terrains. It was also tested for climbing stairs and the results were positive. Thus this mechanism was selected for testing.A design was developed, inspired by the R - Hex Project: it had a similar wheel geometry with more spokes to increase the effective contact with the ground and has four wheels. This design was simulated with the load and environment described in the problem statement and results were as follows:The robot was able to climb stairs after some optimisation to the design and was also able to carry the necessary load. However, there was a lot of vibration and sudden shifts in the position of the centre of gravity of the package due to frequent slipping of the wheels on the stair edges. This is an undesirable trait and thus the model was discarded.Curved Spokes VariationRocker BogieThe second mechanism that was selected for testing was based on the Rocker Bogie Mechanism, first introduced by NASA in it’s Mars Rovers. The use of this mechanism would enable us to climb over an obstacle that is two times the size of the wheel diameter and this system would also act as a suspension without the use of springs or torsion beams. These traits encouraged us to consider it for testing.CAD models were made for several variations of this mechanism and simulated in the environment described in the research paper. The variations included independent and linked bogies, Dimensional changes in the links of the mechanism, Several pivot points and a modified version of the Bogie. All these modifications were made as an attempt to tune the mechanism to tackle varying step dimensions and increase traction, which is necessary to move the package up the stairs. Although the design developed after these modifications was capable of climbing stairs, it failed to do so in the presence of load, thus making it impractical to use it in the robot.6 Wheel VersionIn Figure: 6 Wheel Rocker BogieIn figure: Center of mass position vs time graph for rocker-bogie based model8 Wheel VersionIn Figure: 8 Wheel Rocker BogieContinuous TracksThe third mechanism selected for testing is the Continuous Track. The concept can be dated back to the 1900’s and has proved to be a great solution for a propulsion system where high traction and grip is essential.  Other positive traits being, better mobility (ability to perform spot turns), better performance in terms of providing a smoother climb. A couple of designs have been made in solidworks and then tested in Sketchup using the MsPhysics plugin. The results were satisfactory and thus convinced us to use this mechanism in the final bot.In figure: Continuous tracks modelProposed DesginThe proposed design uses the continuous track mechanism. To  ensure the tilt stability of the bot, the centre of gravity must be low; to ensure this, we have the electronic components (motors, steppers and  battery being the heavy parts) placed at a very low point. The robot need not shift modes to climb stairs, and can travel at relatively higher speed on the ground which decreases the overall time required for the robot to climb the stairs. We have two wheels on each side with a forward wheel drive system, this enables us to shift the center of gravity forward helping us to prevent toppling during the climb manoeuvre. We also will be able to perform spot turns, helping us to change the direction of the robot on the quarter landing. We have a retractable arm system at the rear end of the robot which doubles as an emergency brake and as a system which helps the robot to align itself with the slope of the stairs at the beginning of the descent.          The entire robot has been divided into four different parts :- The electronics compartment, The package compartment and the secondary electronics compartments at the front and rear ends of the bot. The electronics compartment houses the heaviest parts, adding to the  stability of the bot. The package compartment is bigger than the maximum package dimensions,  this was done to add cushioning  inside the package compartment to ensure the safety of the package. The electronics compartments at the front and rear will house the camera for the stair detection and IR Distance sensors for obstacle detection.Technical &amp;amp; Hardware ArchitectureIn figure: Complete Hardware architectureThe above flowchart represents the internal architecture of the robot. Enabled with two seperate microcontrollers for input collection and output distribution, the OBC is used to run complex image processing algorithms from the camera stream data and act as a bridge between the two microcontrollers. The above architecture has been chosen to keep the entire design modular.In figure: Complete Software ArchitectureIR Distance SensorAn IR distance sensor uses a beam of infrared light reflected off an object to measure its distance. The distance is calculated using triangulation of the beam of light. The sensor consists of an IR LED and a light detector or PSD (Position Sensing Device).When the beam of light gets reflected by an object, it reaches the light detector and an ‘optical spot’  forms on the PSD. The sensor has a built-in signal processing circuit. This circuit processes the position of the optical spot on the PSD to determine the position (distance) of the reflective object. It outputs an analog signal which depends on the position of the object in front of the sensor.Non-Linear Responce CurvesIR distance sensors output an analog signal, which changes depending on the distance between the sensor and an object. A generic analog IR sensor gives out analog readings of about 2.5 V when an object is 100 cm away to 1.4 V when an object is 500 cm away. The graph also shows why the usable detection range starts at 100 cm. Notice that the output voltage of an object that is 30 cm away is the same as the output voltage for an object that is 160 cm away. The usable detection range therefore starts after the peak at roughly 100 cm or 2.5 V.In figure: Output voltage vs distanceThe output voltage when plotted against the inverse of the distance results in a linear plot for distances greater than 100cm.In figure: Output voltage vs inverse number of distanceThe linear function for the above graph is represented by the equation:\\[y = 137500x + 1125\\]With $y$ equal to the output voltage in mV and $x$ equal to 1/distance in cm. This results in the following formula for the distance between the sensor and an object:\\[Distance (cm) = 1/((Output_voltage_mV – 1125)/137500)\\]This function is based on data from the datasheets of SHARP IR Sensor. The output characteristics of the sensor will vary slightly from sensor to sensor so you might get inaccurate readings.Bluetooth ModuleThe Bluetooth module is used to create a Bluetooth Network consisting of a Personal Area Network or a piconet which contains a minimum of 2 to maximum of 8 bluetooth peer devices- Usually a single master and upto 7 slaves. A master is the device which initiates communication with other devices. The master device governs the communications link and trafﬁc between itself and the slave devices associated with it. A slave device is the device that responds to the master device. Slave devices are required to synchronize their transmit/receive timing with that of the masters. In addition, transmissions by slave devices are governed by the master device (i.e., the master device dictates when a slave device may transmit). Speciﬁcally, a slave may only begin its transmissions in a time slot immediately following the time slot in which it was addressed by the master, or in a time slot explicitly reserved for use by the slave device.Data transfer is achieved through embedded low cost transceivers. Bluetooth works on the frequency band of 2.45GHz and can support upto 721KBps along with three voice channels.Stepper MotorStepper motors are DC motors that move in discrete steps. They have multiple coils that are organized in groups called “phases”. By energizing each phase in sequence, the motor will rotate, one step at a time.With a computer controlled stepping you can achieve very precise positioning and/or speed control. For this reason, stepper motors are the motor of choice for many precision motion control applications.Features of Stepper MotorPositioning : Since steppers move in precise repeatable steps, they excel in applications requiring precise positioning such as 3D printers, CNC, Camera platforms and X,Y Plotters. Some disk drives also use stepper motors to position the read/write head.Speed Control : Precise increments of movement also allow for excellent control of rotational speed for process automation and robotics.Low Speed Torque :  Normal DC motors don’t have very much torque at low speeds. A Stepper motor has maximum torque at low speeds, so they are a good choice for applications requiring low speed with high precision.Torque/ Speed characteristicsOne weakness with the SM is its limited torque abilities at high speeds, since the torque of a SM will fall with rising speed above the cutoff speed (i.e. in resonant speed) , as shown in figure below. When the motor is operating below its cutoff speed, the rise and fall times of the current through the motor windings occupy an insignificant fraction of each step, while at the cutoff speed, the step duration is comparable to the sum of the rise and fall times.In figure: Torque vs speedStep Angle of the SMThe angle through which the motor shaft revolves for each command signal is named the step angle (β) as shown in figure below. Reduced the step angle, increased the number of steps per revolution and higher the resolution or accuracy of positioning obtained. The step angles can be as slight as 0.72º or as large as 90º. But the most traditional step sizes are 1.8º, 2.5º, 7.5º and 15º.Types of SMs According to Arrangement of Stator WindingsThe stator part of the Stepper Motor holds numerous windings. The arrangement of these windings is the major factor that differentiates various kinds of SMs from an electrical point of opinion. Thus SMs may be wound using either unipolar windings or bipolar windings and as follows.Unipolar MotorsThe unipolar stepper motor operates with one winding with a center tap per phase. Each section of the winding is switched on for each direction of the magnetic field. Each winding is made relatively simple with the commutation circuit, this is done since the arrangement has a magnetic pole which can be reversed without switching the direction of the current.Bipolar MotorsWith bipolar stepper motors, there is only a single winding per phase. The driving circuit needs to be more complicated to reverse the magnetic pole, this is done to reverse the current in the winding. This is done with an H-bridge arrangement, however, there are several driver chips that can be purchased to make this a more simple task. Unlike the unipolar stepper motor, the bipolar stepper motor has two leads per phase, neither of which are common. Static friction effects do happen with an H-bridge with certain drive topologies, however, this can be reduced by dithering the stepper motor signal at a higher frequency.DC MotorsA DC motor is any of a class of rotary electrical motors that converts direct current electrical energy into mechanical energy. The most common types rely on the forces produced by magnetic fields. Nearly all types of DC motors have some internal mechanism, either electromechanical or electronic, to periodically change the direction of current in part of the motor. The main advantages of DC motors are:  DC motors provide a fine control of the speed which can not be attained by AC motors.  DC motors can develop rated torque at all speeds from standstill to rated speed.  Developed torque at standstill is many times greater than the torque developed by an AC motor of equal power and speed rating.High Torque geared motorsTo generate high torques using a given motor, an assembly of gears is used. A gear’s transmission torque changes as it increases or decreases speed. Generally, by reducing the speed, a small torque at the input side is transmitted as a larger torque at the output side. The characteristic curves of a given motor are charts that enable us to determine the torque output at a specific speed.In figure: Main features - chartsTo illustrate, for the motor in the figure above, delivering a torque of 25 mNm, the corresponding speed is 2000 rpm, at which the motor draws approximately 0.76A of current. This chart also tells us that this torque, speed and power are not optimum values for this type of motor. This is because the nominal (i.e. optimum) values apply when the motor is running at more or less maximum efficiency. However, because in practice the duty point of the motor will not always coincide with these values, the efficiency will be lower at that point, such that in practice the motors will often run hotter than expected.It is important to consider all the characteristics while choosing a motor for a particular application.Motor driversMotor drivers act as an interface between the motors and the control circuits. Motors require a high amount of current whereas the controller circuit works on low current signals. So the function of motor drivers is to take a low-current control signal and then turn it into a higher-current signal that can drive a motor. They consist of integrated H- bridge circuits, which are used to change the direction of rotation of the connected motors.EncodersAn encoder is an electromechanical device that provides an electrical signal that is used for speed and/or position control. Encoders turn mechanical motion into an electrical signal that is used by the control system to monitor specific parameters of the application and make adjustments if necessary to maintain the machine operating as desired. The parameters monitored are determined by the type of application and can include speed, distance, RPM, position among others. Applications that utilize encoders or other sensors to control specific parameters are often referred to as closed-loop feedback or closed-loop control systems.The application in which the motor encoder is being utilized will determine the motor encoder technology that needs to be used. The two broad types of motor encoder technologies available are:Absolute Encoders: The output of an absolute motor encoder indicates both the motion and the position of the motor shaft. Absolute motor encoders are most often used on Servo Motors in applications where position accuracy is required.Incremental Encoders: Incremental encoders provide a specific number of equally spaced pulses per revolution (PPR) or per inch or millimeter of linear motion. Incremental encoders consist of three basic components: a slotted disc, a light source and a dual light detector, as shown in Figure 6. The light source shines on the disc, which has a regularly spaced radial pattern of transmissive and reflective elements, called encoder increments. The quadrature light detector measures the amount of light passed through the slotted disc and generates two quadrature output pulse signals, denoted by A and B.For the application of feedback control to motion systems with optical incremental encoders, the position is generally measured at a fixed sampling frequency. The measurement accuracy is limited by the quantization of the encoder, i.e., by the number of slits on the encoder disc. The quantization errors can be reduced by either using more expensive encoders with more increments or by using smart signal processing techniques. Furthermore, the position information of the encoders is distorted by several encoder imperfections, such as eccentricity and tilt of the disc, misalignment of the light detector/source, and a non-equidistant slit distribution.In figure: Output pulses given by incremental encodersThe quadrature incremental signals are typically decoded to obtain up to four times the base pulses per revolution (PPR). Quadrature decoding counts both the rising and falling edges of Channel A and B.Encoder pulses during a sampling interval can be converted to motor speed using the equation:\\(Cm  =  π* Dn / (n*Ce)\\)where,Cm = conversion factor that translates encoder pulses into linear wheel displacementDn = nominal wheel diameter (in mm)Ce = encoder resolution (in pulses per revolution)n = gear ratio of the reduction gear between the motor (where the encoder is attached) and the drive wheelIMUThe term IMU stands for “Inertial Measurement Unit,” and is used  to describe a collection of measurement tools. When installed in a device, these tools can capture data about the device’s movement. IMUs contain sensors such as accelerometers, gyroscopes, and magnetometers. IMUs combine input from several different sensor types in order to accurately output movement. The primary goal of an IMU is to track the 3D position and orientation of a rigid body in time without using any external references. For example, if the body begins at rest, the accelerometer can use the gravitational field to determine the orientation of the body. There is no way for an IMU to determine the initial x-y-z position of the body,2 so only relative motion can be estimated. When the body begins to move, it must experience either linear accelerations, angular velocities, or both. Angular velocity can be numerically integrated by microcontroller software to estimate the orientation of the body, and linear accelerations can be integrated to get linear velocities and again to get the net position change from the initial position. Sophisticated software integrators exist; they typically employ extended Kalman filters.Because of sensor errors and errors due to numerical integration, estimates of linear velocity and position tend to accumulate error over time. These errors can be mitigated by occasionally referencing an external reference such as GPS. IMUs allow systems that rely on GPS to continue functioning when GPS is briefly unavailable.An IMU can consist of a PCB with separate accelerometer and gyro ICs; a single IC incorporating both an accelerometer and a gyro; or a complete integrated solution, including onboard estimation software. An example IMU is the STMicroelectronics ASM330LXH, which features a three-axis accelerometer with a user-selectable full scale between ± 2g and ± 16g and a three-axis gyro with a user-selectable full scale between ± 125 and ± 2000 degrees/s. It communicates by I2C or SPI.Data fusion algorithmsIn terms of Inertial Measurement Unit (IMU) data fusion algorithm, the complementary filter and Kalman filter are the most widely used algorithms. Each has its unique advantages and disadvantages.Complementary filterThe complementary filter consists of both low and high pass filters and as it is easier to implement this filter was implemented for getting precise data. In the Inertial Measurement Unit (IMU) accelerometer, gyroscope and magnetometer are integrated. Accelerometer and the gyroscope are the main sensor and the magnetometer is the correction sensor. All the forces working on the object are measured by an accelerometer and as the small forces create disturbance in measurement, long term measurement is reliable. So for the accelerometer, a low pass filter is needed for correction. In the gyroscopic sensor the integration is done over a period of time the value starts to drift in the long term, so a high pass filter is needed for gyroscopic data correction.In figure: Output pulses given by incremental encodersKalman filterFor a dynamic system by predicting the future value from the previous data the situation of the system can be guessed. For this type of system which is continuously changing the Kalman filter is the perfect to make an educated guess of what happened. As this filter predicts the future from previous data so some initial value is considered in the beginning. In this implementation the filter starts working after the IMU is on and measures the initial value. Then the value is transmitted to the filter and thus the filter starts computing. Kalman filter uses correlation between prediction and what actually happened to make the prediction error.In figure: Output pulses given by incremental encodersTemperature SensorTemperature sensors can be classified into two types based on their calibration methods. The output voltage can be linearly proportional to the Centigrade temperature or the Kelvin Scale. The devices that use the Centigrade Scale for calibration have an advantage over linear temperature sensors calibrated in Kelvin, as the user is not required to subtract a large constant voltage from the output to obtain convenient Centigrade scaling. Further, most Centigrade based temperature sensors don’t need external calibration or trimming to provide typical accuracies of ±¼°C at room temperature and ±¾°C over a full −55°C to 150°C temperature range. Most of the Integrated Circuits based temperature sensors use a delta-V BE architecture. The temperature-sensing element is then buffered by an amplifier and provided to the VOUT pin.Transfer Functions &amp;amp; ErrorThe accuracy specifications for a Temperature sensor can be calculated with a simple linear transfer function:\\(V_{OUT} = 10 mv/°C × T\\)where• $V_{OUT}$ is the LM35 output voltage• $T$ is the temperature in °CBelow attached is a comparison graph between the actual temperature and the error in the readings taken using a variety of temperature sensors.In figure: Output pulses given by incremental encodersDC Voltage SensorMost of the voltage sensors use a resistor divider circuit consisting of a sensor and a reference resistor which is used to measure the scaled down voltage. The voltage that is developed across the reference resistor or sensor is buffered and then given to the ADC. The output voltage of the sensor can be expressed as\\[V_{out} = \\dfrac{R_{M}}{R_{M}+R_{F}}V_{ref}\\]One major drawback of the above method is the amplification of the voltage across both the sensor and the reference resistor by the amplifier. The voltage changes due to the change in resistance of the sensor can be isolated and amplified using a resistance bridge.In figure: Internal circuit of a DC voltage sensorThe output voltage can be calculated using the following formula:\\[V_{out} = \\dfrac{R_{M}\\delta}{R_{M}+R_{F}(1+(R/R)(1+\\delta))}V_{ref}A\\]Camera ModulesMost Camera Modules come with parallel &amp;amp; MIPI interfaces. CMOS camera modules will be effective in any application or environment because of their versatility and wide range of options to choose from. Embedded camera modules can be interfaced to various processors likeNXP IMX8, IMX7, IMX6 &amp;amp; NVIDIA’s Jetson Nano, Xavier and TX2, Google Coral Dev Board, Rockchip RK960 Board and TI’ DM3730.The light collected by the object through the lens is converted into an electrical signal by a CMOS or CCD integrated circuit, and then converted into a digital image signal by an internal image processor (ISP) to be output to a digital signal which is then processed by the on board computer to filter out the required components from the images captured.On-Board-ComputerThe on-board computer (OBC) is the component of the robot which acts as the ‘brain’. It has a similar function as the CPU in a personal computer. It takes input from various electronic components and gives output to various others.The architecture of our robot makes the OBC take direct input from two camera modules, a Bluetooth module and a gsm module. It also takes indirect input (through a microcontroller) from four IR distance sensors, a temperature sensor, a voltage sensor, an IMU and a tilt sensor.Similarly, the OBC is also responsible for giving output instructions to the status lights module, and the microcontroller which is controlling the motors and steppers.The OBC takes in input from the cameras and processes the frames to detect stairs (during normal operation) and to detect the wishmaster (during the Follow-the-wishmaster mode). Additionally, the OBC uses the readings from the IR distance sensors to detect obstacles and directs the output microcontroller to do the necessary. The information from the temperature and voltage sensors is used by the OBC to determine the overall health of the power systems and is sent to the wishmaster’s app and the central server through the bluetooth and gsm modules respectively.MicrocontrollersA single-board microcontroller is a microcontroller built onto a single printed circuit board. This board provides all of the circuitry necessary for a useful control task: a microprocessor, I/O circuits, a clock generator, RAM, stored program memory and any necessary support ICs.The robot houses two microcontroller boards. One of them is used to take inputs from various sensors like IR distance sensors, voltage sensor, temperature sensor, tilt sensor and the IMU. It takes input through the GPIO pins and passes the data onto the OBC which decides what to do with the above information.After the OBC processes the information, it passes instructions to the microcontroller which controls the output. This microcontroller is responsible for controlling the motor drivers which in turn, command the motors. The stepper motors used to control the ‘emergency braking arms’ are also commanded by this microcontroller. Finally, the output microcontroller also controls the smart lock system.NeoPixel StripsThe WS2812 Integrated Light Source — or NeoPixel in Adafruit parlance — is the latest advance in the quest for a simple, scalable and affordable full-color LED. Red, green and blue LEDs are integrated alongside a driver chip into a tiny surface-mount package controlled through a single wire.Each individual NeoPixel draws up to 60 milliamps at maximum brightness white (red + green + blue). In actual use though, it’s rare for all pixels to be turned on that way. When mixing colors and displaying animations, the current draw will be much less.Features &amp;amp; Add-onsOur current design is very simplistic and allows us to add various out-of-the-box features. The following is a list of features that are currently planned for the proposed design. This list is not exhaustive and may be expanded in the future.IoT based Data Logging ModuleThe robot is equipped with a GSM module that lets it access the internet and relay back data like position, health of the vehicle and other crucial information to a central server that can be used to monitor the robot and command it in case of emergencies in real time. The feature is extremely useful when the number of robots in the warehouse or for delivery are scaled up.Manual Control ModeThe “Manual Control Mode” enables the wishmaster to control the robot from their smartphone. This lets the wishmaster help the robot do complex maneuvers like crossing roads, moving around obstacles quickly and efficiently if the need be.Stair Detection ModuleThe feed from the camera is taken and processed frame by frame using various image processing techniques such as thresholding, masking, conversion to binary, eroding and dilating the image to finally filter out the endpoints of each of the stairs. The robot will then use these to align itself with the center of the staircase while making the climb or descending.        Status Indication ModuleNeoPixel strips have been placed at the 4 corners of the robot. This enables the robot to visually communicate with people in its vicinity. The status indication module is used to send out visual signals to help people around the robot respond appropriately when interacting with the robot. A few important cases where there visual signals can be used include manoeuvring mechanisms like turnings, low battery, or other pre-programmable scenarios like securement of the package lock, and physical damage to the robot.Follow Wishmaster ModeOur robot also has a special mode called the ‘Follow wishmaster’ mode. When this mode is activated by the wishmaster (using the app), the robot will automatically start to follow the wishmaster( who will be standing in front of the robot) using its camera modules. This will be accomplished by using the computer vision technique called ‘Haar classifiers (for person detection)’.  The robot will also be programmed such that it never collides into the wishmaster or any other obstacle during this operation (using the IR distance sensors).Smart Lock ModuleThe package to be carried on the robot, will be loaded into a package container. The package container is a partition in the body of the robot which will be secured using the smart lock system. The smart lock system is a solenoid valve powered lock. It will be controlled by the wishmaster using the bluetooth enabled app (which is also used to control the robot in the manual mode).The solenoid lock denotes a latch for electrical locking and unlocking. It is available in ‘unlocking in the power-on mode’ type, and ‘locking and keeping in the power-on mode’ type, which can be used selectively for situations. The power-on unlocking type enables unlocking only while the solenoid is powered on. A door with this type is locked and not opened in case of power failure or wire disconnection, ensuring excellent safety. Hence, we will be using the ‘unlocking in power on mode’ type lock.Emergency Braking SystemIn the event of an emergency, like power failure, the robot might be stranded in the middle of a flight of stairs. To tackle this problem we have installed this emergency braking system. When the robot detects that the battery is going to die, it will automatically go into a low power mode where the remaining power is used to send the details of the problem along with the location of the robot to the central server. This contingency power is also used to lower the arms powered by the stepper motors towards the downward slope of the stairs so that in the event of the robot slipping, the arms will stop it from going further down and provide support to stay there until help arrives. The arms will have a rubber stopper with high grip at their ends to maximise the safety of the robot.Kill SwitchThe kill switch is a safety mechanism used to shut off the robot in the event of an emergency. Unlike a normal shut-down switch or shut-down procedure, which shuts down all systems in order and turns off the machine without damage, a kill switch is designed and configured to abort the operation as quickly as possible (even if it damages the equipment) and to be operated simply and quickly, so that even a panicked operator with impaired executive functions or a by-stander can activate it.The switch will be installed on the robot in a manner which will make it easily noticeable and operable.Obstacle DetectionObstacle detection is a very important feature of our robot. This will help the robot to avoid colliding  with the various obstacles present in its environment. We will be using four IR distance sensors (one on each side of the robot) to measure its distance from the nearby obstacles. If the reading from any of the sensors goes below a particular safety threshold, the robot will stop moving. This will be useful for the robot to avoid damage to itself as well as other things, both living and nonliving."
  },
  {
    "title": "Hands On with AVR - 02 Interrupts",
    "url": "/posts/AVR-Diaries-02-Interrupts/",
    "categories": "Resources, AVR Dairies",
    "tags": "atmega328p, external interrupts, pin change interrupts, ISR",
    "date": "2020-05-13 16:23:20 +0530",
    "snippet": "Most processors have interrupts. Interrupts let you respond to “external/internal” events while doing something else.This article uses concepts that have been covered in the previous post “AVR Diaries 01 - Port Manipulation”, if your not familiar with port manipulation, it is highly recommended that you go through the earlier parts before continuing any further.Understanding the need for interruptsBefore we dive headfirst into interrupts and all the superpowers they grant to us, let’s take a step back and see why we need interrupts. Let’s take a simple sketch where an LED on pin 13 is turned on every time we press a button connected to pin 9. The LED turns off right after we release the button.int prev_state = LOW;int LED_state;int current_state;void setup() {  pinMode(13, OUTPUT);  pinMode(9, INPUT_PULLUP);}void loop() {  current_state = digitalRead(9);  current_state = !current_state;  if(current_state ==HIGH &amp;amp;&amp;amp; prev_state==LOW){    LED_state = HIGH;  }  else if(current_state ==LOW &amp;amp;&amp;amp; prev_state==HIGH){    LED_state = LOW;  }  prev_state=current_state;  digitalWrite(13, LED_state);}This program on its own isn’t that great and doesn’t seem to be much useful, let us consider that the above sketch is part of a bigger program where an math intensive task runs in the void loop for about 2 second along with the above program.The resulting code would look like thisint prev_state = LOW;int LED_state;int current_state;void setup() {  pinMode(13, OUTPUT);  pinMode(9, INPUT_PULLUP);}void loop() {  current_state = digitalRead(9);  current_state = !current_state;  if(current_state ==HIGH &amp;amp;&amp;amp; prev_state==LOW){    LED_state = HIGH;  }  else if(current_state ==LOW &amp;amp;&amp;amp; prev_state==HIGH){    LED_state = LOW;  }  prev_state=current_state;  digitalWrite(13, LED_state);  delay(5000);}You can immediately notice that the push button is no longer responsive, and some of the inputs are not registered and processed. The Arduino is busy doing some other heavy math tasks for 5 seconds every loop cycle. Therefore the Arduino will not record any inputs during these 5 seconds.When we start writing bigger and more complex projects in arduino, reaction time to events is very crucial and interrupts are the arduinos way of dealing with time critical events.Interrupts &amp;amp; their typesA very technical and broad definition would be that  An Interrupt is a signal emitted by hardware or software when a process or an event needs immediate attention. It alerts the processor to a high priority process requiring interruption of the current working process.A more simple and understandable definion would be  An Interrupt’s job is to make sure that the processor responds quickly to important events. When a certain signal is detected, an Interrupt (as the name suggests) interrupts whatever the processor is doing, and executes some code designed to react to whatever external stimulus is being fed to the Arduino. Once that code has wrapped up, the processor goes back to whatever it was originally doing as if nothing happened.Arudino Uno has a wide varitiy of interrupts each specifically designed for a paticular task.We will be looking at three of them:  External Interrupts  Pin Change Interrupts  Timer Interrupts (covered in the next post)External InterruptsThese are the most commonly used interrupts, like the name suggests they are responsible for triggering an interrupt based on changes in signals originating outside the arduino, like button presses for example. Note that not all pins are capable of this and Digital Pins 2 and 3 are the only two pins that can detect external interrupts in the Arduino uno.External interrupts can be setup via two different methods we will now be looking at both the methods.Method 1We will be using the external interrupt on digital pin 2, to toggel an LED on pin 13 everytime we press we button. The trigger mode is set to rising.volatile int state = LOW; const int led = 13;const int button = 2;void setup(){  pinMode(led, OUTPUT);  pinMode(button,INPUT_PULLUP);  attachInterrupt(digitalPinToInterrupt(button),blinker,FALLING);}void blinker(){  state = state == HIGH ? LOW : HIGH;  digitalWrite(led,state);}void loop(){ delay(5000);//some random process to show that interrupts do interrupt.}  General syntax for attachInterrupt() is attachInterrupt(digitalPinToInterrupt(pin), ISR, mode)          The first parameter to attachInterrupt() is an interrupt number. Normally you should use digitalPinToInterrupt(pin) to translate the actual digital pin to the specific interrupt number. For example, if you connect to pin 3, use digitalPinToInterrupt(3) as the first parameter to attachInterrupt().      the ISR to call when the interrupt occurs; this function must take no parameters and return nothing. This function is sometimes referred to as an interrupt service routine.      mode: defines when the interrupt should be triggered. Four constants are predefined as valid values.                              LOW to trigger the interrupt whenever the pin is low,                                CHANGE to trigger the interrupt whenever the pin changes value                                RISING to trigger when the pin goes from low to high,                                FALLING for when the pin goes from high to low.                              Method 2We will be working on the same example that was demonstrated in method 1, but we will now be using port manipulation instead of pre-defined functions.The three main registers we will be working with are EICRA, EIMSK, EIFR.  EICRA (External Interrupt Control Register A)This register contains 4 bits two each for one external interrupt pin. They are used to select the trigger mode for the external interrupt. For our current example we are only conserned with the external interrupt on digital pin 2 which is refered to as INT0.  EIMSK (External Interrupt Mask Register)This register has two bits one for each of the external interrupts, they are called External Interrupt Request Enable bits, when set to high along with the Global Interrupts being enabled in the SREG. The external interrupt for that paticular pin will be enabled. Activity on the pin will cause an interrupt request even if INT1 is configured as an output. The corresponding interrupt of External Interrupt Request is executed from the Interrupt Vector.  EIFR (External Interrupt Flag Register)This register contains two bits one each for the external interrupts, When an edge or logic change on the INTx pin triggers an interrupt request, INTFx becomes set (one). If the Ibit in SREG and the INTx bit in EIMSK are set (one), the MCU will jump to the corresponding Interrupt Vector.The flag is cleared when the interrupt routine is executed. Alternatively, the flag can be cleared by writing a logical one to it. This flag is always cleared when INTx is configured as a level interrupt.ISR(INT0_vect){  PORTB ^= (1 &amp;lt;&amp;lt; 5); // Toggle the state of the LED}void setup(){  cli();  //Enable D13 as OUTPUT  DDRB |= (1 &amp;lt;&amp;lt; 5);  //Enable D2 as INPUT_PULLUP  DDRD &amp;amp;= ~(1 &amp;lt;&amp;lt; 2);  PORTD |= (1 &amp;lt;&amp;lt; 2);  //Enable Falling edge  EICRA = 0;  EICRA |= (1 &amp;lt;&amp;lt; ISC01);  //Enable Mask  EIMSK = 0;  EIMSK |= (1 &amp;lt;&amp;lt; INT0);  //Enable global interrupts  sei();}void loop(){  delay(5000); //some random process to show that interrupts do interrupt.}sei() - Enable Global Interrupts by setting the I-bit in the SREG register to one.cli() - Disable Global Interrupts by setting the I-Bit in the SREG register to zero.Pin Change InterruptsThe Pin Change Interrupt is triggered when the state of any of the pins with the PCINT enabled is changed. We primarly deal with three registers PCICR, PCIFR, PCMSKx.  PCICR (Pin Change Interrupt Control Register)When the PCIEx bit is set (one) and the I-bit in the Status Register (SREG) is set (one), pin change interrupt for that paticular port is enabled. Any change on any enabled PCINT pins will cause an interrupt. The corresponding interrupt of Pin Change Interrupt Request is executed from the PCIx Interrupt Vector. PCINT pins are enabled individually by the PCMSKx Register  PCIFR (Pin Change Interrupt Flag Register)When a logic change on any PCINT pins triggers an interrupt request, PCIFx becomes set (one). If the I-bit in SREG and the PCIEx bit in PCICR are set (one), the MCU will jump to the corresponding Interrupt Vector. The flag is cleared when the interrupt routine is executed. Alternatively, the flag can be cleared by writing a logical one to it.  PCMSKx (Pin Change Mask Register x)Each PCINT-bit selects whether pin change interrupt is enabled on the corresponding I/O pin. If PCINT is set and the PCIEx bit in PCICR is set, pin change interrupt is enabled on the corresponding I/O pin. If PCINT is cleared, pin change interrupt on the  corresponding I/O pin is disabled.  NOTE: Pin Change Interrupts do not have mode selection, therefore any change in the logic level will trigger an interrupt.Let us now look at a sketch that we earlier worked with using external interrupts, we will be using pin change interrupts on PORT B and C with masks enabled only for pins A1 and D9 to toggle and LED connected to digital pin 13. Given that a single press of the push button will raise the pin change interrupts twice as its senstive to any change and not just rising or falling, we also will be using a small polling based counter to toggle the LED like before.volatile int counter_1 =0;volatile int counter_2 =0;ISR (PCINT0_vect) //ISR for Digital Pin 9 {  counter_1++;  if (counter_1%2 ==0){     PORTB ^= (1&amp;lt;&amp;lt;5);   }   }ISR (PCINT1_vect) // ISR for A1 Pin {  counter_2++;  if (counter_2%2 ==0){    PORTB ^= (1&amp;lt;&amp;lt;5);   }   }void setup() {  cli(); // Disable Global Interrupts  PCICR = 0;   PCICR |= (1&amp;lt;&amp;lt;PCIE0)|(1&amp;lt;&amp;lt;PCIE1); // Enable Pin Change Interrupts for PORT B and C  PCMSK0 = 0; // Reset Masks  PCMSK1 = 0; // Reset Masks  PCMSK0 |= (1&amp;lt;&amp;lt;PCINT1); // Enable Pin Change mask for A1 Pin  PCMSK1 |= (1&amp;lt;&amp;lt;PCINT9); //// Enable Pin Change mask for Digital Pin 9  sei(); // Enable Global Interrupts  pinMode(13,OUTPUT);   pinMode(A1,INPUT_PULLUP);  pinMode(9,INPUT_PULLUP);}void loop() {  delay(5000); //some random process to show that interrupts do interrupt.} We can also add an millis based cooldown time once a trigger has been called to take care of debouncing and multiple interrupts being called for a single press. Further, we can even save the states of the pins and compare them to the states of the pins as soon as the interrupt has been called to find out which pin caused the interrupt to be triggered. These two applications are left for you to try them on your own.Input Servie RoutineISRs are special kinds of functions that have some unique limitations most other functions do not have. An ISR cannot have any parameters, and they shouldn’t return anything.Generally, an ISR should be as short and fast as possible. If your sketch uses multiple ISRs, only one can run at a time, other interrupts will be executed after the current one finishes in an order that depends on the priority they have. millis() relies on interrupts to count, so it will never increment inside an ISR. Since delay() requires interrupts to work, it will not work if called inside an ISR. micros() works initially but will start behaving erratically after 1-2 ms. delayMicroseconds() does not use any counter, so it will work as normal.Typically global variables are used to pass data between an ISR and the main program. To make sure variables shared between an ISR and the main program are updated correctly, declare them as volatile.Quick Facts about ISR  ISR’s have a priority order incase two or more of them are called at once.  Once inside an ISR the global interrupts are turned off, to prevent nested interrupt calls and once the ISR has finished its service the global interrupts are turned back on. You can manually turn on global interrupts inside an ISR for some crazy results.  Do not use Serial, Delay and other heavy functions inside an ISR.  External interrupts, pin-change interrupts, and the watchdog timer interrupt, can also be used to wake the processor up. This can be very handy, as in sleep mode the processor can be configured to use a lot less power (eg. around 10 microamps). A rising, falling, or low-level interrupt can be used to wake up a gadget (eg. if you press a button on it), or a “watchdog timer” interrupt might wake it up periodically (eg. to check the time or temperature). Pin-change interrupts could be used to wake the processor if a key is pressed on a keypad, or similar. The processor can also be awoken by a timer interrupt (eg. a timer reaching a certain value, or overflowing) and certain other events, such as an incoming I2C message.  You must have seen me using cli() to disable interrupts quite often in the examples covered above, this is to make sure an interrupt isn’t triggered while I am writing data to the registers.External ResourcesNick Gammon’s notes on Interrupts"
  },
  {
    "title": "Robotics Theory - 2D Robotics",
    "url": "/posts/Robotics-Theory-2D-Robotics/",
    "categories": "Resources, Robotics Theory",
    "tags": "matlab, manipulator, differential drive, projectile motion",
    "date": "2020-05-04 23:47:20 +0530",
    "snippet": "IntroductionCoordinate Transformation in 2DThe motion of manipulators and robots is often very complex and mathematically demanding. A single coordinate frame usually isn’t sufficient, and its often convenient to use multiplecoordinate frames (fixed or moving) to make things easier.TerminologyFixed and World coordinate frames are used interchangeably and mean the same thing.Mobile and Moving coordinate frames are used interchangeably and mean the same thing.What is a Coordinate Frame ?If $p$ is a vector in $R^n$ and $X = {x^1 , x^2 , …x^n}$ be a complete orthonormal set of $R^n$, thencoordinates of $p$ with respect to $X$ are denoted as $[p]^x$.\\[\\begin{aligned}	p &amp;amp;= \\Sigma{[p]_k^x}\\hat{x^k} \\\\	[p]_k^x &amp;amp;= p.x^k\\end{aligned}\\]TranslationAssume a fixed frame $(O_0 − X_0 − Y_0 )$ and a mobile frame $(O_1 − X_1 − Y_1 )$. Let $C$ be a pointin space. The relation between both the frames and $C$ can be derived using simple math.The following inferences can be made from the above figure:Position of C $\\mathit{wrt}$ frame $(O_0-X_0-Y_0)$ can be represented as\\[\\begin{equation*}	C^0 = \\begin{bmatrix}		X_C^0\\\\		Y_C^0	\\end{bmatrix}\\end{equation*}\\]Position of C $\\textit{wrt}$ frame $(O_1-X_1-Y_1)$ can be represented as\\[\\begin{equation*}	C^1 = \\begin{bmatrix}		X_C^1\\\\		Y_C^1	\\end{bmatrix}\\end{equation*}\\]Position of $O_1$ $\\textit{wrt}$ frame $(O_0-X_0-Y_0)$ can be represented as\\[\\begin{equation*}	O_1^0 = \\begin{bmatrix}		X_0\\\\		Y_0	\\end{bmatrix}\\end{equation*}\\]Position of $O_0$ $\\textit{wrt}$ frame $(O_1-X_1-Y_1)$ can be represented as\\[\\begin{equation*}	O_0^1 = \\begin{bmatrix}		-X_0\\\\		-Y_0	\\end{bmatrix}\\end{equation*}\\]RotationAssume a fixed frame $(O_0-X_0-Y_0)$ and a mobile frame $(O_1-X_1-Y_1)$. Let C be a point in space. The relation between both the frames and C can be derived using simple math.The following equations can be written down based on the above figure\\[\\hat{X_1} = (X_1\\cos{\\theta})\\hat{X_0} + (X_1\\sin{\\theta})\\hat{Y_0}\\]\\[\\hat{Y_1} = - (Y_1\\sin{\\theta})\\hat{X_0} + (Y_1\\cos{\\theta})\\hat{Y_0}\\]Upon rearranging the terms,\\[\\begin{equation*}	\\begin{bmatrix}		\\hat{X_1}\\\\		\\hat{Y_1}\\\\	\\end{bmatrix} = \\begin{bmatrix} 	\\cos{\\theta} &amp;amp; \\sin{\\theta}\\\\ 	-\\sin{\\theta} &amp;amp; \\cos{\\theta}\\\\ \\end{bmatrix}\\begin{bmatrix}	\\hat{X_0}\\\\	\\hat{Y_0}\\\\\\end{bmatrix}\\end{equation*}\\]The above trigonometric matrix is called the rotation matrix, denoted by $R$. The relation between the coordinates of point C in the fixed and mobile frame can therefore be expressed as:\\(C^1 = R_0^1C^0\\)Combined Translation &amp;amp; RotationAssume a fixed frame $(O_0-X_0-Y_0)$ and a mobile frame $(O_2-X_2-Y_2)$. Let us assume an intermediate mobile frame given by $(O_1-X_1-Y_1)$. Let C be a point of interest, The frame $O_2$ can be generated by translation from frame $O_0$ to give frame $O_1$ and then rotation by $\\theta$ to give frame $O_2$.The coordinates of point C between the mobile and fixed frame can be related using the following:\\[C^0 = O_1^0 + R_1^0C^1\\]2D - RR ManipulatorConsider the following manipulator diagram for a 2 DoF planar robotic arm.$P^1 = \\begin{bmatrix}	l_1	0\\end{bmatrix};$$Q^2 = \\begin{bmatrix}	l_2	0\\end{bmatrix};$\\[R_1^0 = \\begin{bmatrix}	\\cos{\\theta_1} &amp;amp; -\\sin{\\theta_1} \\\\	\\sin{\\theta_1} &amp;amp; \\cos{\\theta_1} \\\\\\end{bmatrix}\\]\\[R_2^1 = \\begin{bmatrix}	\\cos{\\theta_2} &amp;amp; -\\sin{\\theta_2}\\\\	\\sin{\\theta_2} &amp;amp; \\cos{\\theta_2}\\\\\\end{bmatrix}\\]$P^0 = R_1^0P^1$$Q^1 = R_2^1Q^2 + P^1$$Q^0 = R_1^0Q^1 = R_1^0(R_2^1Q^2 + P^1)$\\[\\begin{align}	P^0 &amp;amp;= \\begin{bmatrix}		X_p^0\\\\		Y_p^0\\\\	\\end{bmatrix}=\\begin{bmatrix}	l_1\\cos{\\theta_1}\\\\	l_1\\sin{\\theta_1}\\end{bmatrix}\\\\Q^0 &amp;amp;= \\begin{bmatrix}	X_Q^0\\\\	Y_Q^0\\\\\\end{bmatrix}= \\begin{bmatrix}	l_1\\cos{\\theta_1} + l_2\\cos{(\\theta_1+\\theta_2)}\\\\	l_1\\sin{\\theta_1} + l_2\\sin{(\\theta_1+\\theta_2)}\\\\\\end{bmatrix}\\end{align}\\]Forward KinematicsThe equations for the RR manipulator can be used to draw the arm in MATLAB, the code for the same is given below.%% Lec1_RRManu.m %%	clc%clear allclose all%% Manipulator Parametersl1 = 0.5; l2 = 0.3;theta1 = 0.3; theta2 = 0.4;%% World Framex_0 = 0; y_0 = 0;%% Link 1x_p0 = l1 * cos(theta1);y_p0 = l1 * sin(theta1);%% Link 2x_q0 = l1 * cos(theta1) + l2 * cos(theta1 + theta2);y_q0 = l1 * sin(theta1) + l2 * sin(theta1 + theta2);%% Draw Linesline([x_0, x_p0], [y_0, y_p0], &#39;Linewidth&#39;, 5, &#39;Color&#39;, [204, 204, 1] / 255);line([x_q0, x_p0], [y_q0, y_p0], &#39;Linewidth&#39;, 5, &#39;Color&#39;, &#39;cyan&#39;);axis(&#39;equal&#39;);xlabel(&#39;x&#39;);ylabel(&#39;y&#39;);title(&#39;RR-Manipulator&#39;)Upon executing the code you should be able see something similar in your figure window.Inverse KinematicsWe can also solve for $\\theta_1$ and $\\theta_2$ given the end-effector position. This is most useful in everyday life implementation as we often know one way or another to which position our arm should move. This approach is called inverse kinematics. Below given program is a simple example on how we can use matlab to solve the obtained system of non-linear equations for a given value of end-effector positions  $\\verb|x_ref,y_ref|$, using $fsolve$. Other numerical methods for solving system of non-linear equations that can be implemented from scratch will be discussed in later sections.%% Lec2_InverseRRManu.m %%clcclose alll1 = 0.5; l2 = 0.5;x_ref = 0.5; y_ref = 0;param = [l1 l2 x_ref y_ref];x0 = [0.1 0.1];[x, fval, exitflag] = fsolve(@Lec2_InvFun, x0, optimoptions(&#39;fsolve&#39;, &#39;Display&#39;, &#39;iter&#39;), param)theta1 = x(1);theta2 = x(2);%% World Framex_0 = 0; y_0 = 0;%% Link 1x_p0 = l1 * cos(theta1);y_p0 = l1 * sin(theta1);%% Link 2x_q0 = l1 * cos(theta1) + l2 * cos(theta1 + theta2);y_q0 = l1 * sin(theta1) + l2 * sin(theta1 + theta2);%% Draw Linesline([x_0, x_p0], [y_0, y_p0], &#39;Linewidth&#39;, 5, &#39;Color&#39;, [204, 204, 1] / 255);line([x_q0, x_p0], [y_q0, y_p0], &#39;Linewidth&#39;, 5, &#39;Color&#39;, &#39;cyan&#39;);axis(&#39;equal&#39;);xlabel(&#39;x&#39;);ylabel(&#39;y&#39;);title(&#39;RR-Manipulator&#39;)As you might have noticed the above program depends on an external function called \\verb|Lec2_InvFun|. The code for the function is given below.%% Lec2_InvFun.m %%function F = Lec2_InvFun(x, param)l1 = param(1); l2 = param(2);x_ref = param(3); y_ref = param(4);theta1 = x(1); theta2 = x(2);x_q0 = l1 * cos(theta1) + l2 * cos(theta1 + theta2);y_q0 = l1 * sin(theta1) + l2 * sin(theta1 + theta2);F = [x_q0 - x_ref; y_q0 - y_ref];endUpon executing the code you should be able see something similar in your figure windowTracing TrajectoriesIf we can write the mathematical equation for a curve the x,y points on the curve can be passed into the inverse kinematics block for the manipulator which allows the robotic arm to trace the shape of the curve. The below given program does the same, it also shows the animation of the manipulator. Note, that the below given program uses $\\verb|Lec2_InvFun|$ for running.%% Lec2_TrajManu.m %%clcclose all%% Parameter Settingl1 = 1; l2 = 1;%% Trajectory Generatorphi = linspace(0, 2 * pi, 51);x_ref = 1 + 0.5 * cos(phi); y_ref = 0.5 + 0.5 * sin(phi);theta1 = 0.5; theta2 = 0.5;theta1_f = zeros(length(phi), 1);theta2_f = zeros(length(phi), 1);for i = 1:length(phi)    theta = [theta1, theta2];    options = optimoptions(&#39;fsolve&#39;, &#39;Display&#39;, &#39;Off&#39;, &#39;MaxIter&#39;, 100, &#39;MaxFunEval&#39;, 300);    param = [l1 l2 x_ref(i) y_ref(i)];    [x, fval, exitflag] = fsolve(@Lec2_InvFun, theta, options, param);     theta1 = x(1); theta2 = x(2);     theta1_f(i, 1) = x(1);    theta2_f(i, 1) = x(2); end%% World Framex_0 = 0; y_0 = 0;%% Animationfor i = 1:length(phi)     % Link 1    x_p0 = l1 * cos(theta1_f(i));    y_p0 = l1 * sin(theta1_f(i));     % Link 2    x_q0 = l1 * cos(theta1_f(i)) + l2 * cos(theta1_f(i) + theta2_f(i));    y_q0 = l1 * sin(theta1_f(i)) + l2 * sin(theta1_f(i) + theta2_f(i));     % Tracer    plot(x_q0, y_q0, &#39;ko&#39;, &quot;MarkerSize&quot;, 5, &#39;MarkerEdgeColor&#39;, &#39;k&#39;, &#39;MarkerFaceColor&#39;, &#39;k&#39;);    hold on;     h1 = line([x_0, x_p0], [y_0, y_p0], &#39;Linewidth&#39;, 5, &#39;Color&#39;, [204, 204, 1] / 255);    h2 = line([x_q0, x_p0], [y_q0, y_p0], &#39;Linewidth&#39;, 5, &#39;Color&#39;, &#39;cyan&#39;);    grid on;    axis(&#39;equal&#39;);    xlabel(&#39;x&#39;);    ylabel(&#39;y&#39;);    title(&#39;RR-Manipulator&#39;)    xlim([- 2 2]);    ylim([- 2 2]);     pause(0.3);    if i &amp;lt; length(phi)        delete(h1);        delete(h2);    endendUpon executing the code you should be able see something similar in your figure windowDifferential DriveConsider a fixed frame ($O_0-X_0-Y_0$) and a mobile frame ($O_1-X_1-Y_1$), the mobile frame is attached to a two wheel differential drive robot, such that the $X_1$ axis is pointing forwards and the $Y_1$ is perpendicular to the orientation of the wheels.Let $r$ be the radius of the wheels and $2b$ be the distance between the wheels, and $\\theta$ be the angle the angle the mobile frame makes with the horizontal of the fixed frame. Forward KinematicsThe forward kinematics of the differential drive car involves finding ($X_C^0(t),Y_C^0(t)$) given $b,r$, initial position ($X_C^0(t=0),Y_C^0(t=0)$) and the driving history of the robot  ($\\dot{\\phi_r}(t),\\dot{\\phi_l}(t)$).The distance traveled by the wheel if it rotates by an angle $\\phi$ is given by $r\\phi$. The rate of change of distance travelled by the wheel is therefore given by $r\\dot{\\phi}$. Assuming the right wheel is denoted by the subscript r, similar assumptions and derivations can be made for the left wheel.Case 1: $\\dot{\\phi_l}=0, \\dot{\\phi_r}\\neq0$The distance moved by the right wheel is given by, $X_{wr}=r\\phi_r$. The velocity of the right wheel is given by,\\[\\dot{X_{wr}^1} =r\\dot{\\phi_r}\\]From geometry, the speed of the center C in the direction of $X_1$ can be written as,\\[\\dot{X_C^1}= 0.5r\\dot{\\phi_r}\\]Case 2: $\\dot{\\phi_l}\\neq0, \\dot{\\phi_r}=0$From symmetry similar arguments can be made for the left wheel, giving the following equations.\\[\\dot{X_{wl}^1} =r\\dot{\\phi_l}\\]\\[\\dot{X_C^1}= 0.5r\\dot{\\phi_l}\\]From case 1 \\&amp;amp; 2, the velocity of the point C in the directions of $X_1$ and $Y_1$ can be written down as,\\[\\begin{align}	\\dot{X_C^1} &amp;amp;= 0.5r(\\dot{\\phi_l}+\\dot{\\phi_r})\\\\	\\dot{Y_C^1} &amp;amp;= 0\\end{align}\\]The velocity of point C in frame 0 and 1 can be related as,\\[\\begin{align*}	\\dot{C^0} &amp;amp;= R_1^0\\dot{C^1}\\\\	\\begin{bmatrix}		\\dot{X_C^0}\\\\		\\dot{Y_C^0}	\\end{bmatrix} &amp;amp;=	\\begin{bmatrix}		\\cos{\\theta} &amp;amp; -\\sin{\\theta}\\\\		\\sin{\\theta} &amp;amp; \\cos{\\theta}\\\\ 	\\end{bmatrix}	\\begin{bmatrix}		\\dot{X_C^1}\\\\		\\dot{Y_C^1}	\\end{bmatrix}\\\\	\\begin{bmatrix}		\\dot{X_C^0}\\\\		\\dot{Y_C^0}	\\end{bmatrix} &amp;amp;=	\\begin{bmatrix}		\\cos{\\theta} &amp;amp; -\\sin{\\theta}\\\\		\\sin{\\theta} &amp;amp; \\cos{\\theta}\\\\ 	\\end{bmatrix}	\\begin{bmatrix}		0.5r(\\dot{\\phi_r}+\\dot{\\phi_l})\\\\		0	\\end{bmatrix}\\\\	\\dot{X_C^0(t)} &amp;amp;= 0.5r(\\dot{\\phi_r}+\\dot{\\phi_l})\\cos{\\theta}\\\\	\\dot{Y_C^0(t)} &amp;amp;= 0.5r(\\dot{\\phi_r}+\\dot{\\phi_l})\\sin{\\theta}\\end{align*}\\]For finding the expression for rate of change of $\\theta$,Case 1: $\\dot{\\phi_l}=0, \\dot{\\phi_r}\\neq0$\\[X_{wr} = r\\phi_r\\]from geometry,\\[X_{wr} = 2b\\theta\\]therefore gives,\\[\\dot{\\theta} = \\dfrac{r\\dot{\\phi_r}}{2b}\\]Similar arguments can be made for the other case, only exception being the angle $\\theta$ is negative. Therefore from case 1 and 2 the following can be derived.\\[\\dot{\\theta} = \\dfrac{r}{2b}(\\dot{\\phi_r}-\\dot{\\phi_l})\\]Therefore to summarize,\\[\\begin{align*}\\dot{X_C^0} &amp;amp;= v\\cos{\\theta}\\\\\\dot{Y_C^0} &amp;amp;= v\\sin{\\theta}\\\\\\dot{\\theta} &amp;amp;= \\omega\\\\v &amp;amp;= 0.5r(\\dot{\\phi_r}+\\dot{\\phi_l})\\\\w &amp;amp;= \\dfrac{r}{2b}(\\dot{\\phi_r}-\\dot{\\phi_l})\\end{align*}\\]To find $X_C^0,Y_C^0,\\theta$, we will be using Euler’s integration.\\[\\begin{align*}	\\dot{X_C^0} &amp;amp;= \\dfrac{X_C(t_{i+1})-X_C(t_i)}{t_{i+1}-t_i} = v(t_i)\\cos{\\theta(t_i)}\\\\ 	\\dot{Y_C^0} &amp;amp;= \\dfrac{Y_C(t_{i+1})-Y_C(t_i)}{t_{i+1}-t_i} = v(t_i)\\sin{\\theta(t_i)}\\\\	\\dot{\\theta} &amp;amp;= \\dfrac{\\theta(t_{i+1})-\\theta(t_i)}{t_{i+1}-t_i} = \\omega(t_i)\\end{align*}\\]Upon rearranging the terms,\\[\\begin{align*}	X_C^0(t_{i+1}) &amp;amp;= X_C^0(t_{i}) + hv(t_{i})\\cos{\\theta(t_i)}\\\\	Y_C^0(t_{i+1}) &amp;amp;= Y_C^0(t_{i}) + hv(t_{i})\\sin{\\theta(t_i)}\\\\	\\theta(t_{i+1}) &amp;amp;= \\theta(t_i) + h\\omega(t_i)\\end{align*}\\]%% Lec3_DiffCar.m %%clcclose allparms.R = 0.1; %Radius of the chassis for animation% parms.writeMovie = 0; %set to 1 to get a movie output% parms.nameMovie = &#39;car.avi&#39;;%% Initialize Statesfps = 10;parms.delay = 0.2;z0 = [0 0 - pi / 2]; %initial position of the carh = 0.1; %step size%% Motiont1 = 0:h:1;speed1 = ones(1, length(t1));omega1 = zeros(1, length(t1));t2 = 1 + h:h:2;speed2 = zeros(1, length(t2));omega2 = pi / 2 * ones(1, length(t2));t3 = 2 + h:h:3;speed3 = ones(1, length(t3));omega3 = zeros(1, length(t3));t4 = 3 + h:h:4;t5 = 4 + h:h:5;t6 = 5 + h:h:6;t7 = 6 + h:h:7;t = [t1 t2 t3 t4 t5 t6 t7];speed = [speed1 speed2 speed3 speed2 speed3 speed2 speed3];omega = [omega1 omega2 omega3 omega2 omega3 omega2 omega3];%% Euler Integrationz = z0; %Initial parametersfor i = 1:length(t) - 1    u = [speed(i) omega(i)];    zz = Lec3_Euler(h, z0, u);    z0 = zz;    z = [z; z0];end% t_interp = linspace(0,t(end),fps*t(end));% [m,n] = size(z);% for i=1:n%     z_interp(:,i) = interp1(t,z(:,i),t_interp);% end%% Animationfigure(1)Lec3_Animate(t, z, parms);%% Lec3_Animate.m %%function Lec3_Animate(t, z, parms)R = parms.R;phi = linspace(0, 2 * pi);x_circle = R * cos(phi);y_circle = R * sin(phi);% if (parms.writeMovie)%     mov = VideoWriter(parms.nameMovie);%     open(mov);%endn = length(t);for i = 1:n    theta = z(i, 3);    x_robot = z(i, 1) + x_circle;    y_robot = z(i, 2) + y_circle;    x_dir = z(i, 1) + [0 R * cos(theta)];    y_dir = z(i, 2) + [0 R * sin(theta)];     plot(z(1:i, 1), z(1:i, 2), &#39;r&#39;, &#39;Linewidth&#39;, 2); hold on;    light_blue = [176, 224, 230] / 255;    h1 = patch(x_robot, y_robot, light_blue);    h2 = line(x_dir, y_dir, &#39;Color&#39;, &#39;black&#39;, &#39;Linewidth&#39;, 2);    axis(&#39;equal&#39;);    %span = max([-min(min(z(:,1:2))) max(max(z(:,1:2)))]);    %span = max([span 2]);    %axis([-span span -span span]);    axis([- 2 2 - 2 2]);    grid on;     pause(parms.delay)    delete(h1);    delete(h2);    %     if (parms.writeMovie)    %         axis off %does not show axis    %         set(gcf,&#39;Color&#39;,[1,1,1]) %set background to white    %         writeVideo(mov,getframe);    %     end end% if (parms.writeMovie)%     close(mov);end%% Lec3_Euler.m %%function F = Lec3_Euler(h, z0, u)v = u(1);omega = u(2);x_t0 = z0(1);y_t0 = z0(2);theta_t0 = z0(3);xdot_c = v * cos(theta_t0);ydot_c = v * sin(theta_t0);thetadot = omega;x_t1 = x_t0 + xdot_c * h;y_t1 = y_t0 + ydot_c * h;theta_t1 = theta_t0 + thetadot * h;F = [x_t1, y_t1, theta_t1];endUpon successful execution of the above program you should be able to see a figure similar to thisInverse KinematicsInverse Kinematics is defined as finding the values of $v,\\omega$ given ($X_P^0,Y_p^0$) as a function of time, where $P$ is a point fixed on the robot as shown in the figure.From previous sections, we have the following relations\\[\\begin{align*}	P^0 &amp;amp;= R_1^0P^1\\\\	C^0 &amp;amp;= R_1^0C^1\\\\	P^0 - C^0 &amp;amp;= R_1^0(P^1-C^1)\\\\	\\begin{bmatrix}		X_P^0 - X_C^0\\\\		X_P^0 - Y_C^0	\\end{bmatrix} &amp;amp;=	\\begin{bmatrix}		\\cos{\\theta} &amp;amp; -\\sin{\\theta}\\\\		\\sin{\\theta} &amp;amp; \\cos{\\theta}	\\end{bmatrix}	\\begin{bmatrix}		X_P^1 - X_C^1\\\\		Y_p^1 - Y_C^1	\\end{bmatrix}\\\\	\\begin{bmatrix}		X_P^0 - X_C^0\\\\		X_P^0 - Y_C^0	\\end{bmatrix} &amp;amp;=	\\begin{bmatrix}		\\cos{\\theta} &amp;amp; -\\sin{\\theta}\\\\		\\sin{\\theta} &amp;amp; \\cos{\\theta}	\\end{bmatrix}	\\begin{bmatrix}		P_X\\\\		P_Y	\\end{bmatrix}\\\\	\\begin{bmatrix}		X_P^0\\\\		Y_P^0	\\end{bmatrix} &amp;amp;=	\\begin{bmatrix}		X_C^0\\\\		Y_C^0	\\end{bmatrix} + \\begin{bmatrix}	\\cos{\\theta} &amp;amp; -\\sin{\\theta}\\\\	\\sin{\\theta} &amp;amp; \\cos{\\theta}\\end{bmatrix}\\begin{bmatrix}	P_X\\\\	P_Y\\end{bmatrix}\\\\	\\begin{bmatrix}	X_C^0\\\\	Y_C^0\\end{bmatrix} &amp;amp;=\\begin{bmatrix}	X_P^0\\\\	Y_P^0\\end{bmatrix} + \\begin{bmatrix}	\\cos{\\theta} &amp;amp; -\\sin{\\theta}\\\\	\\sin{\\theta} &amp;amp; \\cos{\\theta}\\end{bmatrix}\\begin{bmatrix}	P_X\\\\	P_Y\\end{bmatrix}\\\\\\end{align*}\\]Differentiating the above equations with time,\\[\\begin{align*}	\\begin{bmatrix}		\\dot{X_P^0}\\\\		\\dot{Y_P^0}	\\end{bmatrix} &amp;amp;=	\\begin{bmatrix}		\\dot{X_C^0}\\\\		\\dot{Y_C^0}	\\end{bmatrix} + 	\\begin{bmatrix}		\\dot{\\theta}\\sin{\\theta} &amp;amp; -\\dot{\\theta}\\cos{\\theta}\\\\		\\dot{\\theta}\\cos{\\theta} &amp;amp; -\\dot{\\theta}\\sin{\\theta}	\\end{bmatrix}	\\begin{bmatrix}		P_X\\\\		P_Y	\\end{bmatrix}\\\\	\\begin{bmatrix}		\\dot{X_P^0}\\\\		\\dot{Y_P^0}	\\end{bmatrix} &amp;amp;=	\\begin{bmatrix}		v\\cos{\\theta}\\\\		v\\sin{\\theta}	\\end{bmatrix} + 	\\begin{bmatrix}		\\omega\\sin{\\theta} &amp;amp; -\\omega\\cos{\\theta}\\\\		\\omega\\cos{\\theta} &amp;amp; -\\omega\\sin{\\theta}	\\end{bmatrix}	\\begin{bmatrix}		P_X\\\\		P_Y	\\end{bmatrix}\\\\	\\begin{bmatrix}		\\dot{X_P^0}\\\\		\\dot{Y_P^0}	\\end{bmatrix} &amp;amp;=	\\begin{bmatrix}		\\cos{\\theta} &amp;amp; (-P_X\\sin{\\theta} - P_Y\\cos{\\theta})\\\\		\\sin{\\theta} &amp;amp; (P_X\\cos{\\theta} - P_Y\\sin{\\theta})	\\end{bmatrix}	\\begin{bmatrix}		v\\\\		\\omega	\\end{bmatrix}\\end{align*}\\]We will be using a proportional controller for driving the error towards zero.\\[\\dot{X_P^0} = K_{P_x}(X_{ref}-X_P^0)\\]\\[\\dot{Y_P^0} = K_{P_Y}(Y_{ref}-Y_P^0)\\]Where $K_{P_x},K_{P_y}$ are user chosen gains and $X_{ref},Y_{ref}$ are the desired reference points. Substituting these values in the above derived equations.\\(\\begin{align*}		\\begin{bmatrix}			K_{P_x}(X_{ref}-X_P^0)\\\\			K_{P_Y}(Y_{ref}-Y_P^0)		\\end{bmatrix} &amp;amp;=		\\begin{bmatrix}			\\cos{\\theta} &amp;amp; (-P_X\\sin{\\theta} - P_Y\\cos{\\theta})\\\\			\\sin{\\theta} &amp;amp; (P_X\\cos{\\theta} - P_Y\\sin{\\theta})		\\end{bmatrix}		\\begin{bmatrix}			v\\\\			\\omega		\\end{bmatrix}\\\\		\\begin{bmatrix}			v\\\\			\\omega		\\end{bmatrix} &amp;amp;=		\\begin{bmatrix}			\\cos{\\theta} - \\left(\\dfrac{P_X}{P_Y}\\right) \\sin{\\theta} &amp;amp; \\sin{\\theta} + \\left( \\dfrac{P_X}{P_Y}\\right) \\cos{\\theta}\\\\			-\\left( \\dfrac{1}{P_X}\\right) \\sin{\\theta} &amp;amp; -\\left( \\dfrac{1}{P_X}\\right) \\cos{\\theta} 		\\end{bmatrix}		\\begin{bmatrix}			K_{P_x}(X_{ref}-X_P^0)\\\\			K_{P_Y}(Y_{ref}-Y_P^0)		\\end{bmatrix}	\\end{align*}\\)Also from forward kinematics we know the following,\\[\\begin{align*}	\\dot{X_C^0} &amp;amp;= v\\cos{\\theta}\\\\	\\dot{Y_C^0} &amp;amp;= v\\sin{\\theta}\\\\	\\dot{\\theta} &amp;amp;= \\omega\\end{align*}\\]All of the above derived equations can be used to create a trajectory following differential drive robot the code attached below follows a mathematical trajectory called “astroid” the code also has an PD controller implemented to keep the error close to zero.%% Lec4_InvDiff.m %%clcclose all%% Parametersparms.R = 0.1;parms.px = 0.05;parms.py = 0;parms.Kp = 100;parms.Kd = 0.5;parms.delay = 0.001;t0 = 0;tend = 10;fps = 10;t = t0:0.01:tend;h = 0.01;%% Trajectoryx_center = 0;y_center = 0;a = 1;x_ref = x_center + a * cos(2 * pi * t / tend) .^ 3;y_ref = y_center + a * sin(2 * pi * t / tend) .^ 3;%% Initilizationtheta0 = pi / 2;[x0 y0] = Lec4_ptP_to_ptC(x_ref(1), y_ref(1), theta0, parms);z0 = [x0 y0 theta0];z = z0;e = [0 0];v = 0;omega = 0;for i = 1:length(t) - 1    x_c = z(end, 1);    y_c = z(end, 2);    theta = z(end, 3);     [x_p, y_p] = Lec4_ptC_to_ptP(x_c, y_c, theta, parms);     error = [x_ref(i + 1) - x_p y_ref(i + 1) - y_p];    e = [e; error];    err_p_dot = [parms.Kp * error(1) + parms.Kd * ((e(end, 1) - e(end - 1, 1)) / h); parms.Kp * error(2) + parms.Kd * ((e(end, 2) - e(end - 1, 2)) / h)];    c = cos(theta); s = sin(theta);    px = parms.px; py = parms.py;    A = [c - (py / px) * s s + (py / px) * c; ...     - (1 / px) * s (1 / px) * c];    u = A * err_p_dot; %u = [v omega]    v = [v; u(1)];    omega = [omega; u(2)];    %zz = ode4(@Lec4_RHS,[t(i) t(i+1)],z0,u);    zz = Lec3_Euler(h, z0, u);    z0 = zz(end, :);    z = [z; z0];endt_interp = linspace(t0, tend, fps * tend);[m, n] = size(z);for i = 1:n    z_interp(:, i) = interp1(t, z(:, i), t_interp);endfigure(1)Lec3_Animate(t_interp, z_interp, parms);figure(2)subplot(2, 1, 1)plot(t, v, &#39;m&#39;); hold onplot(t, omega, &#39;c&#39;);ylabel(&#39;Velocity&#39;);legend(&#39;v&#39;, &#39;\\omega&#39;, &#39;Location&#39;, &#39;Best&#39;);subplot(2, 1, 2)plot(t, e(:, 1), &#39;r&#39;); hold onplot(t, e(:, 2), &#39;b&#39;);legend(&#39;error x&#39;, &#39;error y&#39;, &#39;Location&#39;, &#39;Best&#39;);ylabel(&#39;error&#39;);xlabel(&#39;time&#39;);%% Lec4_ptC_to_ptP.m %%function [x_p, y_p] = Lec4_ptC_to_ptP(x_c, y_c, theta, parms)c = cos(theta); s = sin(theta);p = [parms.px; parms.py];Xp = [c - s; s c] * p + [x_c; y_c];x_p = Xp(1); y_p = Xp(2);end%% Lec4_ptP_to_ptC.m %%function [x_c, y_c] = Lec4_ptP_to_ptC(x_p, y_p, theta, parms)c = cos(theta); s = sin(theta);p = [parms.px; parms.py];Xc = - [c - s; s c] * p + [x_p; y_p];x_c = Xc(1); y_c = Xc(2);end%% ode4.m %%function Y = ode4(odefun, tspan, y0, varargin)%ODE4  Solve differential equations with a non-adaptive method of order 4.%   Y = ODE4(ODEFUN,TSPAN,Y0) with TSPAN = [T1, T2, T3, ... TN] integrates%   the system of differential equations y&#39; = f(t,y) by stepping from T0 to%   T1 to TN. Function ODEFUN(T,Y) must return f(t,y) in a column vector.%   The vector Y0 is the initial conditions at T0. Each row in the solution%   array Y corresponds to a time specified in TSPAN.%%   Y = ODE4(ODEFUN,TSPAN,Y0,P1,P2...) passes the additional parameters%   P1,P2... to the derivative function as ODEFUN(T,Y,P1,P2...).%%   This is a non-adaptive solver. The step sequence is determined by TSPAN%   but the derivative function ODEFUN is evaluated multiple times per step.%   The solver implements the classical Runge-Kutta method of order 4.%%   Example%         tspan = 0:0.1:20;%         y = ode4(@vdp1,tspan,[2 0]);%         plot(tspan,y(:,1));%     solves the system y&#39; = vdp1(t,y) with a constant step size of 0.1,%     and plots the first component of the solution.%if ~ isnumeric(tspan)    error(&#39;TSPAN should be a vector of integration steps.&#39;);endif ~ isnumeric(y0)    error(&#39;Y0 should be a vector of initial conditions.&#39;);endh = diff(tspan);if any(sign(h(1)) * h &amp;lt;= 0)    error(&#39;Entries of TSPAN are not in order.&#39;)endtry    f0 = feval(odefun, tspan(1), y0, varargin{:});catch    msg = [&#39;Unable to evaluate the ODEFUN at t0,y0. &#39;, lasterr];    error(msg);endy0 = y0(:); % Make a column vector.if ~ isequal(size(y0), size(f0))    error(&#39;Inconsistent sizes of Y0 and f(t0,y0).&#39;);endneq = length(y0);N = length(tspan);Y = zeros(neq, N);F = zeros(neq, 4);Y(:, 1) = y0;for i = 2:N    ti = tspan(i - 1);    hi = h(i - 1);    yi = Y(:, i - 1);    F(:, 1) = feval(odefun, ti, yi, varargin{:});    F(:, 2) = feval(odefun, ti + 0.5 * hi, yi + 0.5 * hi * F(:, 1), varargin{:});    F(:, 3) = feval(odefun, ti + 0.5 * hi, yi + 0.5 * hi * F(:, 2), varargin{:});    F(:, 4) = feval(odefun, tspan(i), yi + hi * F(:, 3), varargin{:});    Y(:, i) = yi + (hi / 6) * (F(:, 1) + 2 * F(:, 2) + 2 * F(:, 3) + F(:, 4));endY = Y.&#39;;%% Lec4_RHS.m %%function zdot = Lec4_RHS(t, z, u)v = u(1); omega = u(2);theta = z(3);x_c_dot = v * cos(theta);y_c_dot = v * sin(theta);theta_dot = omega;zdot = [x_c_dot y_c_dot theta_dot]&#39;;Upon successful execution of the above program, you will end up with the following two figures.Projectile Motion2-D projectile motion will be analyzed in this section, Euler Lagrange Equations will be introduced and used for finding out the equations of motion of the projectile.Forward KinematicsLet the position of the projectile wrt to the fixed frame be denoted as ($X,Y$). The launch angle is denoted by $\\theta$ and the drag force is given by $\\vec{F_d}=-Cv^2\\hat{v}$.\\[\\vec{F_d}=-Cv^2\\hat{v} = -Cv^2\\dfrac{\\vec{v}}{|\\vec{v}|} = -C|v|\\hat{v}\\]\\[F_x = -C\\dot{x}\\sqrt{\\dot{x}^2 + \\dot{y}^2}\\]\\[F_y = -C\\dot{y}\\sqrt{\\dot{x}^2 + \\dot{y}^2}\\]From Euler-Lagrange Formulaizm,\\[\\begin{align*}	L &amp;amp;= T - V\\\\	T &amp;amp;= 0.5mv^2 = 0.5m(\\dot{x}^2+\\dot{y}^2)\\\\	V &amp;amp;= mgy\\\\	L &amp;amp;= 0.5m(\\dot{x}^2+\\dot{y}^2) - mgy\\end{align*}\\]Given that,\\[\\begin{align*}	\\dfrac{d}{dt}\\left(\\dfrac{\\partial L}{\\partial \\dot{q_j}}\\right) - \\dfrac{\\partial L}{\\partial q_j} &amp;amp;= Q_j\\end{align*}\\]In the direction of x,\\[\\begin{align*}	\\dfrac{d}{dt}\\left(\\dfrac{\\partial (0.5m(\\dot{x}^2+\\dot{y}^2 )-mgy)}{\\partial \\dot{x}}\\right) - \\dfrac{\\partial (0.5m(\\dot{x}^2+\\dot{y}^2 )-mgy)}{\\partial x} &amp;amp;= -C\\dot{x}\\sqrt{\\dot{x}^2+\\dot{y}^2}\\\\	\\ddot{x} &amp;amp;= -\\dfrac{C}{m}\\dot{x}\\sqrt{\\dot{x}^2+\\dot{y}^2}\\end{align*}\\]In the direction of y,\\[\\begin{align*}	\\dfrac{d}{dt}\\left(\\dfrac{\\partial (0.5m(\\dot{x}^2+\\dot{y}^2 )-mgy)}{\\partial \\dot{y}}\\right) - \\dfrac{\\partial (0.5m(\\dot{x}^2+\\dot{y}^2 )-mgy)}{\\partial y} &amp;amp;= -C\\dot{y}\\sqrt{\\dot{x}^2+\\dot{y}^2}\\\\	\\ddot{y} &amp;amp;= -g -\\dfrac{C}{m}\\dot{y}\\sqrt{\\dot{x}^2+\\dot{y}^2}\\end{align*}\\]Integrating $\\ddot{x}$ and $\\ddot{y}$ $\\textit{wrt}$ time should give us $\\dot{x},\\dot{y},x,y$."
  },
  {
    "title": "Hands On with AVR - 03 Timers",
    "url": "/posts/AVR-Diaries-03-Timers/",
    "categories": "Resources, AVR Dairies",
    "tags": "atmega328p, timer, fastPWM, normal mode, CTC mode",
    "date": "2020-05-04 20:48:20 +0530",
    "snippet": "The ATmega328p has two 8 bit timers and one 16 bit timer. Each timer has multiple modes of working. We will be covering all the timers together in the tutorial, one mode at a time.Timers &amp;amp; PrescalersA timer is a specialized type of clock used to measure time intervals. It maintains the timing of an operation in sync with a system clock or an external clock. The purpose of the Prescaler is to allow the timer to be clocked at the rate a user desires.Normal Mode – Timer 0/2The simplest mode to mess around with, the timer starts from BOTTOM (0x00) and goes all the way to the MAX/TOP (0xFF) and overflows back to the BOTTOM (0x00). When the timer overflows it maybe programmed to generate an Timer Overflow Interrupt.In normal operation the Timer/Counter Overflow Flag (TOV0) will be set in the same timer clock cycle as the TCNT0 becomes zero. The TOV0 Flag in this case behaves like a ninth bit, except that it is only set, not cleared. However, combined with the timer overflow interrupt that automatically clears the TOV0 Flag, the timer resolution can be increased by software. There are no special cases to consider in the Normal mode, a new counter value can be written anytime.The normal mode can be used to do tasks at regular intervals of time, logging data from sensors, monitoring the status of battery and many such activities . The time period between two interrupts in the normal mode can be set using the appropriate prescaler and load value. For the sake of this example, let us consider a situation where we are supposed to toggle D13 every one second. Assuming FCPU = 16 MHz, the resolution can be calculated using:Resolution = Prescaler/FCPUUsing an initial load value of 6, will give us 250 counts before an overflow occurs. Therefore with a prescaler of 64 it takes 1000us for each overflow to occur. Using a software counter to keep track of overflows, we can see that 1000 overflows would take 1 second. The below example shows how to do the same through programming.volatile long count = 0; ISR(TIMER2_OVF_vect) {  TCNT2 = 6;  if (count &amp;gt;= 1000){ // 1 second     PORTB ^= (1 &amp;lt;&amp;lt; 5); //toggle D13    count = 0;  }  else {    count++;  }} void setup(){  pinMode(13, OUTPUT);  cli(); //Disable Global Interrupts  TCCR2A = 0; //Normal Mode  TCCR2B = 0;  TIMSK2 |= (1 &amp;lt;&amp;lt; TOIE2); //Enable overflow interrupt  sei(); //Enable Global Interrupts  TCNT2 = 6; //Load value  TCCR2B |= (1 &amp;lt;&amp;lt; CS22); // Prescaler set to 64} void loop(){}Normal Mode – Timer 1The normal mode for timer 1 is very similar to normal mode of timer 2/0, the only difference being the step count which arises due to the 8 and 16 bit values of the timers.The counter simply overruns when it passes its maximum 16-bit value (MAX = 0xFFFF) and then restarts from the BOTTOM (0x0000). In normal operation the Timer/Counter Overflow Flag (TOV1) will be set in the same timer clock cycle as the TCNT1 becomes zero. The TOV1 Flag in this case behaves like a 17th bit, except that it is only set, not cleared. However, combined with the timer overflow interrupt that automatically clears the TOV1 Flag, the timer resolution can be increased by software. There are no special cases to consider in the Normal mode, a new counter value can be written anytime.CTC Mode – Timer 0/2In the normal mode we had to reset the timer after each overflow to a load value, this is a very inefficient way of generating a waveform as writing into TCNTn is an software level change which uses the CPU resources and also its a lot slower than a hardware level change.Enter CTC mode, where the timer starts from the BOTTOM (0x00) and counts till it reaches the TOP which is equal to OCRnA and NOT MAX(0xFF). The OCRnA defines the top value for the counter, hence also its resolution. This mode allows greater control of the compare match output frequency. It also simplifies the operation of counting external events. An interrupt can be generated each time the counter value reaches the TOP value by using the OCF2A Flag. If the interrupt is enabled, the interrupt handler routine can be used for updating the TOP value.For generating a waveform output in CTC mode, the OCnA output can be set to toggle its logical level on each compare match by setting the COMnA bits. The OCnA value will not be visible on the port pin unless the data direction for the pin is set to output. As for the Normal mode of operation, the TOVn Flag is set in the same timer clock cycle that the counter countsfrom MAX to BOTTOM(0x00).Lets take up the example from normal mode, and add a few more features to it. Lets generate three different waveforms, a 500 Hz wave by toggling the OCnA pin, another 500 Hz wave using the TIMERn_COMPA_vect and finally a 0.5 Hz wave using a software prescaler present insider the Interrupt handle like we did in the normal mode.volatile long count = 0; ISR(TIMER2_COMPA_vect) {  PORTB ^= (1);  if (count &amp;gt;= 1000) {    count = 0;    PORTB ^= (1 &amp;lt;&amp;lt; 5);  }  else {    count++;  }} void setup() {  pinMode(13, OUTPUT);  pinMode(11, OUTPUT);  pinMode(8, OUTPUT);  cli();  TCCR2A = 0;  TCCR2A |= (1 &amp;lt;&amp;lt; COM2A0) | (1 &amp;lt;&amp;lt; WGM21); //toggle OCR2A &amp;amp; CTC Mode  TCCR2B = 0;  TCNT2 = 0;  OCR2A = 249; // Clear Compare value  TIMSK2 |= (1 &amp;lt;&amp;lt; OCIE2A); //enable Output Compare Interrupt  sei();  TCCR2B |= (1 &amp;lt;&amp;lt; CS22); //Prescaler set to 64} void loop() {  // put your main code here, to run repeatedly: }Yellow – D13, Pink – D8, Blue – D11Although this program does get the job done, it’s not the most efficient way to get the task done. The 500 Hz wave generated by TIMER2_COMPA_vect is a software triggered wave, meaning the CPU is involved in the process which can be a problem in CPU intensive sketches.We will be writing a better version of the same code using both the output compare units instead of just OCRnA, here the OCnB pin is toggled directly through hardware which frees up the CPU.Let’s generate three different waveforms, a 500 Hz wave by toggling the OCnA pin, another 500 Hz wave using the OCnB pin and finally a 0.5 Hz wave using a software prescaler present insider the Interrupt handle like we did in the normal mode. Another added feature would be the fact that OCRnB can be used to create a phase shift between the two 500 Hz waves.volatile long count = 0; ISR(TIMER2_COMPA_vect) {  if (count &amp;gt;= 1000) {    count = 0;    PORTB ^= (1 &amp;lt;&amp;lt; 5);  }  else {    count++;  }} void setup() {  pinMode(13, OUTPUT);  pinMode(11, OUTPUT);  pinMode(3, OUTPUT);  cli();  TCCR2A = 0;  TCCR2A |= (1 &amp;lt;&amp;lt; COM2A0) | (1 &amp;lt;&amp;lt; COM2B0) | (1 &amp;lt;&amp;lt; WGM21); //toggle OCR2A,OCR2B &amp;amp; CTC Mode  TCCR2B = 0;  TCNT2 = 0;  OCR2A = 249; // Compare value  OCR2B = 100; // Compare value  TIMSK2 |= (1 &amp;lt;&amp;lt; OCIE2A); //enable Output Compare Interrupt  sei();  TCCR2B |= (1 &amp;lt;&amp;lt; CS22); //Prescaler set to 64} void loop() {  // put your main code here, to run repeatedly: }        The phase shift between OCnA and OCnB can be seen in the above pictures.  Notice that all the examples we have discussed till now in CTC mode are for 50% duty cycle.Finally before we switch to the next mode let us see how we can generate a 50 Hz PWM signal having 40% duty cycle using CTC mode.volatile long count = 0; ISR(TIMER2_COMPA_vect) {  count++;  OCR2A = count % 2 == 0 ? 2 : 1;} void setup() {  pinMode(11, OUTPUT);  cli();  TCCR2A = 0;  TCCR2A |= (1 &amp;lt;&amp;lt; COM2A0) | (1 &amp;lt;&amp;lt; WGM21);  TCCR2B = 0;  TCNT2 = 0;  OCR2A = 1;  TIMSK2 |= (1 &amp;lt;&amp;lt; OCIE2A);  sei();  TCCR2B |= (1 &amp;lt;&amp;lt; CS22); //pre  // put your setup code here, to run once:} void loop() {  // put your main code hereD11 – PinkForce Output CompareIn non-PWM Waveform Generation modes, the match output of the comparator can be forced by writing a one to the Force Output Compare (FOCnx) bit. Forcing compare match will not set the OCFnx Flag or reload/clear the timer, but the OCnx pin will be updated as if a real compare match had occurred (the COM11:0 bits settings define whether the OCnx pin is set, cleared or toggled).Input Capture UnitThe Timer/Counter incorporates an Input Capture unit that can capture external events and give them a timestamp indicating time of occurrence. The external signal indicating an event, or multiple events, can be applied via the ICP1 pin or alternatively, via the analog-comparator unit. The time-stamps can then be used to calculate frequency, duty-cycle, and other features of the signal applied. Alternatively the time-stamps can be used for creating a log of the events.When a change of the logic level (an event) occurs on the Input Capture pin (ICP1), alternatively on the Analog Comparator output (ACO), and this change confirms to the setting of the edge detector, a capture will be triggered. When a capture is triggered, the 16-bit value of the counter (TCNT1) is written to the Input Capture Register (ICR1). The Input Capture Flag (ICF1) is set at the same system clock as the TCNT1 value is copied into ICR1 Register. If enabled (ICIE1 = 1), the Input Capture Flag generates an Input Capture interrupt. The ICF1 Flag is automatically cleared when the interrupt is executed. Alternatively the ICF1 Flag can be cleared by software by writing a logical one to its I/O bit location.The ICR1 Register can only be written when using a Waveform Generation mode that utilizes the ICR1 Register for defining the counter’s TOP value. In these cases the Waveform Generation mode (WGM13:0) bits must be set before the TOP value can be written to the ICR1 Register. When writing the ICR1 Register the high byte must be written to the ICR1H I/O location before the low byte is written to ICR1L.CTC Mode – Timer 1The CTC Mode for timer 1 is similar to the CTC mode available in timer 0/2, only difference being the option to use ICR1 as the top value other than the usual option to use OCR1A.ISR(TIMER1_CAPT_vect) {  TCCR1C |= (1 &amp;lt;&amp;lt; FOC1A) | (1 &amp;lt;&amp;lt; FOC1B);} void setup() {  Serial.begin(9600);  pinMode(9, OUTPUT); //OC1A  pinMode(10, OUTPUT); //OC1B  TCCR1A = 0;  cli();  TCCR1A |= (1 &amp;lt;&amp;lt; COM1A0) | (1 &amp;lt;&amp;lt; COM1B0); //toggle  TCCR1B = 0;  TCCR1B |= (1 &amp;lt;&amp;lt; WGM12) | (1 &amp;lt;&amp;lt; WGM13); // CTC+ICR  TIMSK1 |= (1 &amp;lt;&amp;lt; ICIE1); // Enable ICR interrupt  TCNT1 = 0;  ICR1 = 2039;  OCR1A = 1223; //40% duty cycle  OCR1B = 815; //60% duty cycle 815  sei();// Enable Global Interrupts  TCCR1B |= (1 &amp;lt;&amp;lt; CS11); //8 prescaler} void loop() {  // put your main code here, to run repeatedly:}Fast PWMIn the simplest PWM mode, The fast Pulse Width Modulation or fast PWM mode (WGM02:0 = 3 or 7) provides a high frequency PWM waveform generation option. The fast PWM differs from the other PWM option by its single-slope operation. The counter counts from BOTTOM to TOP then restarts from BOTTOM. TOP is defined as 0xFF when WGM2:0 = 3, and OCR0A when WGM2:0 = 7. In non-inverting Compare Output mode, the Output Compare (OC0x) is cleared on the compare match between TCNT0 and OCR0x, and set at BOTTOM. In inverting Compare Output mode, the output is set on compare match and cleared at BOTTOM.The Timer/Counter Overflow Flag (TOV0) is set each time the counter reaches TOP. If the interrupt is enabled, the interrupt handler routine can be used for updating the compare value.In fast PWM mode, the compare unit allows generation of PWM waveforms on the OC0x pins. Setting the COM0A1:0 bits to one allows the OC0A pin to toggle on Compare Matches if the WGM02 bit is set. This option is not available for the OC0B pin. The actual OC0x value will only be visible on the port pin if the data direction for the port pin is set as output. The PWM waveform is generated by setting (or clearing) the OC0x Register at the compare match between OCR0x and TCNT0, and clearing (or setting) the OC0x Register at the timer clock cycle the counter is cleared (changes from TOP to BOTTOM).The PWM frequency for the output can be calculated by the following equation:  Frequency = fclock/(N*(TOP+1))WHERE N IS THE PRESCALER.Using OCRnA as the top value will give us more freedom in controlling the frequency of the output waveform, but OCRnA cannot be used both as the top value and the PWM compare value. Only OCRnB can be used to generate waveforms. However, there is a special-case mode “Toggle OCnA on Compare Match” that will toggle output A at the end of each cycle, generating a fixed 50% duty cycle and half frequency in this case.void setup() {  cli();  TCCR2A = 0; //clear the values  TCCR2A |= ((1&amp;lt;&amp;lt;COM2A1)|(1&amp;lt;&amp;lt;COM2B1)|(1&amp;lt;&amp;lt;WGM20)|(1&amp;lt;&amp;lt;WGM21)); // set to fast PWM with std TOP and use non-inverting mode for OCnA and OCnB  TCCR2B = 0;  pinMode(11,OUTPUT);  pinMode(3,OUTPUT);  OCR2A = 204; // 30% duty cycle  OCR2B = 76; // 80% duty cycle  TCCR2B |= (1&amp;lt;&amp;lt;CS22); //prescaler set to 64  sei();  // put your setup code here, to run once: } void loop() {  // put your main code here, to run repeatedly: }Coming Up  Phase Correct PWM  Phase and Frequency Correct PWM"
  },
  {
    "title": "Hands On with AVR - 01 Port Manipulation",
    "url": "/posts/AVR-Diaries-01-Port-Manipulation/",
    "categories": "Resources, AVR Dairies",
    "tags": "atmega328p, port, pin, register, bit math",
    "date": "2020-05-04 19:13:20 +0530",
    "snippet": "Port Manipulation refers to the technique of directly working with the underlying registers of the ATmega chip(in this context) instead of relying on predefined Arduino functions. This is primarily done to reduce your code’s memory footprint and make it run faster.To understand how the memory and speed get affected by using the Arduino functions, lets take an example,Here is the code for the arduino pinMode() function:void pinMode(uint8_t pin, uint8_t mode){        uint8_t bit = digitalPinToBitMask(pin);        uint8_t port = digitalPinToPort(pin);        volatile uint8_t *reg, *out;         if (port == NOT_A_PIN) return;         // JWS: can I let the optimizer do this?        reg = portModeRegister(port);        out = portOutputRegister(port);         if (mode == INPUT) {                uint8_t oldSREG = SREG;                cli();                *reg &amp;amp;= ~bit;                *out &amp;amp;= ~bit;                SREG = oldSREG;        } else if (mode == INPUT_PULLUP) {                uint8_t oldSREG = SREG;                cli();                *reg &amp;amp;= ~bit;                *out |= bit;                SREG = oldSREG;        } else {                uint8_t oldSREG = SREG;                cli();                *reg |= bit;                SREG = oldSREG;        }}As you can see, although we tend to treat it as a single line of code, internally, it contains many lines of code, and multiple function calls to get its job done. This article will learn how to write into registers directly and further understand that pinMode and other stock functions internally do the same.Before starting port manipulation, you need to get yourself familiar with bit math and bit manipulation; here is a quick intro to it.Bit ManipulationManipulating the internal registers of the ATmega chip involves flipping bits in 8-bit (usually) arrays, as the ATmega328p is 8-bit in nature, so our primary learning objective would be to learn how to flip, read, write to bit/bits without affecting or disturbing the other bits in a given array.Setting a bit HIGHLet us learn how to set a bit HIGH/1 in a given array. For this example let us consider the following.You are given an 8-bit array and you do not know the states of any of the pins, you are further required to turn the 4th bit in the array to 1. (irrespective of its current state)Given array :: xxxx yxxxCreate an array :: 0000 1000 (can be done using 1«3)Perform OR operation on both the arrays.Resulting array :: xxxx 1xxxAs we can see, irrespective of the initial state of the 4th bit, it is now set to 1 for sure. Also, notice how the other 7 bits remain unaffected by this operation regardless of their initial state.To put this up as a syntax, assuming the given array is called EXP. The above logic can be written asEXP = EXP | (1&amp;lt;&amp;lt;3); //One way to write it EXP |= (1&amp;lt;&amp;lt;3); //A more compact way of writing the same expression.Setting a bit LOWSetting a bit LOW/0 is pretty much similar to setting a bit HIGH/1 in terms of our approach. Let us consider an example to understand better. You are given an 8-bit array, and the initial states of all the pins are unknown. We have to set the 4th bit in the given array to 0. (irrespective of its current state)Given array :: xxxx yxxxCreate an array :: 0000 1000 (can be done using 1«3)Invert the array :: 1111 0111 (can be done using ~(1«3) )Perform AND operation on the inverted array and the given array.Resulting array :: xxxx 0xxxJust like we saw in the previous example of setting a bit HIGH, only the 4th bit is set to 0, and all the other pins are left unaffected irrespective of their initial states. To put this in syntax, assuming the given array is called EXP. The above logic can be written asEXP = EXP &amp;amp; ~(1&amp;lt;&amp;lt;3); //One way to write it EXP &amp;amp;= ~(1&amp;lt;&amp;lt;3); //A more compact way of writing the same expression.Reading the state of a bitJust like setting the setting a bit HIGH and LOW, we might at times want to read the current state of the bit. Let us consider the following example. You are given an 8-bit array, and the initial states of the bits are unknown. We have to find the current state of the 4th bit in the given array.Given array :: xxxx yxxxLeft shift the given array :: 000x xxxy (such that y is at the corner, can be done using EXP»3 )Perform AND operation with the shifted array and 0000 0001.Resulting array :: 0000 000yThe above logic can be written as,RESULT = EXP&amp;gt;&amp;gt;3 &amp;amp; 1;Flipping the state of a bitSometimes especially in loops, we might just need to flip the state of a bit over and over again without bothering about its initial or final states. For example let us consider an 8-bit array where we are supposed to flip the state of the 4th bit.Given array :: xxxx yxxxCreate an array :: 0000 1000 (can be created using 1«3 )Perform XOR Operation on the two arrays.Resulting array :: xxxx (~y)xxxThe above logic can be written as,EXP = EXP ^ (1&amp;lt;&amp;lt;3); //One way to write itEXP ^= (1&amp;lt;&amp;lt;3); //A more compact way of writing the same expressionPort RegistersPort registers allow for faster manipulation of the I/O pins, the predefined functions for GPIO in Arduino, while easy to use, conceal a lot of the features and functionality the ATmega can offer.The ATmega 328p has three ports, refer to the yellow tag markers in the below picture for the labels.Port B – Digital pins 8 to 13Port C – Analog PinsPort D – Digital pins 0 to 7What are ports ?Ports are collections of Pins sharing a set of common SFR’s. (vaguely speaking, a group of pins is called a port)To intuitively understand the need for port registers, let’s quickly list all the attributes a GPIO pin can have in general.  INPUT / OUTPUT / INPUT_PULLUP – PinMode()  HIGH / LOW – DigitalWrite()  “Value the pin stores” – DigitalRead()Each of these attributes have one register associated for them, they are defined as the following.  Data Direction Register (DDR) – INPUT / OUTPUT  Data Register (PORT) – HIGH / LOW  Input Pin Register (PIN) – “value the pin stores”NOTE: AnalogRead() and AnalogWrite() have separate registers and will be discussed in ADC and timers posts.The DDR register, determines whether the pin is an INPUT or OUTPUT. The PORT register controls whether the pin is HIGH or LOW, and the PIN register reads the state of INPUT pins set to input with pinMode(). DDR and PORT registers may be both written to, and read. PIN registers correspond to the state of inputs and may only be read.Each bit of these registers corresponds to a single pin; e.g. the low bit of DDRB, PORTB, and PINB refers to pin PB0 (digital pin 8).Let us look at some examples on how to use these registers.// LED Blinking Code void setup(){  DDRB |= 1&amp;lt;&amp;lt;6; //pinMode(13,OUTPUT);} void loop(){  PORTB |= (1&amp;lt;&amp;lt;5); //digitalWrite(13,HIGH);  delay(1000);  PORTB &amp;amp;= ~(1&amp;lt;&amp;lt;5); //digitalWrite(13,LOW);  delay(1000);}  Note that the entire code block in void loop can be replaced by the following code:void loop(){  PORTB ^= (1&amp;lt;&amp;lt;5);  delay(1000);}Note that the code DDRB |= 00101110; is the equivalent of setting the pins 8, 12 as inputs and 9,10,11,13 as outputs, this would have taken 6 pinMode function calls to achieve, similarly other tasks like setting multiple pins HIGH or LOW or reading the sates of the pins can be achieved in fewer lines of code.PIN register reads the data from all the pins at once, and can be used like the other registers shown in the above examples. More examples will be taken up in the subsequent tutorials.As a concluding example let us take up the following code./*Objective : The void loop is busy with a processshown as delay(5000). You need the AVR to perform some smaller and more important tasks once in a while.*//*volatile int state = LOW; const int led = 12;const int button = 2;void setup(){  pinMode(led, OUTPUT);  pinMode(button,INPUT_PULLUP);  attachInterrupt(digitalPinToInterrupt(button),blinker,FALLING);}void blinker(){     state = state == HIGH ? LOW : HIGH;  digitalWrite(led,state);}void loop(){ delay(5000);}*/ /*--------------------------------------------------*/ ISR(INT0_vect){  PORTB ^= (1 &amp;lt;&amp;lt; 4);} void setup(){     //Enable D12 as OUTPUT  DDRB |= (1 &amp;lt;&amp;lt; 4);  //Enable D2 as INPUT_PULLUP  DDRD &amp;amp;= ~(1 &amp;lt;&amp;lt; 2);  PORTD |= (1 &amp;lt;&amp;lt; 2);   //Enable Falling edge  EICRA = 0;  EICRA |= (1 &amp;lt;&amp;lt; ISC01);  //Enable Mask  EIMSK = 0;  EIMSK |= (1 &amp;lt;&amp;lt; INT0);  //Enable global interrupts  sei();   // put your setup code here, to run once:}void loop(){  delay(5000);  // put your main code here, to run repeatedly:}The schematic for the above code is attached below. While the code might contain a few new topics like interrupts, it is well documented and understandable. We will dive into the interrupts part in the following tutorial.External Resouces"
  },
  {
    "title": "Underwater Rover",
    "url": "/posts/Underwater-Rover/",
    "categories": "Projects, Autonomous Robots",
    "tags": "jetson, underwater, imu, mpu6050, thruster, bldc, ros",
    "date": "2019-12-15 19:13:20 +0530",
    "snippet": "The primary objective of this project was to build an underwater rover platform that can be used to experiment with topics ranging from control systems and computer vision to embedded systems and machine learning.Hardware LayoutFabricationTest StandThe test stand is used to measure the thrust that was generated by different combinations of motors and propellers. Devices that measure thrust are already available in the market, but they are meant for measuring the thrust of propellers designed to work in the air.Our design is based on these devices that work in air, and was modified so that the electronics can be placed outside the water, while the propeller and the motors are inside water.The device works on the principle of moments. It consists of an arm that is pivoted near one end. This end has an extension that makes contact with a load cell that measures the thrust. The other end of the arm is attached to the motor fitted with the propeller, and is dipped inside water. As the motor rotates, the propeller generates thrust and pulls the arm in the forward direction, which results in a clockwise moment about the pivot point. This moment is balanced by the anticlockwise moment that is generated by the normal force of the load cell on the other end. This force is measured by the load cell and is adjusted according to the arm lengths on either side of the pivot to give the final reading of the thrust generated by the propeller.The load cell needs to be calibrated before every test in order to remove the weight of the arm from interfering with the readings. The parameters of the load cell were set in such a way that it gives a precision of upto 1gf.Two prototypes of the test stand were built and tested.Prototype 1This is a makeshift prototype which was built using pieces of wood and the broken arm of a quadcopter. The motor was dipped in a bucket of water to measure the thrust.Limitations:  As the arm lengths were very small, the motor could not be tested at greater depths.  Since the setup was limited to testing in a bucket of water, the readings of the thrust were not fully accurate because propellers create turbulence when water hits the walls of the bucket.  The arms were made of wood, which could very easily get spoiled in water. Propellers of larger thrust could also break the arms apart.Prototype 2This is the model that was used for all the final testing. The arms and frame are both made of stainless steel extrusions that were welded together in the workshop. The frame was then used along with a 1000 litre water tank.Limitations  Since the arms were also made of stainless steel, they were heavy and could interfere with the readings. This was taken into account in the calibration phase of the load cell.Advantages  The bigger tank ensured that there was no turbulence due to water hitting the walls of the vessel.  Since the arm was made of stainless steel, it was much more durable, and could be tested with all possible combinations of motors and propellers.Rover                      After all possible combinations of motors and propellers were tested to get the best possible thrust, the actual rover was fabricated. The thrust values were important to be calculated before fabricating the rover since the amount of upward thrust required for the rover to stay at a particular height needed to be known so that the rover could be designed to be positively buoyant if necessary.The main parts of the rover can be listed as:      Central enclosure: This is the single cylindrical enclosure in the centre of the rover, that houses all the electrical components, the power system and the control system. It is made up of a thick stainless steel tube, with a diameter that is sufficiently large to fit in the single board computer (Raspberry Pi 4 or Jetson Nano) along with the LiPo battery back comfortably.        Front dome: This is the clear dome made of acrylic in the front of the rover. It will be used to house the camera in the future prototypes. Its shape ensures the maximum possible field of view for the camera. This was fixed permanently to the central enclosure using clay epoxy, in order to make the joint waterproof.        Backplate: This is the clear plate at the back of the rover. It houses the valves that were used to connect the cables from the thrusters to the power distribution inside the central enclosure. These valves were permanently fixed to the plate to ensure no leakage of water from the joints. The plate is fixed to the central enclosure using a set of 12 bolts and nuts. Since this plate was made to be removable, its waterproofing was of utmost importance. We made a custom seal for the backplate using a soft acrylic sheet. Furthermore, we used silicone sealant before fastening the bolts to ensure the best possible waterproofing of the joint.        Thruster shrouds: These are the four tubes that enclose the mounts for the thrusters. The configuration of thrusters on our rover are:          Two thrusters facing in the horizontal direction on the far ends of both the sides.      Two thrusters facing the vertical direction adjacent to the central enclosure, on both sides.      These shrouds help in increasing the thrust that was generated by the thrusters while simultaneously protecting them from hitting large objects underwater. They are also made out of thick stainless steel cylindrical tubes.We also attached a thin sheet of stainless steel to join the shrouds from end to end in order to give the design a more streamlined look, as well as to increase the surface area of the rover, which helped in increasing the buoyant force acting on it.  Emergency stop: This component is incorporated into the top of the central enclosure so that it is easy to access. It is designed like a ‘key’ which when detached from the bot will cut off power to all the components of the bot.The entire rover was fabricated in the central workshop.Stainless steel tubes were cut and welded together in order to first make the basic frame of the bot. Mounts for the thrusters were also welded in place.Next, the dome and backplate along with the valves were attached.Finally, the sheet covering the thruster shrouds was welded and the bot was painted black to give it a finishing touch.PropulsionMotorsBrushless DC motors have been chosen for the project as they are the most ideal choice of actuators for high speed and high torque applications. Unlike typical DC motors which are carbon brushes for commutation, BLDC motors have zero contact points which therefore allows them to obtain high speeds and also incur less maintenance. After testing out various BLDC motors in the thruster test stand we decided to go ahead with 400 kv and 1000 kv motors for vertical and horizontal thrusters respectively. These motors, unlike traditional ones, need specialised driving circuits called “Electronic Speed Controllers”  for controlling them. We went ahead with 80A and 35A ESC’s for the 400kv and 1000kv motors based on their power consumption.Most of the commercially available BLDC motors are meant for usage in air and not water due to which we had to prepare the motors before using them for our project. While we have observed that dipping a BLDC underwater as it is doesn’t cause any problems and the motor works normally. It is well documented that prolonged usage underwater causes corrosion of the chemical coating on the stator coils that once depleted causes internal shorting and therefore drastically reduces performance before permanently killing the motor.We went ahead with a 2 part epoxy resin that was applied on top of the stator coils so that they do not come in contact with the water, special care was taken to select the resin as thermal conductivity and water resistance were two important factors for our use case. The stators were coated in epoxy and left to cure overnight, the results were promising and there was no noticeable difference in the amount of thrust produced although we did see the lifetime of the motors increase drastically.The curing process, although working, can be made better by using vacuum to remove air pockets that were generated during the epoxy mixing process.PropellersCustom propellers were designed for the BLDC motors using MATLAB, Solidworks and OpenProp ( an open source software used for the design and analysis of marine propellers). Given that the propellers were 3D printed in sandboxx using PLA filament. The propellers were designed for easy and fast fabrication due to which they were of fixed pitch. Two different propellers were created as the motor housing diameters for the 1000kv and 400kv motors were very different.Each propeller model once generated in MATLAB was exported into solidworks as a set of splines and the final model was formed via extrusion and shape forming.TestingMultiple subsystems of the rover were tested individually many times before the final assembly took place, given that the team had no access to a proper swimming pool. Initial testing took place in any available container ranging from a water bucket to 1000 L PVC Tank that we custom cut out for the project. The final testing for the robot took place in a pool outside of the campus, we had a limited set of tools at hand given the location and shortage of crew. The final test did show a few problems but they were not major enough and the rover did function as per expectations.Upcoming WorkThe current rover design has many shortcomings and given that it was the first prototype we were working on, the focus was more on testing the feasibility of the idea and getting to know what it takes to get a rover working rather than actually making a full fledged model. That being said the team is currently working on a revised prototype that addresses most if not all of the shortcomings in the first model. Since the campus now has a swimming pool we believe that the testing and debugging will be very easy when compared to the past. Given the remote setting since last semester the team has been working on the simulation and software side of the rover."
  },
  {
    "title": "Spherical Robot",
    "url": "/posts/Spherical-Robot/",
    "categories": "Projects, DIY",
    "tags": "atmega328p, l298, ct6b",
    "date": "2019-04-25 19:13:20 +0530",
    "snippet": "A project to explore the use cases and challenges of using a sphericalrobot for omnidirectional traversal.Note: This is an old project and has been recently documented, most of the project media (photos, videos) are lost, I’ll try updating this blog with photos and videos as I find them.Hardware  Arduino UNO  L298N  100RPM 12V DC Geared Motor  2500 mAh Lipo Battery  CT6B  2 peice Hamster WheelDesignCADFinal Model"
  },
  {
    "title": "Gaming Wheel",
    "url": "/posts/Gaming-Wheel/",
    "categories": "Projects, DIY",
    "tags": "atmega328p, game, imu, interactive, mpu6050",
    "date": "2018-12-22 19:13:20 +0530",
    "snippet": "A cheap and affordable gaming wheel based on the Atmega-328p,MPU6050 and capacitive touch sensors for an immersive gamingexperience.Hardware  Arduino Uno  ADXL345 (or any IMU or tilt sensor)  Base Frame  Large value resistors  PVC or other material for structureSoftware  Arduino IDE  Processing/AHK  Some game to test out the projectWorkingAs the process description chart above states, the entire game plan takes user input and mimics keyboard or mouse activity.Keyboard, mouse, and other input devices are called HID Devices. While some microcontrollers like the Arduino Leonardo can be used as HID Devices, I decided to use an Arduino Uno to make things more challenging.I ended up using a 3-axis accelerometer and some make-shift capacitive touch sensors for taking user input. The Arduino Uno processes data from the sensors and sends commands via Serial Communication to Processing IDE, which conveniently can pull keyboard and mouse hooks, which help us interact with Softwares/games running on the system.BuildPrototypeAn early prototype to test the idea and feasibility was made using some spare cardboard lying around. Evolution Montage            ResultsRig TestVice CityNeeds For Speed"
  }
]
