---
title: Locomotion with Weighted Belief in Exteroception
date: 2024-04-12 13:13:20 +/-0530
categories: [Projects, Quadrupeds]
tags: [reinforcement learning, lidar, quadruped] # TAG names should always be lowercase
math: true
image: /assets/img/LIDAR-DRL/lidar_points_cover.png
hidden: false
---

<!-- ![Image1](/assets/img/LIDAR-DRL/lidar_student.png){: .shadow} -->

This project goes over the integration of elevation maps (typically obtained from LIDARs/Depth Cameras) into the locomotion pipeline and the intricacies involved in this process. We will try to reimplement and upgrade(in my humble opinion) a pretty well known paper from ETH RSL titled [Learning robust perceptive locomotion for quadrupedal robots in the wild et al. Takahiro Miki](https://leggedrobotics.github.io/rl-perceptiveloco/).

> **Note:** Detailed results and video clips can be found in the [Results](#results) section below.

## Paper Overview

As you might have noticed, this paper is pretty huge (in both quality and quantity), and in a way is a compilation of their past work and new results. Therefore I will try to break this paper into several sections and focus on one topic at a time.

The entire paper can be broken down into the following parts:

- Foot Trajectory Generator for Gait
- Noise Model for the LIDAR/Elevation Points
- Belief State Encoder/Decoder Arch

### Foot Trajectory Generators

The paper builds on top of their older work on blind locomotion using FTG, so if you are unaware of FTGs please refer to my blog post on it [here](https://textzip.github.io/posts/FTG-DRL/) and also to these two papers[[1]](https://arxiv.org/pdf/1910.02812),[[2]](https://arxiv.org/abs/2010.11251).

But a TLDR about FTGs is basically the fact that they provide a method to offload the gait generation from the locomotion policy via the the use of a bunch of Trajectory Generation Equations, and the policy is given control over some of the parameters that are embedded into these equations and finally the output of the policy and the output of the FTG is added before being passed onto the robot.

### Noise in LIDAR

A crucial part of the paper involves the noise model that is used to bridge the sim2real gap between the scan dots that are obtained using ground truth in sim vs the real world noisy values. The following snippet from the paper explains all the different types of noises that are superimposed and added to the ground truth.

![Image1](/assets/img/LIDAR-DRL/noise.jpg){: .shadow}

As you might have noticed from the above annotated image, the paper has some missing elements for the noise parameters, I will be sharing the noise parameters I have used in the custom implementation section.

### Belief State Encoder/Decoder

![Image1](/assets/img/LIDAR-DRL/original_arch.jpg){: .shadow}

## Custom Implementation

### Teacher Training

![Image1](/assets/img/LIDAR-DRL/teacher_training.jpg){: .shadow}

### Student Training

![Image1](/assets/img/LIDAR-DRL/student_training.jpg){: .shadow}

### Modified Belief State Encoder/Decoder

![Image1](/assets/img/LIDAR-DRL/encoder.jpg){: .shadow}
![Image1](/assets/img/LIDAR-DRL/decoder.jpg){: .shadow}

### Deployment

![Image1](/assets/img/LIDAR-DRL/system_overview.jpg){: .shadow}

## Results

<iframe width="640" height="385" src="https://youtube.com/embed/Qu5r9823Za0" frameborder="0" allowfullscreen></iframe>

<iframe width="640" height="385" src="https://youtube.com/embed/Vu6mLTbqOj0" frameborder="0" allowfullscreen></iframe>
