<!DOCTYPE html>
<html lang="en-US" 
>
<head>
 <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
 <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Deep Reinforcement Learning - Part 1 - Multi-Arm Bandits" />
<meta name="author" content="Jai Krishna" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This post is part of the Deep Reinforcement Learning Blog Series. For a detailed introduction about the series, the curicullum, sources and references used, please check Part 0 - Getting Started." />
<meta property="og:description" content="This post is part of the Deep Reinforcement Learning Blog Series. For a detailed introduction about the series, the curicullum, sources and references used, please check Part 0 - Getting Started." />
<link rel="canonical" href="https://textzip.github.io/posts/DRL-1/" />
<meta property="og:url" content="https://textzip.github.io/posts/DRL-1/" />
<meta property="og:site_name" content="Jai Krishna" />
<meta property="og:image" content="https://textzip.github.io/assets/img/drl_logo.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-11-15T19:13:20+05:30" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://textzip.github.io/assets/img/drl_logo.jpg" />
<meta property="twitter:title" content="Deep Reinforcement Learning - Part 1 - Multi-Arm Bandits" />
<meta name="twitter:site" content="@twitter_username" />
<meta name="twitter:creator" content="@Jai Krishna" />
<meta name="google-site-verification" content="google_meta_tag_verification" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Jai Krishna"},"dateModified":"2023-01-23T16:42:20+05:30","datePublished":"2022-11-15T19:13:20+05:30","description":"This post is part of the Deep Reinforcement Learning Blog Series. For a detailed introduction about the series, the curicullum, sources and references used, please check Part 0 - Getting Started.","headline":"Deep Reinforcement Learning - Part 1 - Multi-Arm Bandits","image":"https://textzip.github.io/assets/img/drl_logo.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"https://textzip.github.io/posts/DRL-1/"},"url":"https://textzip.github.io/posts/DRL-1/"}</script>
 <title>Deep Reinforcement Learning - Part 1 - Multi-Arm Bandits | Jai Krishna
 </title>
<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png">
<link rel="manifest" href="/assets/img/favicons/site.webmanifest">
<link rel="shortcut icon" href="/assets/img/favicons/favicon.ico">
<meta name="apple-mobile-web-app-title" content="Jai Krishna">
<meta name="application-name" content="Jai Krishna">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous">
<link rel="dns-prefetch" href="https://fonts.gstatic.com">
  <link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials">
  <link rel="dns-prefetch" href="https://www.google-analytics.com">
  <link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous">
  <link rel="dns-prefetch" href="https://www.googletagmanager.com">
<link rel="preconnect" href="https://cdn.jsdelivr.net">
<link rel="dns-prefetch" href="https://cdn.jsdelivr.net">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css">
<link rel="stylesheet" href="/assets/css/style.css">
<link rel="stylesheet"
    href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css">
  <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script defer
    src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script>
  <script async
    src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js"></script>
<script defer src="/assets/js/dist/post.min.js"></script>
  <script>
  /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */
  MathJax = {
    tex: {
      inlineMath: [              /* start/end delimiter pairs for in-line math */
        ['$','$'],
        ['\\(','\\)']
      ],
      displayMath: [             /* start/end delimiter pairs for display math */
        ['$$', '$$'],
        ['\\[', '\\]']
      ]
    }
  };
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>
  <script defer src="/app.js"></script>
<script
  async
  src="https://www.googletagmanager.com/gtag/js?id=G-KGYYX7MFB2"
></script>
<script>
  /* global dataLayer */
  window.dataLayer = window.dataLayer || [];
  function gtag() {
    dataLayer.push(arguments);
  }
  gtag("js", new Date());
  gtag("config", "G-KGYYX7MFB2");
</script>
 <body data-spy="scroll" data-target="#toc">
<div id="sidebar" class="d-flex flex-column align-items-end">
 <div class="profile-wrapper text-center">
   <div id="avatar">
      <a href="/" alt="avatar" class="mx-auto">
        <img src="/assets/img/profile.jpg" alt="avatar" onerror="this.style.display='none'">
      </a>
   </div>
   <div class="site-title mt-3">
      <a href="/">Jai Krishna</a>
   </div>
   <div class="site-subtitle font-italic">Robotics | Electronics | Design</div>
 </div>
 <ul class="w-100">
   <li class="nav-item">
      <a href="/" class="nav-link">
        <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i>
        <span>HOME</span>
      </a>
   <li class="nav-item">
      <a href="/projects/" class="nav-link">
        <i class="fa-fw fas fa-microchip ml-xl-3 mr-xl-3 unloaded"></i>
        <span>PROJECTS</span>
      </a>
   <li class="nav-item">
      <a href="/resources/" class="nav-link">
        <i class="fa-fw fas fa-chalkboard-teacher ml-xl-3 mr-xl-3 unloaded"></i>
        <span>RESOURCES</span>
      </a>
   <li class="nav-item">
      <a href="/categories/" class="nav-link">
        <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i>
        <span>CATEGORIES</span>
      </a>
   <li class="nav-item">
      <a href="/archives/" class="nav-link">
        <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i>
        <span>ARCHIVES</span>
      </a>
   <li class="nav-item">
      <a href="/about/" class="nav-link">
        <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i>
        <span>ABOUT | CONTACT</span>
      </a>
   <li class="nav-item">
      <a href="/tags/" class="nav-link">
        <i class="fa-fw fas fa-chalkboard-teacher ml-xl-3 mr-xl-3 unloaded"></i>
        <span>TAGS</span>
      </a>
 </ul>
 <div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center">
      <a href="https://github.com/TextZip" aria-label="github"
        class="order-3"
        target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
      <a href="
          javascript:location.href = 'mailto:' + ['textzip','gmail.com'].join('@')" aria-label="email"
        class="order-4"
        >
        <i class="fas fa-envelope"></i>
      </a>
      <a href="https://www.linkedin.com/in/jai-krishna-bandi" aria-label="linkedin"
        class="order-5"
        target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
        <span class="icon-border order-2"></span>
      <span id="mode-toggle-wrapper" class="order-1">
<i class="mode-toggle fas fa-adjust"></i>
<script type="text/javascript">
  class ModeToggle {
    static get MODE_KEY() { return "mode"; }
    static get DARK_MODE() { return "dark"; }
    static get LIGHT_MODE() { return "light"; }
    constructor() {
      if (this.hasMode) {
        if (this.isDarkMode) {
          if (!this.isSysDarkPrefer) {
            this.setDark();
          }
        } else {
          if (this.isSysDarkPrefer) {
            this.setLight();
          }
        }
      }
      var self = this;
      /* always follow the system prefers */
      this.sysDarkPrefers.addListener(function() {
        if (self.hasMode) {
          if (self.isDarkMode) {
            if (!self.isSysDarkPrefer) {
              self.setDark();
            }
          } else {
            if (self.isSysDarkPrefer) {
              self.setLight();
            }
          }
          self.clearMode();
        }
        self.updateMermaid();
      });
    } /* constructor() */
    setDark() {
      $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE);
    }
    setLight() {
      $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE);
    }
    clearMode() {
      $('html').removeAttr(ModeToggle.MODE_KEY);
      sessionStorage.removeItem(ModeToggle.MODE_KEY);
    }
    get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); }
    get isSysDarkPrefer() { return this.sysDarkPrefers.matches; }
    get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; }
    get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; }
    get hasMode() { return this.mode != null; }
    get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); }
    /* get the current mode on screen */
    get modeStatus() {
      if (this.isDarkMode
        || (!this.hasMode && this.isSysDarkPrefer) ) {
        return ModeToggle.DARK_MODE;
      } else {
        return ModeToggle.LIGHT_MODE;
      }
    }
    updateMermaid() {
      if (typeof mermaid !== "undefined") {
        let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default");
        let config = { theme: expectedTheme };
        /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */
        $(".mermaid").each(function() {
          let svgCode = $(this).prev().children().html();
          $(this).removeAttr("data-processed");
          $(this).html(svgCode);
        });
        mermaid.initialize(config);
        mermaid.init(undefined, ".mermaid");
      }
    }
    flipMode() {
      if (this.hasMode) {
        if (this.isSysDarkPrefer) {
          if (this.isLightMode) {
            this.clearMode();
          } else {
            this.setLight();
          }
        } else {
          if (this.isDarkMode) {
            this.clearMode();
          } else {
            this.setDark();
          }
        }
      } else {
        if (this.isSysDarkPrefer) {
          this.setLight();
        } else {
          this.setDark();
        }
      }
      this.updateMermaid();
    } /* flipMode() */
  } /* ModeToggle */
  let toggle = new ModeToggle();
  $(".mode-toggle").click(function() {
    toggle.flipMode();
  });
</script>
      </span>
 </div>
</div>
<div id="topbar-wrapper" class="row justify-content-center topbar-down">
 <div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between">
    <span id="breadcrumb">
        <span>
          <a href="/">
            Posts
          </a>
        </span>
          <span>Deep Reinforcement Learning - Part 1 - Multi-Arm Bandits</span>
    </span>
    <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i>
   <div id="topbar-title">
      Post
   </div>
    <i id="search-trigger" class="fas fa-search fa-fw"></i>
    <span id="search-wrapper" class="align-items-center">
      <i class="fas fa-search fa-fw"></i>
      <input class="form-control" id="search-input" type="search"
        aria-label="search" autocomplete="off" placeholder="Search...">
      <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i>
    </span>
    <span id="search-cancel" >Cancel</span>
 </div>
</div>
   <div id="main-wrapper">
     <div id="main">
<div class="row">
 <div id="post-wrapper" class="col-12 col-lg-11 col-xl-8">
   <div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4">
     <h1 data-toc-skip>Deep Reinforcement Learning - Part 1 - Multi-Arm Bandits</h1>
     <div class="post-meta text-muted d-flex flex-column">
       <div>
          <span class="semi-bold">
            Bandi Jai Krishna
          </span>
<em class="timeago"
    data-ts="1668519800"
      data-toggle="tooltip" data-placement="bottom" 
      title="Tue, Nov 15, 2022,  7:13 PM +0530"
    >
  2022-11-15
</em>
       </div>
       <div>
          <span>
<em class="timeago lastmod"
    data-ts="1674472340"
      data-toggle="tooltip" data-placement="bottom" 
      title="Mon, Jan 23, 2023,  8:12 PM +0900"
    >
  2023-01-23
</em>
          </span>
<span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2237 words">12 min</span>
       </div>
     </div>
     <div class="post-content">
       <p>This post is part of the Deep Reinforcement Learning Blog Series.  For a detailed introduction about the series, the curicullum, sources and references used, please check <a href="https://textzip.github.io/posts/DRL-0/">Part 0 - Getting Started</a>.</p>
<h3 id="structure-of-the-blog">Structure of the blog</h3>
<ul>
 <li><a href="https://textzip.github.io/posts/DRL-0/">Part 0 - Getting Started</a>
 <li><a href="https://textzip.github.io/posts/DRL-1/">Part 1 - Multi-Arm Bandits</a>
 <li><a href="https://textzip.github.io/posts/DRL-2/">Part 2 - Finite MDP</a> | <strong>Prerequisites</strong>: <em>python, gymnasium, numpy</em>
 <li><a href="https://textzip.github.io/posts/DRL-3/">Part 3 - Dynamic Programming</a> | <strong>Prerequisites</strong>: <em>python, gymnasium, numpy</em>
 <li><a href="https://textzip.github.io/posts/DRL-4/">Part 4 - Monte Carlo, Temporal Difference &amp; Bootstrapping Methods</a> | <strong>Prerequisites</strong>: <em>Python, gymnasium, numpy</em>
 <li>Part 5 - Deep SARSA and Q-Learning | <strong>Prerequisites</strong>: <em>python, gymnasium, numpy, torch</em>
 <li>Part 6 - REINFORCE, AC, DDPG, TD3, SAC
 <li>Part 7 - A2C, PPO, TRPO, GAE, A3C
 <li>TBA (HER, PER, Distillation)
</ul>
<p>Reinforcement learning loosely refers to the area of machine learning where an agent is tasked with learning about the concequences of its actions and in that process also maximize the numerical reward signal collected over time.</p>
<h1 id="k-arm-bandits">K-Arm Bandits</h1>
<p>In this simplified setting we assume that the problem is non-associatve and therefore does not involve learning to act in more than one situation.</p>
<blockquote>
 <p>Consider the following problem, You are faced repeatedly with a choice among k different options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.</p>
</blockquote>
<p>Each of the actions has an expected/mean reward associated with it, called the <em>value</em> of the action ($A_t$). At any timestep $t$ the reward for choosing a paticular action is denoted by $R_t$. The value of an arbitrary action can then be represented as:</p>
\[q_*(a) \ \dot{=} \ \mathbb{E}[R_t|A_t = a ]\]
<p><em>The value of the action is defined as the expected reward associated with choosing the action.</em></p>
<p>Given the values of all actions it is quite trivial to solve the k-arm bandit problem as one would choose the action with the highest value all the time. But in most close-to-real life senarios the action values are not given and need to be estimated first. Let $Q_t(a)$ denote the estimated value of the action $a$ at time $t$.</p>
<p>We have two possible moves here,</p>
<ul>
 <li>Make our estimates better by sampling all actions aka <strong>Exploration</strong>
 <li>Choose the highest action value given the current estimates aka <strong>Exploit</strong>
</ul>
<p>The need to balance <strong>exploration</strong> and <strong>exploitation</strong> is a distinctive challenge that arises in reinforcement learning. Below mentioned are a few methods to do so..</p>
<h2 id="action-value-methods">Action-value Methods</h2>
<p>We begin by looking more closely at methods for estimating the values of actions and for using the estimates to make action selection decisions, which we collectively call action-value methods.</p>
<h3 id="sample-average">Sample Average</h3>
<p>Recall that the value of an action is the mean reward recieved when the action is choosen, one approach therefore is to average all the rewards for an action. Given by</p>
\[Q_{n+1}(a) \ \dot{=} \ \dfrac{R_1+R_2+..+R_{n}}{n}\]
<p><em>The estimated value of an action after being selected n times</em>
For easier implementation can be written as a recursive formula as follows:</p>
\[\begin{align*}
Q_{n+1}(a) &amp;= \dfrac{1}{n} \left( \sum_{i=1}^{i=n}{R_i} \right)\\ 
 &amp;= \dfrac{1}{n} \left( R_n + (n-1)\dfrac{1}{(n-1)}\sum_{i=1}^{i=n-1}{R_i} \right)\\
&amp;= \dfrac{1}{n} \left( R_n + (n-1)Q_n(a) \right)\\
&amp;= \dfrac{1}{n} \left( R_n + nQ_n(a) - Q_n(a) \right)\\
&amp;= Q_n(a) + \dfrac{1}{n}\left[ R_n - Q_n(a)\right]
\end{align*}\]
<p>The above update rule can be summarised as follows:</p>
\[NewEstimate = OldEstimate + stepSize[Target-OldEstimate]\]
<p>A few observations based on the above:</p>
<ul>
 <li>$[Target-OldEstimate]$ is the error in the estimate that we wish to minimize by taking a step towards the target.
 <li>The stepSize parameter is not constant (For e.g. in sample avg it was 1/n) and is often denoted by $\alpha_t(a)$
</ul>
<p>Given the estimates we can now focus on exploitation, a simple rather “greedy” method would be to choose the highest action value which can be represented as:</p>
\[A_t = \text{argmax}_{a}Q_t(a)\]
<p>The expression $\text{argmax}_{a}$ denotes that a is choosen such that $Q_t(a)$ is maximised with ties broken arbitrarily.</p>
<p>The above method of action selection is quite weak as it does no exploration and therefore can be acting on false assumtions of the action values. A better approach would be to exploit most of the time but every now and then explore values as well. This is called the $\varepsilon$-greedy methods where the decision to explore is based on a small probability $\varepsilon$.</p>
<h3 id="exponential-recency-weighted-average">Exponential Recency-Weighted Average</h3>
<p>The combination of $\varepsilon$-greedy with sample averages works well for stationary problems where the reward probabilties do not change with time. But as stated at the start in most cases this assumption isn’t valid. Therefore the solution for problems that involve non-statinoary reward distributions is to give more weight to recent rewards when compared to old rewards and one possible way to achieve this is to use a constant stepSize parameter that lies between $[0,1]$.</p>
<p>The estimated value from sample value method can be re-written as follows:</p>
\[\begin{align*}
Q_{n+1}(a) &amp;= Q_n(a) + \alpha\left[ R_n - Q_n(a)\right]\\
&amp;= \alpha{R_n}+ (1-\alpha)Q_n\\
&amp;= (1-\alpha)^nQ_1 + \sum_{i=1}^n \alpha(1-\alpha)^{n-i}R_i
\end{align*}\]
<p>The final step can be derived based on the derivation in the sample based method section and therefore has been cut-short.</p>
<p>The above is called weighted-average because the sum of weights is equal to 1, infact the weights decrease exponentially and the above method is therefore called Exponential Recency-Weighted Average.</p>
<h3 id="initial-values--bias">Initial Values &amp; Bias</h3>
<p>When an action is being selected for the first time in sample average method the denominator is 0, therfore a default value is assumed in such cases and in Exponential Recency-Weighted Average method, the value of $Q_2$ depends on the assumption $Q_1$ which again has to be assumed.</p>
<p>Therefore these methods are biased by their initial estimates. For the sample-average methods, the bias disappears once all actions have been selected at least once, but for methods with constant stepSize, the bias is permanent, though decreasing over time as seen in the equation derived above.</p>
<p>In practice this kind of bias is helpful when the initial values choosen are based on expert knowledge. Initial values can also be used to encourage exploration, instead of setting the initial values to 0 if we set them to a very high value aka “optimistic value” the agent tries each action and gets dissapointed but keeps on trying until all the values are sampled atleast once this method often reffered to as optimistic initial value makes sure there is exploration at the start even when used with pure greedy methods.</p>
<h3 id="exponential-recency-weighted-average-without-initial-bias">Exponential Recency-Weighted Average without Initial Bias</h3>
<p>Given that the Exponential Recency-Weighted Average while works on non-stationary problems but suffers from initial bias and the sample average methods are less effected by the initial bias but are not effective against non-stationary problems, there is a need for another method that can work well with non-stationary problems without any intial bias.</p>
<p>One such method can be formulated with the use of a new stepSize parameter defined as</p>
\[\beta_n \ \dot{=} \ \dfrac{\alpha}{\bar{o}_n}\]
<p>To process the nth reward for a particular action, where $\alpha$&lt;0 is the conventional stepSize and $\bar{o}_n$ is the trace of one that starts at 0:</p>
<p>\(\bar{o}_n = \bar{o}_{n-1} + \alpha(1-\bar{o}_{n-1}), \text{ for } n&gt;0, \text{ with } \bar{o}_0 \ \dot{=} \ 0\)</p>
<h3 id="upper-conﬁdence-bound">Upper-Conﬁdence-Bound</h3>
<p>In the $\varepsilon$-greedy methods, while randomly picking actions every once in a while to encourage exploration helps, it makes much more sense to pick these actions based on some guided hurestic rather than random-sampling.</p>
<p>One possible alternative is to pick among the non-greedy actions is to opt for actons that have higher degree of uncertainty in their estimates, such a mechanism will therefore allow us to get better overall estimates for all action values.</p>
<p>The idea of upper conﬁdence bound (UCB) action selection is that the square-root term is a measure of the uncertainty or variance in the estimate of a’s value. The quantity being max’ed over is thus a sort of upper bound on the possible true value of action a,with
c determining the conﬁdence level.</p>
<p>The action is therefore selected as follows:</p>
\[A_t \ \dot{=} \ \text{argmax}_a \left[Q_t(a) + c\sqrt{\dfrac{ln\ t}{N_t(a)}} \right]\]
<p>Where, $N_t(a)$ denotes the number of times action $a$ has been selected prior to time $t$ and $c$ &gt;0 controls the degree of exploration. When an action is being explored for the first time it is considered to be a maximizing action.</p>
<p>Each time $a$ is selected the uncertainty is presumably reduced: $N_t(a)$ increments, and, as it appears in the denominator, the uncertainty term decreases. On the other hand, each time an action other than $a$ is selected, $t$ increases but $N_t(a)$ does not; because $t$ appears in the numerator, the uncertainty estimate increases.</p>
<p>The use of the natural logarithm means that the increases get smaller over time, but are
unbounded; all actions will eventually be selected, but actions with lower value estimates,
or that have already been selected frequently, will be selected with decreasing frequency
over time.</p>
<h3 id="gradient-bandit">Gradient-Bandit</h3>
<p>So far we have considered methods that estimate action values and use
those estimates to select actions. This is often a good approach, but it is not the only one possible.</p>
<p>In this section we consider learning a numerical preference for each action $a$, which we denote as $H_t(a) \in \mathbb{R}$. The larger the preference, the more often that action is taken, but the preference has no interpretation in terms of reward. Only the relative
preference of one action over another is important; if we add 1000 to all the action preferences there is no effect on the action probabilities, which are determined according
to a soft-max distribution (i.e., Gibbs or Boltzmann distribution) as follows:</p>
\[\text{Pr}\{A_t=a\} \ \dot{=} \ \dfrac{e^{H_t(a)}}{\sum_{b=1}^{k}e^{H_t(b)}}  \ \dot{=} \ \pi_t(a)\]
<p>Where $\pi_t(a)$ is the probability of choosing action $a$ at time $t$. Initially all actions have the same preference, therefore $H_1(a)=0$.</p>
<p>There is a natural learning algorithm for soft-max action preferences based on the idea
of stochastic gradient ascent. On each step, after selecting action $A_t$ and receiving the
reward $R_t$, the action preferences are updated by:</p>
\[\begin{align*}
H_{t+1}(A_t) \ &amp;\dot{=} \ H_t(A_t) + \alpha(R_t - \bar{R}_t)(1-\pi_t(A_t)) \\
H_{t+1}(a) \ &amp;\dot{=} \ H_t(a) - \alpha(R_t - \bar{R}_t)\pi_t(a) \text{ for all } a\neq A_t \\
\end{align*}\]
<p>where $\alpha &gt; 0$ is a step-size parameter, and $\bar{R}_t \in \mathbb{R}$ is the average of the rewards up to but
not including time t (with $\bar{R}_1 = R_1$), which can be computed incrementally.  The $\bar{R}_t$ term serves as a
baseline with which the reward is compared. If the reward is higher than the baseline, then the probability of taking $A_t$ in the future is increased, and if the reward is below baseline, then the probability is decreased. The non-selected actions move in the opposite
direction.</p>
<h2 id="convergence-conditions">Convergence Conditions</h2>
<p>Not all stepSize values guarentee that the expected action value converges to the true values. A well-known result in stochastic approximation theory gives us the conditions required to
assure convergence with probability 1:</p>
<ul>
 <li>The ﬁrst condition is required to guarantee that the steps are large enough to eventually overcome any initial conditions or random ﬂuctuations.
 <li>The second condition guarantees that eventually the steps become small enough to assure convergence.
</ul>
<p>These can be expressed as follows:</p>
\[\begin{align*}
\sum_{n=1}^{\infty}\alpha_n(a) &amp;= \infty\\
\sum_{n=1}^{\infty}\alpha_n^2(a) &amp;&lt; \infty\\
\end{align*}\]
<h2 id="associative-search">Associative Search</h2>
<p>In the above discussed methods the learner either tries to ﬁnd a single best action when the task is stationary, or tries to track the best action as it changes over time when the task is nonstationary. However, in a general reinforcement learning task there is more than one situation, and the goal is to learn a policy: a mapping from situations to the actions that are best in those situations. To set the stage for the full problem, we brieﬂy discuss the simplest way in which nonassociative tasks extend to the associative setting.</p>
<p>As an example, suppose there are several different k-armed bandit tasks, and that on each step you confront one of these chosen at random. Thus, the bandit task changes randomly from step to step. If the probabilities with which each task is selected for you
do not change over time, this would appear as a single stationary k-armed bandit task, and you could use one of the methods described in this chapter. Now suppose, however, that when a bandit task is selected for you, you are given some distinctive clue about its identity (but not its action values). Maybe you are facing an actual slot machine that changes the color of its display as it changes its action values. Now you can learn a policy associating each task, signaled by the color you see, with the best action to take when facing that task—for instance, if red, select arm 1; if green, select arm 2. With the right policy you can usually do much better than you could in the absence of any information distinguishing one bandit task from another.
This is an example of an associative search task, so called because it involves both trial-and-error learning to search for the best actions, and association of these actions with the situations in which they are best.</p>
<p>Associative search tasks are intermediate between the k-armed bandit problem and the full reinforcement learning problem. They are like the full reinforcement learning problem in that they involve learning a policy, but they are also like our version of the k-armed bandit problem in that each action affects only the immediate reward. If actions are allowed to affect the next situation as well as the reward, then we have the full reinforcement learning problem.</p>
     </div>
     <div class="post-tail-wrapper text-muted">
       <div class="post-meta mb-3">
          <i class="far fa-folder-open fa-fw mr-1"></i>
            <a href='/categories/resources/'>Resources</a>,
            <a href='/categories/deep-reinforcement-learning/'>Deep Reinforcement Learning</a>
       </div>
       <div class="post-tags">
          <i class="fa fa-tags fa-fw mr-1"></i>
          <a href="/tags/bandits/"
            class="post-tag no-text-decoration" >bandits</a>
          <a href="/tags/sample-average/"
            class="post-tag no-text-decoration" >sample average</a>
          <a href="/tags/weighted-average/"
            class="post-tag no-text-decoration" >weighted average</a>
          <a href="/tags/ucb/"
            class="post-tag no-text-decoration" >ucb</a>
          <a href="/tags/bias/"
            class="post-tag no-text-decoration" >bias</a>
         </div>
       <div class="post-tail-bottom
          d-flex justify-content-between align-items-center mt-3 pt-5 pb-2">
         <div class="license-wrapper">
            This post is licensed under
            <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a>
            by the author.
         </div>
<div class="share-wrapper">
  <span class="share-label text-muted mr-1">Share</span>
  <span class="share-icons">
        <a href="https://twitter.com/intent/tweet?text=Deep Reinforcement Learning - Part 1 - Multi-Arm Bandits - Jai Krishna&url=https://textzip.github.io/posts/DRL-1/" data-toggle="tooltip" data-placement="top"
          title="Twitter" target="_blank" rel="noopener" aria-label="Twitter">
          <i class="fa-fw fab fa-twitter"></i>
        </a>
        <a href="https://www.facebook.com/sharer/sharer.php?title=Deep Reinforcement Learning - Part 1 - Multi-Arm Bandits - Jai Krishna&u=https://textzip.github.io/posts/DRL-1/" data-toggle="tooltip" data-placement="top"
          title="Facebook" target="_blank" rel="noopener" aria-label="Facebook">
          <i class="fa-fw fab fa-facebook-square"></i>
        </a>
        <a href="https://telegram.me/share?text=Deep Reinforcement Learning - Part 1 - Multi-Arm Bandits - Jai Krishna&url=https://textzip.github.io/posts/DRL-1/" data-toggle="tooltip" data-placement="top"
          title="Telegram" target="_blank" rel="noopener" aria-label="Telegram">
          <i class="fa-fw fab fa-telegram"></i>
        </a>
    <i class="fa-fw fas fa-link small" onclick="copyLink()"
        data-toggle="tooltip" data-placement="top" title="Copy link"></i>
  </span>
</div>
       </div>
     </div>
   </div>
 </div>
<div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down">
 <div class="access">
   <div id="access-lastmod" class="post">
      <span>Recent Update</span>
     <ul class="post-content pl-0 pb-1 ml-1 mt-2">
       <li><a href="/posts/Loco-DRL/">Proprioceptive Locomotion in Unstructured Environments</a>
       <li><a href="/posts/FTG-DRL/">Policy Modulated Trajectory Generation for Quadrupeds</a>
       <li><a href="/posts/NST-DRL/">Neural Style Transfer for Locomotion</a>
       <li><a href="/posts/LIDAR-DRL/">Locomotion with Weighted Belief in Exteroception</a>
       <li><a href="/posts/BiMan-DRL/">Bi-Manual Loco-Manipulation in Quadrupeds for Opening Doors</a>
     </ul>
   </div>
   <div id="access-tags">
      <span>Trending Tags</span>
     <div class="d-flex flex-wrap mt-3 mb-1 mr-3">
        <a class="post-tag" href="/tags/atmega328p/">atmega328p</a>
        <a class="post-tag" href="/tags/imu/">imu</a>
        <a class="post-tag" href="/tags/game/">game</a>
        <a class="post-tag" href="/tags/reinforcement-learning/">reinforcement learning</a>
        <a class="post-tag" href="/tags/bellman/">bellman</a>
        <a class="post-tag" href="/tags/mdp/">mdp</a>
        <a class="post-tag" href="/tags/optimal-value/">optimal value</a>
        <a class="post-tag" href="/tags/touch/">touch</a>
        <a class="post-tag" href="/tags/quadruped/">quadruped</a>
        <a class="post-tag" href="/tags/ros/">ros</a>
     </div>
   </div>
 </div>
    <script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script>
   <div id="toc-wrapper" class="pl-0 pr-4 mb-5">
      <span class="pl-3 pt-2 mb-2">Contents</span>
     <nav id="toc" data-toggle="toc"></nav>
   </div>
</div>
</div>
<div class="row">
 <div class="col-12 col-lg-11 col-xl-8">
   <div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4">
 <div id="related-posts" class="mt-5 mb-2 mb-sm-4">
   <h3 class="pt-2 mt-1 mb-4 ml-1"
      data-toc-skip>Further Reading</h3>
   <div class="card-deck mb-4">
     <div class="card">
        <a href="/posts/DRL-0/">
         <div class="card-body">
<em class="timeago small"
    data-ts="1674027800"
    >
  2023-01-18
</em>
           <h3 class="pt-0 mt-1 mb-3" data-toc-skip>Deep Reinforcement Learning - Part 0 - Getting Started</h3>
           <div class="text-muted small">
             <p>
                Getting into deep reinforcement learning is a tough but rewarding journey. The primary reason it is so difficult for beginners is due to the shear number of fields involved. Deep Reinforcement Lear...
             </p>
           </div>
         </div>
        </a>
     </div>
     <div class="card">
        <a href="/posts/DRL-3/">
         <div class="card-body">
<em class="timeago small"
    data-ts="1674200600"
    >
  2023-01-20
</em>
           <h3 class="pt-0 mt-1 mb-3" data-toc-skip>Deep Reinforcement Learning - Part 3 - Dynamic Programming</h3>
           <div class="text-muted small">
             <p>
                This post is part of the Deep Reinforcement Learning Blog Series.  For a detailed introduction about the series, the curicullum, sources and references used, please check Part 0 - Getting Started.
...
             </p>
           </div>
         </div>
        </a>
     </div>
     <div class="card">
        <a href="/posts/DRL-4/">
         <div class="card-body">
<em class="timeago small"
    data-ts="1674459800"
    >
  2023-01-23
</em>
           <h3 class="pt-0 mt-1 mb-3" data-toc-skip>Deep Reinforcement Learning - Part 4 - Monte Carlo, Temporal Difference & Bootstrapping Methods</h3>
           <div class="text-muted small">
             <p>
                This post is part of the Deep Reinforcement Learning Blog Series.  For a detailed introduction about the series, the curicullum, sources and references used, please check Part 0 - Getting Started.
...
             </p>
           </div>
         </div>
        </a>
     </div>
   </div>
 </div>
<div class="post-navigation d-flex justify-content-between">
  <a href="/posts/AGV/" class="btn btn-outline-primary"
    prompt="Older">
   <p>Autonomus Ground Vehicle</p>
  </a>
  <a href="/posts/DRL-2/" class="btn btn-outline-primary"
    prompt="Newer">
   <p>Deep Reinforcement Learning - Part 2 - Finite MDP</p>
  </a>
</div>
    <script src="https://utteranc.es/client.js"
        repo="TextZip/textzip.github.io"
        issue-term="title"
        theme="dark-blue"
        crossorigin="anonymous"
        async>
    </script>
   </div>
 </div>
</div>
<footer class="d-flex w-100 justify-content-center">
 <div class="d-flex justify-content-between align-items-center">
   <div class="footer-left">
     <p class="mb-0">
        © 2025
        <a href="https://github.com/TextZip">Bandi Jai Krishna</a>.
        <span data-toggle="tooltip" data-placement="top"
          title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span>
     </p>
   </div>
   <div class="footer-right">
     <p class="mb-0">
        Powered by
        <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>
        with
        <a href="https://github.com/cotes2020/jekyll-theme-chirpy"
          target="_blank" rel="noopener">Chirpy</a>
        theme.
     </p>
   </div>
 </div>
</footer>
     </div>
<div id="search-result-wrapper" class="d-flex justify-content-center unloaded">
 <div class="col-12 col-sm-11 post-content">
   <div id="search-hints">
     <h4 class="text-muted mb-4">Trending Tags</h4>
        <a class="post-tag" href="/tags/atmega328p/">atmega328p</a>
        <a class="post-tag" href="/tags/imu/">imu</a>
        <a class="post-tag" href="/tags/game/">game</a>
        <a class="post-tag" href="/tags/reinforcement-learning/">reinforcement learning</a>
        <a class="post-tag" href="/tags/bellman/">bellman</a>
        <a class="post-tag" href="/tags/mdp/">mdp</a>
        <a class="post-tag" href="/tags/optimal-value/">optimal value</a>
        <a class="post-tag" href="/tags/touch/">touch</a>
        <a class="post-tag" href="/tags/quadruped/">quadruped</a>
        <a class="post-tag" href="/tags/ros/">ros</a>
   </div>
   <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
 </div>
</div>
   </div>
   <div id="mask"></div>
    <a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button">
      <i class="fas fa-angle-up"></i>
    </a>
<script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script>
<script>
SimpleJekyllSearch({
  searchInput: document.getElementById('search-input'),
  resultsContainer: document.getElementById('search-results'),
  json: '/assets/js/data/search.json',
  searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0">  <a href="https://textzip.github.io{url}">{title}</a> <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">    {categories}    {tags} </div><p>{snippet}</p></div>',
  noResultsText: '<p class="mt-5">Oops! No result founds.</p>',
  templateMiddleware: function(prop, value, template) {
    if (prop === 'categories') {
      if (value === '') {
        return `${value}`;
      } else {
        return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
      }
    }
    if (prop === 'tags') {
      if (value === '') {
        return `${value}`;
      } else {
        return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
      }
    }
  }
});
</script>
  
