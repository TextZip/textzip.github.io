<!DOCTYPE html>
<html lang="en-US" 
>
<head>
 <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
 <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Deep Reinforcement Learning - Part 3 - Dynamic Programming" />
<meta name="author" content="Jai Krishna" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This post is part of the Deep Reinforcement Learning Blog Series. For a detailed introduction about the series, the curicullum, sources and references used, please check Part 0 - Getting Started." />
<meta property="og:description" content="This post is part of the Deep Reinforcement Learning Blog Series. For a detailed introduction about the series, the curicullum, sources and references used, please check Part 0 - Getting Started." />
<link rel="canonical" href="https://textzip.github.io/posts/DRL-3/" />
<meta property="og:url" content="https://textzip.github.io/posts/DRL-3/" />
<meta property="og:site_name" content="Jai Krishna" />
<meta property="og:image" content="https://textzip.github.io/assets/img/drl_logo.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-01-20T13:13:20+05:30" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://textzip.github.io/assets/img/drl_logo.jpg" />
<meta property="twitter:title" content="Deep Reinforcement Learning - Part 3 - Dynamic Programming" />
<meta name="twitter:site" content="@twitter_username" />
<meta name="twitter:creator" content="@Jai Krishna" />
<meta name="google-site-verification" content="google_meta_tag_verification" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Jai Krishna"},"dateModified":"2023-01-23T16:42:20+05:30","datePublished":"2023-01-20T13:13:20+05:30","description":"This post is part of the Deep Reinforcement Learning Blog Series. For a detailed introduction about the series, the curicullum, sources and references used, please check Part 0 - Getting Started.","headline":"Deep Reinforcement Learning - Part 3 - Dynamic Programming","image":"https://textzip.github.io/assets/img/drl_logo.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"https://textzip.github.io/posts/DRL-3/"},"url":"https://textzip.github.io/posts/DRL-3/"}</script>
 <title>Deep Reinforcement Learning - Part 3 - Dynamic Programming | Jai Krishna
 </title>
<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png">
<link rel="manifest" href="/assets/img/favicons/site.webmanifest">
<link rel="shortcut icon" href="/assets/img/favicons/favicon.ico">
<meta name="apple-mobile-web-app-title" content="Jai Krishna">
<meta name="application-name" content="Jai Krishna">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous">
<link rel="dns-prefetch" href="https://fonts.gstatic.com">
  <link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials">
  <link rel="dns-prefetch" href="https://www.google-analytics.com">
  <link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous">
  <link rel="dns-prefetch" href="https://www.googletagmanager.com">
<link rel="preconnect" href="https://cdn.jsdelivr.net">
<link rel="dns-prefetch" href="https://cdn.jsdelivr.net">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css">
<link rel="stylesheet" href="/assets/css/style.css">
<link rel="stylesheet"
    href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css">
  <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script defer
    src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script>
  <script async
    src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js"></script>
<script defer src="/assets/js/dist/post.min.js"></script>
  <script>
  /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */
  MathJax = {
    tex: {
      inlineMath: [              /* start/end delimiter pairs for in-line math */
        ['$','$'],
        ['\\(','\\)']
      ],
      displayMath: [             /* start/end delimiter pairs for display math */
        ['$$', '$$'],
        ['\\[', '\\]']
      ]
    }
  };
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>
  <script defer src="/app.js"></script>
<script
  async
  src="https://www.googletagmanager.com/gtag/js?id=G-KGYYX7MFB2"
></script>
<script>
  /* global dataLayer */
  window.dataLayer = window.dataLayer || [];
  function gtag() {
    dataLayer.push(arguments);
  }
  gtag("js", new Date());
  gtag("config", "G-KGYYX7MFB2");
</script>
 <body data-spy="scroll" data-target="#toc">
<div id="sidebar" class="d-flex flex-column align-items-end">
 <div class="profile-wrapper text-center">
   <div id="avatar">
      <a href="/" alt="avatar" class="mx-auto">
        <img src="/assets/img/profile.jpg" alt="avatar" onerror="this.style.display='none'">
      </a>
   </div>
   <div class="site-title mt-3">
      <a href="/">Jai Krishna</a>
   </div>
   <div class="site-subtitle font-italic">Robotics | Electronics | Design</div>
 </div>
 <ul class="w-100">
   <li class="nav-item">
      <a href="/" class="nav-link">
        <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i>
        <span>HOME</span>
      </a>
   <li class="nav-item">
      <a href="/projects/" class="nav-link">
        <i class="fa-fw fas fa-microchip ml-xl-3 mr-xl-3 unloaded"></i>
        <span>PROJECTS</span>
      </a>
   <li class="nav-item">
      <a href="/resources/" class="nav-link">
        <i class="fa-fw fas fa-chalkboard-teacher ml-xl-3 mr-xl-3 unloaded"></i>
        <span>RESOURCES</span>
      </a>
   <li class="nav-item">
      <a href="/categories/" class="nav-link">
        <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i>
        <span>CATEGORIES</span>
      </a>
   <li class="nav-item">
      <a href="/archives/" class="nav-link">
        <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i>
        <span>ARCHIVES</span>
      </a>
   <li class="nav-item">
      <a href="/about/" class="nav-link">
        <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i>
        <span>ABOUT | CONTACT</span>
      </a>
   <li class="nav-item">
      <a href="/tags/" class="nav-link">
        <i class="fa-fw fas fa-chalkboard-teacher ml-xl-3 mr-xl-3 unloaded"></i>
        <span>TAGS</span>
      </a>
 </ul>
 <div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center">
      <a href="https://github.com/TextZip" aria-label="github"
        class="order-3"
        target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
      <a href="
          javascript:location.href = 'mailto:' + ['textzip','gmail.com'].join('@')" aria-label="email"
        class="order-4"
        >
        <i class="fas fa-envelope"></i>
      </a>
      <a href="https://www.linkedin.com/in/jai-krishna-bandi" aria-label="linkedin"
        class="order-5"
        target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
        <span class="icon-border order-2"></span>
      <span id="mode-toggle-wrapper" class="order-1">
<i class="mode-toggle fas fa-adjust"></i>
<script type="text/javascript">
  class ModeToggle {
    static get MODE_KEY() { return "mode"; }
    static get DARK_MODE() { return "dark"; }
    static get LIGHT_MODE() { return "light"; }
    constructor() {
      if (this.hasMode) {
        if (this.isDarkMode) {
          if (!this.isSysDarkPrefer) {
            this.setDark();
          }
        } else {
          if (this.isSysDarkPrefer) {
            this.setLight();
          }
        }
      }
      var self = this;
      /* always follow the system prefers */
      this.sysDarkPrefers.addListener(function() {
        if (self.hasMode) {
          if (self.isDarkMode) {
            if (!self.isSysDarkPrefer) {
              self.setDark();
            }
          } else {
            if (self.isSysDarkPrefer) {
              self.setLight();
            }
          }
          self.clearMode();
        }
        self.updateMermaid();
      });
    } /* constructor() */
    setDark() {
      $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE);
    }
    setLight() {
      $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE);
    }
    clearMode() {
      $('html').removeAttr(ModeToggle.MODE_KEY);
      sessionStorage.removeItem(ModeToggle.MODE_KEY);
    }
    get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); }
    get isSysDarkPrefer() { return this.sysDarkPrefers.matches; }
    get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; }
    get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; }
    get hasMode() { return this.mode != null; }
    get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); }
    /* get the current mode on screen */
    get modeStatus() {
      if (this.isDarkMode
        || (!this.hasMode && this.isSysDarkPrefer) ) {
        return ModeToggle.DARK_MODE;
      } else {
        return ModeToggle.LIGHT_MODE;
      }
    }
    updateMermaid() {
      if (typeof mermaid !== "undefined") {
        let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default");
        let config = { theme: expectedTheme };
        /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */
        $(".mermaid").each(function() {
          let svgCode = $(this).prev().children().html();
          $(this).removeAttr("data-processed");
          $(this).html(svgCode);
        });
        mermaid.initialize(config);
        mermaid.init(undefined, ".mermaid");
      }
    }
    flipMode() {
      if (this.hasMode) {
        if (this.isSysDarkPrefer) {
          if (this.isLightMode) {
            this.clearMode();
          } else {
            this.setLight();
          }
        } else {
          if (this.isDarkMode) {
            this.clearMode();
          } else {
            this.setDark();
          }
        }
      } else {
        if (this.isSysDarkPrefer) {
          this.setLight();
        } else {
          this.setDark();
        }
      }
      this.updateMermaid();
    } /* flipMode() */
  } /* ModeToggle */
  let toggle = new ModeToggle();
  $(".mode-toggle").click(function() {
    toggle.flipMode();
  });
</script>
      </span>
 </div>
</div>
<div id="topbar-wrapper" class="row justify-content-center topbar-down">
 <div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between">
    <span id="breadcrumb">
        <span>
          <a href="/">
            Posts
          </a>
        </span>
          <span>Deep Reinforcement Learning - Part 3 - Dynamic Programming</span>
    </span>
    <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i>
   <div id="topbar-title">
      Post
   </div>
    <i id="search-trigger" class="fas fa-search fa-fw"></i>
    <span id="search-wrapper" class="align-items-center">
      <i class="fas fa-search fa-fw"></i>
      <input class="form-control" id="search-input" type="search"
        aria-label="search" autocomplete="off" placeholder="Search...">
      <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i>
    </span>
    <span id="search-cancel" >Cancel</span>
 </div>
</div>
   <div id="main-wrapper">
     <div id="main">
<div class="row">
 <div id="post-wrapper" class="col-12 col-lg-11 col-xl-8">
   <div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4">
     <h1 data-toc-skip>Deep Reinforcement Learning - Part 3 - Dynamic Programming</h1>
     <div class="post-meta text-muted d-flex flex-column">
       <div>
          <span class="semi-bold">
            Bandi Jai Krishna
          </span>
<em class="timeago"
    data-ts="1674200600"
      data-toggle="tooltip" data-placement="bottom" 
      title="Fri, Jan 20, 2023,  1:13 PM +0530"
    >
  2023-01-20
</em>
       </div>
       <div>
          <span>
<em class="timeago lastmod"
    data-ts="1674472340"
      data-toggle="tooltip" data-placement="bottom" 
      title="Mon, Jan 23, 2023,  8:12 PM +0900"
    >
  2023-01-23
</em>
          </span>
<span class="readtime" data-toggle="tooltip" data-placement="bottom" title="1469 words">8 min</span>
       </div>
     </div>
     <div class="post-content">
       <p>This post is part of the Deep Reinforcement Learning Blog Series.  For a detailed introduction about the series, the curicullum, sources and references used, please check <a href="https://textzip.github.io/posts/DRL-0/">Part 0 - Getting Started</a>.</p>
<h3 id="structure-of-the-blog">Structure of the blog</h3>
<ul>
 <li><a href="https://textzip.github.io/posts/DRL-0/">Part 0 - Getting Started</a>
 <li><a href="https://textzip.github.io/posts/DRL-1/">Part 1 - Multi-Arm Bandits</a>
 <li><a href="https://textzip.github.io/posts/DRL-2/">Part 2 - Finite MDP</a> | <strong>Prerequisites</strong>: <em>python, gymnasium, numpy</em>
 <li><a href="https://textzip.github.io/posts/DRL-3/">Part 3 - Dynamic Programming</a> | <strong>Prerequisites</strong>: <em>python, gymnasium, numpy</em>
 <li><a href="https://textzip.github.io/posts/DRL-4/">Part 4 - Monte Carlo, Temporal Difference &amp; Bootstrapping Methods</a> | <strong>Prerequisites</strong>: <em>Python, gymnasium, numpy</em>
 <li>Part 5 - Deep SARSA and Q-Learning | <strong>Prerequisites</strong>: <em>python, gymnasium, numpy, torch</em>
 <li>Part 6 - REINFORCE, AC, DDPG, TD3, SAC
 <li>Part 7 - A2C, PPO, TRPO, GAE, A3C
 <li>TBA (HER, PER, Distillation)
</ul>
<h1 id="dynamic-programming">Dynamic Programming</h1>
<p>The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as a Markov decision process (MDP). Classical DP algorithms are of limited utility in reinforcement learning both because of their assumption of a perfect model and because of their great computational expense, but they are still important theoretically. DP provides an essential foundation for the understanding of the Approx. methods presented in later parts. In
fact, all of these methods can be viewed as attempts to achieve much the same effect as DP, only with less computation and without assuming a perfect model of the environment.</p>
<h2 id="policy-evaluation">Policy Evaluation</h2>
<p>First we consider how to compute the state-value function $v_\pi$ for an arbitrary policy $\pi$. This is called policy evaluation in the DP literature. We also refer to it as the prediction problem.</p>
<p>Recall that,</p>
\[v_\pi(s) = \sum_{a}\pi(a\mid s)\sum_{s',r} p(s',r\mid a,s)[r+\gamma v_\pi(s')]\]
<p>We can evaluate a given policy $\pi$ by iterativly applying the bellman expectation backup as an update rule until the value function $v(s)$ converges to the $v_\pi(s)$.</p>
<h2 id="policy-improvement">Policy Improvement</h2>
<p>We can then improve a given policy by acting greedily with respect to the given value function for the policy $v_\pi(s)$.</p>
<p>The new policy $\pi’$ is better than or equal to the old policy $\pi$. Therefore, $\pi’ \ge \pi$.</p>
<p>If the improvement stops</p>
\[q_\pi(s,\pi'(s)) = \mathtt{max}_{a \in A} \  q_\pi(s,a) \ =   q_\pi(s,\pi(s)) = v_\pi(s)\]
<p>Therefore,</p>
\[v_\pi(s) =  \mathtt{max}_{a \in A}  q_\pi(s,a)\]
<p>which is the bellman optimality equation and $v_\pi(s) = v_\star(s)$.</p>
<h2 id="policy-iteration">Policy Iteration</h2>
<p>Policy Iteration combines the evaluation and improvement steps into a single algorithm where a random policy is taken, its evaluated and then improved upon and the resulting policy is again evaluated and then improved upon and so on until the policy finally converges and becomes the optimal policy.</p>
<p>Policy Iteration is also refered to as the control problem in DP litrature as opposed to the prediction problem that is policy evaluation.</p>
<p><img src="/assets/img/DRL3/Policy_Iteration.png" alt="image1" class="shadow" /></p>
<p>The algorithm can be summaried as follows:</p>
<p><img src="/assets/img/DRL3/PI_algo.png" alt="image1" class="shadow" /></p>
<p>The $\Delta$ in the above code is used to determine the accuracy of the estimation and can be a value close to 0.</p>
<p>The python code for Policy Iteration is as follows:</p>
<p>This code is part of my collection of RL algorithms, that can be found in my GitHub repo <a href="https://github.com/TextZip/drl-algorithms">drl-algorithms</a>.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">gymnasium</span> <span class="k">as</span> <span class="n">gym</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">'</span><span class="s">FrozenLake-v1</span><span class="sh">'</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">map_name</span><span class="o">=</span><span class="sh">"</span><span class="s">8x8</span><span class="sh">"</span><span class="p">,</span>
               <span class="n">is_slippery</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">render_mode</span><span class="o">=</span><span class="sh">"</span><span class="s">human</span><span class="sh">"</span><span class="p">)</span>
<span class="n">observation</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
<span class="n">terminated</span> <span class="o">=</span> <span class="bp">False</span>

<span class="c1"># state-action table
</span><span class="n">policy_probs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">full</span><span class="p">((</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">n</span><span class="p">,</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">),</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">state_values</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">n</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">policy_iteration</span><span class="p">(</span><span class="n">policy_probs</span><span class="p">,</span> <span class="n">state_values</span><span class="p">,</span> <span class="n">theta</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">):</span>
    <span class="n">policy_stable</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="k">while</span> <span class="n">policy_stable</span> <span class="o">==</span> <span class="bp">False</span><span class="p">:</span>
        <span class="c1"># policy eval
</span>        <span class="n">delta</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">while</span> <span class="n">delta</span> <span class="o">&gt;</span> <span class="n">theta</span><span class="p">:</span>
            <span class="n">delta</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">n</span><span class="p">):</span>
                <span class="n">old_value</span> <span class="o">=</span> <span class="n">state_values</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
                <span class="n">new_state_value</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">):</span>
                    <span class="n">probablity</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">P</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
                    <span class="n">new_state_value</span> <span class="o">+=</span> <span class="n">policy_probs</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span><span class="o">*</span><span class="p">(</span>
                        <span class="n">reward</span> <span class="o">+</span> <span class="p">(</span><span class="n">gamma</span><span class="o">*</span><span class="n">state_values</span><span class="p">[</span><span class="n">next_state</span><span class="p">]))</span>
                <span class="n">state_values</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_state_value</span>
                <span class="n">delta</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="nf">abs</span><span class="p">(</span><span class="n">old_value</span><span class="o">-</span><span class="n">state_values</span><span class="p">[</span><span class="n">state</span><span class="p">]))</span>
        <span class="c1"># policy improvement
</span>        <span class="n">policy_stable</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">n</span><span class="p">):</span>
            <span class="n">old_action</span> <span class="o">=</span> <span class="nf">policy</span><span class="p">(</span><span class="n">state</span><span class="o">=</span><span class="n">state</span><span class="p">)</span>
            <span class="n">action_values</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">)</span>
            <span class="n">max_q</span> <span class="o">=</span> <span class="nf">float</span><span class="p">(</span><span class="sh">'</span><span class="s">-inf</span><span class="sh">'</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">):</span>
                <span class="n">probablity</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">P</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">action_values</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="n">probablity</span> <span class="o">*</span> \
                    <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="p">(</span><span class="n">gamma</span><span class="o">*</span><span class="n">state_values</span><span class="p">[</span><span class="n">next_state</span><span class="p">]))</span>
                <span class="k">if</span> <span class="n">action_values</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">max_q</span><span class="p">:</span>
                    <span class="n">max_q</span> <span class="o">=</span> <span class="n">action_values</span><span class="p">[</span><span class="n">action</span><span class="p">]</span>
                    <span class="n">action_probs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">)</span>
                    <span class="n">action_probs</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">policy_probs</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="n">action_probs</span>
        <span class="c1"># check termination condition and update policy_stable variable
</span>            <span class="k">if</span> <span class="n">old_action</span> <span class="o">!=</span> <span class="nf">policy</span><span class="p">(</span><span class="n">state</span><span class="o">=</span><span class="n">state</span><span class="p">):</span>
                <span class="n">policy_stable</span> <span class="o">=</span> <span class="bp">False</span>


<span class="k">def</span> <span class="nf">policy</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">policy_probs</span><span class="p">[</span><span class="n">state</span><span class="p">])</span>


<span class="nf">policy_iteration</span><span class="p">(</span><span class="n">policy_probs</span><span class="o">=</span><span class="n">policy_probs</span><span class="p">,</span> <span class="n">state_values</span><span class="o">=</span><span class="n">state_values</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Done</span><span class="sh">"</span><span class="p">)</span>
<span class="k">while</span> <span class="ow">not</span> <span class="n">terminated</span><span class="p">:</span>
    <span class="n">action</span> <span class="o">=</span> <span class="nf">policy</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span>
    <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">)</span>
<span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
</pre></table></code></div></div>
<p>Another important point is the fact that we do not need to wait for the policy evaluation to converge to $v_\pi$ before performing policy improvement. Therefore, a stopping condition can be introduced without effecting the performance.</p>
<p>When the policy evaluation step is stopped after a single step, k = 1 we arrive at a special case of policy evaluation which is equivalent to another method called value iteration.</p>
<h2 id="value-iteration">Value Iteration</h2>
<p>The bellman optimality backup equation can be applied iteratively until convergence to arrive at the optimal value function $v_\star(s)$. Unlike policy iteration the itermediate value functions do not correspond to any policy. 
The algorithm can be summaried as follows:</p>
<p><img src="/assets/img/DRL3/VI_algo.png" alt="image1" class="shadow" />
The python code for Value Iteration is as follows:</p>
<p>This code is part of my collection of RL algorithms, that can be found in my GitHub repo <a href="https://github.com/TextZip/drl-algorithms">drl-algorithms</a>.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">gymnasium</span> <span class="k">as</span> <span class="n">gym</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">'</span><span class="s">FrozenLake-v1</span><span class="sh">'</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">map_name</span><span class="o">=</span><span class="sh">"</span><span class="s">8x8</span><span class="sh">"</span><span class="p">,</span>
               <span class="n">is_slippery</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">render_mode</span><span class="o">=</span><span class="sh">"</span><span class="s">human</span><span class="sh">"</span><span class="p">)</span>

<span class="n">observation</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
<span class="n">terminated</span> <span class="o">=</span> <span class="bp">False</span>

<span class="c1"># state-action table
</span><span class="n">policy_probs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">full</span><span class="p">((</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">n</span><span class="p">,</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">),</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">state_values</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">n</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">value_iteration</span><span class="p">(</span><span class="n">policy_probs</span><span class="p">,</span> <span class="n">state_values</span><span class="p">,</span> <span class="n">theta</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">):</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">while</span> <span class="n">delta</span> <span class="o">&gt;</span> <span class="n">theta</span><span class="p">:</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">n</span><span class="p">):</span>
            <span class="n">old_value</span> <span class="o">=</span> <span class="n">state_values</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
            <span class="n">action_values</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">)</span>
            <span class="n">max_q</span> <span class="o">=</span> <span class="nf">float</span><span class="p">(</span><span class="sh">'</span><span class="s">-inf</span><span class="sh">'</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">):</span>
                <span class="n">probablity</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">P</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">action_values</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="n">probablity</span> <span class="o">*</span> \
                    <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="p">(</span><span class="n">gamma</span><span class="o">*</span><span class="n">state_values</span><span class="p">[</span><span class="n">next_state</span><span class="p">]))</span>
                <span class="k">if</span> <span class="n">action_values</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">max_q</span><span class="p">:</span>
                    <span class="n">max_q</span> <span class="o">=</span> <span class="n">action_values</span><span class="p">[</span><span class="n">action</span><span class="p">]</span>
                    <span class="n">action_probs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">)</span>
                    <span class="n">action_probs</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">state_values</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="n">max_q</span>
            <span class="n">policy_probs</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="n">action_probs</span>

            <span class="n">delta</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="nf">abs</span><span class="p">(</span><span class="n">old_value</span> <span class="o">-</span> <span class="n">state_values</span><span class="p">[</span><span class="n">state</span><span class="p">]))</span>


<span class="k">def</span> <span class="nf">policy</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">policy_probs</span><span class="p">[</span><span class="n">state</span><span class="p">])</span>


<span class="nf">value_iteration</span><span class="p">(</span><span class="n">policy_probs</span><span class="o">=</span><span class="n">policy_probs</span><span class="p">,</span> <span class="n">state_values</span><span class="o">=</span><span class="n">state_values</span><span class="p">)</span>

<span class="k">while</span> <span class="ow">not</span> <span class="n">terminated</span><span class="p">:</span>
    <span class="n">action</span> <span class="o">=</span> <span class="nf">policy</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span>
    <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">)</span>
<span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
</pre></table></code></div></div>
<p>This code is part of my collection of RL algorithms, that can be found in my GitHub repo <a href="https://github.com/TextZip/drl-algorithms">drl-algorithms</a>.</p>
<div class="table-wrapper"><table>
<thead>
   <tr>
     <th>Problem
     <th>Bellman Equation
     <th>Algorithm
 <tbody>
   <tr>
     <td>Prediction
     <td>Bellman Expectation Equation
     <td>Iterative Policy Evaluation
   <tr>
     <td>Control
     <td>Bellman Expectation Equation + Greedy Policy Improvement
     <td>Policy Iteration
   <tr>
     <td>Control
     <td>Bellman Optimality Equation
     <td>Value Iteration
</table></div>
<h2 id="async-dynamic-programming">Async. Dynamic Programming</h2>
<p>All the methods discussed till now were synchronous in nature and at each step of the iteration we used a loop to go over all the states. We can however also asynchronously backup states in any order and this will also lead to the same solution as long as all the states are selected atleast once. The added advantage is that this greatly reduces computational time and gives rise to methods like prioritised sweeping and others.</p>
<h2 id="generalized-policy-iteration">Generalized Policy Iteration</h2>
<p>We use the term generalized policy iteration (GPI) to refer
to the general idea of letting policy-evaluation and policy improvement processes interact, independent of the granularity and other details of the two processes. Almost all reinforcement learning methods are well described as GPI. That is, all have identifiable policies and value functions, with the policy always being improved with respect to the value function and the value function always being driven toward the value function for the policy. If both the evaluation process and the improvement process stabilize, that is, no longer produce changes, then the value function and policy must be optimal. The value function stabilizes only when it is consistent with the current policy, and the policy stabilizes only when it is greedy with respect to the current value function. Thus, both processes stabilize only when a policy has been found that is greedy with respect to its own evaluation function. This implies that the Bellman optimality equation holds, and thus that the policy and the value function are optimal.</p>
<h2 id="pros-and-cons-of-dynamic-programming">Pros and Cons of Dynamic Programming</h2>
<p>DP is sometimes thought to be of limited applicability because of the curse of dimensionality, the fact that the number of states often grows exponentially with the number
of state variables. Large state sets do create diculties, but these are inherent difficulties of the problem, not of DP as a solution method. In fact, DP is comparatively better suited to handling large state spaces than competing methods such as direct search and linear programming.</p>
<p>But DP methods assume we know the dynamics of the environment which is one of the biggest limiting factors for their direct use in many cases. In the upcoming parts we will loot at methods that tackle this issue.</p>
     </div>
     <div class="post-tail-wrapper text-muted">
       <div class="post-meta mb-3">
          <i class="far fa-folder-open fa-fw mr-1"></i>
            <a href='/categories/resources/'>Resources</a>,
            <a href='/categories/deep-reinforcement-learning/'>Deep Reinforcement Learning</a>
       </div>
       <div class="post-tags">
          <i class="fa fa-tags fa-fw mr-1"></i>
          <a href="/tags/mdp/"
            class="post-tag no-text-decoration" >mdp</a>
          <a href="/tags/optimal-value/"
            class="post-tag no-text-decoration" >optimal value</a>
          <a href="/tags/bellman/"
            class="post-tag no-text-decoration" >bellman</a>
         </div>
       <div class="post-tail-bottom
          d-flex justify-content-between align-items-center mt-3 pt-5 pb-2">
         <div class="license-wrapper">
            This post is licensed under
            <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a>
            by the author.
         </div>
<div class="share-wrapper">
  <span class="share-label text-muted mr-1">Share</span>
  <span class="share-icons">
        <a href="https://twitter.com/intent/tweet?text=Deep Reinforcement Learning - Part 3 - Dynamic Programming - Jai Krishna&url=https://textzip.github.io/posts/DRL-3/" data-toggle="tooltip" data-placement="top"
          title="Twitter" target="_blank" rel="noopener" aria-label="Twitter">
          <i class="fa-fw fab fa-twitter"></i>
        </a>
        <a href="https://www.facebook.com/sharer/sharer.php?title=Deep Reinforcement Learning - Part 3 - Dynamic Programming - Jai Krishna&u=https://textzip.github.io/posts/DRL-3/" data-toggle="tooltip" data-placement="top"
          title="Facebook" target="_blank" rel="noopener" aria-label="Facebook">
          <i class="fa-fw fab fa-facebook-square"></i>
        </a>
        <a href="https://telegram.me/share?text=Deep Reinforcement Learning - Part 3 - Dynamic Programming - Jai Krishna&url=https://textzip.github.io/posts/DRL-3/" data-toggle="tooltip" data-placement="top"
          title="Telegram" target="_blank" rel="noopener" aria-label="Telegram">
          <i class="fa-fw fab fa-telegram"></i>
        </a>
    <i class="fa-fw fas fa-link small" onclick="copyLink()"
        data-toggle="tooltip" data-placement="top" title="Copy link"></i>
  </span>
</div>
       </div>
     </div>
   </div>
 </div>
<div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down">
 <div class="access">
   <div id="access-lastmod" class="post">
      <span>Recent Update</span>
     <ul class="post-content pl-0 pb-1 ml-1 mt-2">
       <li><a href="/posts/Loco-DRL/">Proprioceptive Locomotion in Unstructured Environments</a>
       <li><a href="/posts/FTG-DRL/">Policy Modulated Trajectory Generation for Quadrupeds</a>
       <li><a href="/posts/NST-DRL/">Neural Style Transfer for Locomotion</a>
       <li><a href="/posts/LIDAR-DRL/">Locomotion with Weighted Belief in Exteroception</a>
       <li><a href="/posts/BiMan-DRL/">Bi-Manual Loco-Manipulation in Quadrupeds for Opening Doors</a>
     </ul>
   </div>
   <div id="access-tags">
      <span>Trending Tags</span>
     <div class="d-flex flex-wrap mt-3 mb-1 mr-3">
        <a class="post-tag" href="/tags/atmega328p/">atmega328p</a>
        <a class="post-tag" href="/tags/imu/">imu</a>
        <a class="post-tag" href="/tags/game/">game</a>
        <a class="post-tag" href="/tags/reinforcement-learning/">reinforcement learning</a>
        <a class="post-tag" href="/tags/bellman/">bellman</a>
        <a class="post-tag" href="/tags/mdp/">mdp</a>
        <a class="post-tag" href="/tags/optimal-value/">optimal value</a>
        <a class="post-tag" href="/tags/touch/">touch</a>
        <a class="post-tag" href="/tags/quadruped/">quadruped</a>
        <a class="post-tag" href="/tags/ros/">ros</a>
     </div>
   </div>
 </div>
    <script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script>
   <div id="toc-wrapper" class="pl-0 pr-4 mb-5">
      <span class="pl-3 pt-2 mb-2">Contents</span>
     <nav id="toc" data-toggle="toc"></nav>
   </div>
</div>
</div>
<div class="row">
 <div class="col-12 col-lg-11 col-xl-8">
   <div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4">
 <div id="related-posts" class="mt-5 mb-2 mb-sm-4">
   <h3 class="pt-2 mt-1 mb-4 ml-1"
      data-toc-skip>Further Reading</h3>
   <div class="card-deck mb-4">
     <div class="card">
        <a href="/posts/DRL-0/">
         <div class="card-body">
<em class="timeago small"
    data-ts="1674027800"
    >
  2023-01-18
</em>
           <h3 class="pt-0 mt-1 mb-3" data-toc-skip>Deep Reinforcement Learning - Part 0 - Getting Started</h3>
           <div class="text-muted small">
             <p>
                Getting into deep reinforcement learning is a tough but rewarding journey. The primary reason it is so difficult for beginners is due to the shear number of fields involved. Deep Reinforcement Lear...
             </p>
           </div>
         </div>
        </a>
     </div>
     <div class="card">
        <a href="/posts/DRL-4/">
         <div class="card-body">
<em class="timeago small"
    data-ts="1674459800"
    >
  2023-01-23
</em>
           <h3 class="pt-0 mt-1 mb-3" data-toc-skip>Deep Reinforcement Learning - Part 4 - Monte Carlo, Temporal Difference & Bootstrapping Methods</h3>
           <div class="text-muted small">
             <p>
                This post is part of the Deep Reinforcement Learning Blog Series.  For a detailed introduction about the series, the curicullum, sources and references used, please check Part 0 - Getting Started.
...
             </p>
           </div>
         </div>
        </a>
     </div>
     <div class="card">
        <a href="/posts/DRL-2/">
         <div class="card-body">
<em class="timeago small"
    data-ts="1668584600"
    >
  2022-11-16
</em>
           <h3 class="pt-0 mt-1 mb-3" data-toc-skip>Deep Reinforcement Learning - Part 2 - Finite MDP</h3>
           <div class="text-muted small">
             <p>
                This post is part of the Deep Reinforcement Learning Blog Series.  For a detailed introduction about the series, the curicullum, sources and references used, please check Part 0 - Getting Started.
...
             </p>
           </div>
         </div>
        </a>
     </div>
   </div>
 </div>
<div class="post-navigation d-flex justify-content-between">
  <a href="/posts/DRL-0/" class="btn btn-outline-primary"
    prompt="Older">
   <p>Deep Reinforcement Learning - Part 0 - Getting Started</p>
  </a>
  <a href="/posts/DRL-4/" class="btn btn-outline-primary"
    prompt="Newer">
   <p>Deep Reinforcement Learning - Part 4 - Monte Carlo, Temporal Difference & Bootstrapping Methods</p>
  </a>
</div>
    <script src="https://utteranc.es/client.js"
        repo="TextZip/textzip.github.io"
        issue-term="title"
        theme="dark-blue"
        crossorigin="anonymous"
        async>
    </script>
   </div>
 </div>
</div>
<footer class="d-flex w-100 justify-content-center">
 <div class="d-flex justify-content-between align-items-center">
   <div class="footer-left">
     <p class="mb-0">
        © 2025
        <a href="https://github.com/TextZip">Bandi Jai Krishna</a>.
        <span data-toggle="tooltip" data-placement="top"
          title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span>
     </p>
   </div>
   <div class="footer-right">
     <p class="mb-0">
        Powered by
        <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>
        with
        <a href="https://github.com/cotes2020/jekyll-theme-chirpy"
          target="_blank" rel="noopener">Chirpy</a>
        theme.
     </p>
   </div>
 </div>
</footer>
     </div>
<div id="search-result-wrapper" class="d-flex justify-content-center unloaded">
 <div class="col-12 col-sm-11 post-content">
   <div id="search-hints">
     <h4 class="text-muted mb-4">Trending Tags</h4>
        <a class="post-tag" href="/tags/atmega328p/">atmega328p</a>
        <a class="post-tag" href="/tags/imu/">imu</a>
        <a class="post-tag" href="/tags/game/">game</a>
        <a class="post-tag" href="/tags/reinforcement-learning/">reinforcement learning</a>
        <a class="post-tag" href="/tags/bellman/">bellman</a>
        <a class="post-tag" href="/tags/mdp/">mdp</a>
        <a class="post-tag" href="/tags/optimal-value/">optimal value</a>
        <a class="post-tag" href="/tags/touch/">touch</a>
        <a class="post-tag" href="/tags/quadruped/">quadruped</a>
        <a class="post-tag" href="/tags/ros/">ros</a>
   </div>
   <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
 </div>
</div>
   </div>
   <div id="mask"></div>
    <a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button">
      <i class="fas fa-angle-up"></i>
    </a>
<script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script>
<script>
SimpleJekyllSearch({
  searchInput: document.getElementById('search-input'),
  resultsContainer: document.getElementById('search-results'),
  json: '/assets/js/data/search.json',
  searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0">  <a href="https://textzip.github.io{url}">{title}</a> <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">    {categories}    {tags} </div><p>{snippet}</p></div>',
  noResultsText: '<p class="mt-5">Oops! No result founds.</p>',
  templateMiddleware: function(prop, value, template) {
    if (prop === 'categories') {
      if (value === '') {
        return `${value}`;
      } else {
        return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
      }
    }
    if (prop === 'tags') {
      if (value === '') {
        return `${value}`;
      } else {
        return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
      }
    }
  }
});
</script>
  
