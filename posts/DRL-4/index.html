<!DOCTYPE html>
<html lang="en-US" 
>
<head>
 <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
 <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Deep Reinforcement Learning - Part 4 - Monte Carlo, Temporal Difference &amp; Bootstrapping Methods" />
<meta name="author" content="Jai Krishna" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This post is part of the Deep Reinforcement Learning Blog Series. For a detailed introduction about the series, the curicullum, sources and references used, please check Part 0 - Getting Started." />
<meta property="og:description" content="This post is part of the Deep Reinforcement Learning Blog Series. For a detailed introduction about the series, the curicullum, sources and references used, please check Part 0 - Getting Started." />
<link rel="canonical" href="https://textzip.github.io/posts/DRL-4/" />
<meta property="og:url" content="https://textzip.github.io/posts/DRL-4/" />
<meta property="og:site_name" content="Jai Krishna" />
<meta property="og:image" content="https://textzip.github.io/assets/img/drl_logo.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-01-23T13:13:20+05:30" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://textzip.github.io/assets/img/drl_logo.jpg" />
<meta property="twitter:title" content="Deep Reinforcement Learning - Part 4 - Monte Carlo, Temporal Difference &amp; Bootstrapping Methods" />
<meta name="twitter:site" content="@twitter_username" />
<meta name="twitter:creator" content="@Jai Krishna" />
<meta name="google-site-verification" content="google_meta_tag_verification" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Jai Krishna"},"dateModified":"2023-01-24T14:55:44+05:30","datePublished":"2023-01-23T13:13:20+05:30","description":"This post is part of the Deep Reinforcement Learning Blog Series. For a detailed introduction about the series, the curicullum, sources and references used, please check Part 0 - Getting Started.","headline":"Deep Reinforcement Learning - Part 4 - Monte Carlo, Temporal Difference &amp; Bootstrapping Methods","image":"https://textzip.github.io/assets/img/drl_logo.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"https://textzip.github.io/posts/DRL-4/"},"url":"https://textzip.github.io/posts/DRL-4/"}</script>
 <title>Deep Reinforcement Learning - Part 4 - Monte Carlo, Temporal Difference & Bootstrapping Methods | Jai Krishna
 </title>
<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png">
<link rel="manifest" href="/assets/img/favicons/site.webmanifest">
<link rel="shortcut icon" href="/assets/img/favicons/favicon.ico">
<meta name="apple-mobile-web-app-title" content="Jai Krishna">
<meta name="application-name" content="Jai Krishna">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous">
<link rel="dns-prefetch" href="https://fonts.gstatic.com">
  <link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials">
  <link rel="dns-prefetch" href="https://www.google-analytics.com">
  <link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous">
  <link rel="dns-prefetch" href="https://www.googletagmanager.com">
<link rel="preconnect" href="https://cdn.jsdelivr.net">
<link rel="dns-prefetch" href="https://cdn.jsdelivr.net">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css">
<link rel="stylesheet" href="/assets/css/style.css">
<link rel="stylesheet"
    href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css">
  <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script defer
    src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script>
  <script async
    src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js"></script>
<script defer src="/assets/js/dist/post.min.js"></script>
  <script>
  /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */
  MathJax = {
    tex: {
      inlineMath: [              /* start/end delimiter pairs for in-line math */
        ['$','$'],
        ['\\(','\\)']
      ],
      displayMath: [             /* start/end delimiter pairs for display math */
        ['$$', '$$'],
        ['\\[', '\\]']
      ]
    }
  };
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>
  <script defer src="/app.js"></script>
<script
  async
  src="https://www.googletagmanager.com/gtag/js?id=G-KGYYX7MFB2"
></script>
<script>
  /* global dataLayer */
  window.dataLayer = window.dataLayer || [];
  function gtag() {
    dataLayer.push(arguments);
  }
  gtag("js", new Date());
  gtag("config", "G-KGYYX7MFB2");
</script>
 <body data-spy="scroll" data-target="#toc">
<div id="sidebar" class="d-flex flex-column align-items-end">
 <div class="profile-wrapper text-center">
   <div id="avatar">
      <a href="/" alt="avatar" class="mx-auto">
        <img src="/assets/img/profile.jpg" alt="avatar" onerror="this.style.display='none'">
      </a>
   </div>
   <div class="site-title mt-3">
      <a href="/">Jai Krishna</a>
   </div>
   <div class="site-subtitle font-italic">Robotics | Electronics | Design</div>
 </div>
 <ul class="w-100">
   <li class="nav-item">
      <a href="/" class="nav-link">
        <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i>
        <span>HOME</span>
      </a>
   <li class="nav-item">
      <a href="/projects/" class="nav-link">
        <i class="fa-fw fas fa-microchip ml-xl-3 mr-xl-3 unloaded"></i>
        <span>PROJECTS</span>
      </a>
   <li class="nav-item">
      <a href="/resources/" class="nav-link">
        <i class="fa-fw fas fa-chalkboard-teacher ml-xl-3 mr-xl-3 unloaded"></i>
        <span>RESOURCES</span>
      </a>
   <li class="nav-item">
      <a href="/categories/" class="nav-link">
        <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i>
        <span>CATEGORIES</span>
      </a>
   <li class="nav-item">
      <a href="/archives/" class="nav-link">
        <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i>
        <span>ARCHIVES</span>
      </a>
   <li class="nav-item">
      <a href="/about/" class="nav-link">
        <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i>
        <span>ABOUT | CONTACT</span>
      </a>
   <li class="nav-item">
      <a href="/tags/" class="nav-link">
        <i class="fa-fw fas fa-chalkboard-teacher ml-xl-3 mr-xl-3 unloaded"></i>
        <span>TAGS</span>
      </a>
 </ul>
 <div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center">
      <a href="https://github.com/TextZip" aria-label="github"
        class="order-3"
        target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
      <a href="
          javascript:location.href = 'mailto:' + ['textzip','gmail.com'].join('@')" aria-label="email"
        class="order-4"
        >
        <i class="fas fa-envelope"></i>
      </a>
      <a href="https://www.linkedin.com/in/jai-krishna-bandi" aria-label="linkedin"
        class="order-5"
        target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
        <span class="icon-border order-2"></span>
      <span id="mode-toggle-wrapper" class="order-1">
<i class="mode-toggle fas fa-adjust"></i>
<script type="text/javascript">
  class ModeToggle {
    static get MODE_KEY() { return "mode"; }
    static get DARK_MODE() { return "dark"; }
    static get LIGHT_MODE() { return "light"; }
    constructor() {
      if (this.hasMode) {
        if (this.isDarkMode) {
          if (!this.isSysDarkPrefer) {
            this.setDark();
          }
        } else {
          if (this.isSysDarkPrefer) {
            this.setLight();
          }
        }
      }
      var self = this;
      /* always follow the system prefers */
      this.sysDarkPrefers.addListener(function() {
        if (self.hasMode) {
          if (self.isDarkMode) {
            if (!self.isSysDarkPrefer) {
              self.setDark();
            }
          } else {
            if (self.isSysDarkPrefer) {
              self.setLight();
            }
          }
          self.clearMode();
        }
        self.updateMermaid();
      });
    } /* constructor() */
    setDark() {
      $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE);
    }
    setLight() {
      $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE);
    }
    clearMode() {
      $('html').removeAttr(ModeToggle.MODE_KEY);
      sessionStorage.removeItem(ModeToggle.MODE_KEY);
    }
    get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); }
    get isSysDarkPrefer() { return this.sysDarkPrefers.matches; }
    get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; }
    get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; }
    get hasMode() { return this.mode != null; }
    get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); }
    /* get the current mode on screen */
    get modeStatus() {
      if (this.isDarkMode
        || (!this.hasMode && this.isSysDarkPrefer) ) {
        return ModeToggle.DARK_MODE;
      } else {
        return ModeToggle.LIGHT_MODE;
      }
    }
    updateMermaid() {
      if (typeof mermaid !== "undefined") {
        let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default");
        let config = { theme: expectedTheme };
        /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */
        $(".mermaid").each(function() {
          let svgCode = $(this).prev().children().html();
          $(this).removeAttr("data-processed");
          $(this).html(svgCode);
        });
        mermaid.initialize(config);
        mermaid.init(undefined, ".mermaid");
      }
    }
    flipMode() {
      if (this.hasMode) {
        if (this.isSysDarkPrefer) {
          if (this.isLightMode) {
            this.clearMode();
          } else {
            this.setLight();
          }
        } else {
          if (this.isDarkMode) {
            this.clearMode();
          } else {
            this.setDark();
          }
        }
      } else {
        if (this.isSysDarkPrefer) {
          this.setLight();
        } else {
          this.setDark();
        }
      }
      this.updateMermaid();
    } /* flipMode() */
  } /* ModeToggle */
  let toggle = new ModeToggle();
  $(".mode-toggle").click(function() {
    toggle.flipMode();
  });
</script>
      </span>
 </div>
</div>
<div id="topbar-wrapper" class="row justify-content-center topbar-down">
 <div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between">
    <span id="breadcrumb">
        <span>
          <a href="/">
            Posts
          </a>
        </span>
          <span>Deep Reinforcement Learning - Part 4 - Monte Carlo, Temporal Difference & Bootstrapping Methods</span>
    </span>
    <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i>
   <div id="topbar-title">
      Post
   </div>
    <i id="search-trigger" class="fas fa-search fa-fw"></i>
    <span id="search-wrapper" class="align-items-center">
      <i class="fas fa-search fa-fw"></i>
      <input class="form-control" id="search-input" type="search"
        aria-label="search" autocomplete="off" placeholder="Search...">
      <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i>
    </span>
    <span id="search-cancel" >Cancel</span>
 </div>
</div>
   <div id="main-wrapper">
     <div id="main">
<div class="row">
 <div id="post-wrapper" class="col-12 col-lg-11 col-xl-8">
   <div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4">
     <h1 data-toc-skip>Deep Reinforcement Learning - Part 4 - Monte Carlo, Temporal Difference & Bootstrapping Methods</h1>
     <div class="post-meta text-muted d-flex flex-column">
       <div>
          <span class="semi-bold">
            Bandi Jai Krishna
          </span>
<em class="timeago"
    data-ts="1674459800"
      data-toggle="tooltip" data-placement="bottom" 
      title="Mon, Jan 23, 2023,  1:13 PM +0530"
    >
  2023-01-23
</em>
       </div>
       <div>
          <span>
<em class="timeago lastmod"
    data-ts="1674552344"
      data-toggle="tooltip" data-placement="bottom" 
      title="Tue, Jan 24, 2023,  6:25 PM +0900"
    >
  2023-01-24
</em>
          </span>
<span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2808 words">15 min</span>
       </div>
     </div>
     <div class="post-content">
       <p>This post is part of the Deep Reinforcement Learning Blog Series.  For a detailed introduction about the series, the curicullum, sources and references used, please check <a href="https://textzip.github.io/posts/DRL-0/">Part 0 - Getting Started</a>.</p>
<h3 id="structure-of-the-blog">Structure of the blog</h3>
<ul>
 <li><a href="https://textzip.github.io/posts/DRL-0/">Part 0 - Getting Started</a>
 <li><a href="https://textzip.github.io/posts/DRL-1/">Part 1 - Multi-Arm Bandits</a>
 <li><a href="https://textzip.github.io/posts/DRL-2/">Part 2 - Finite MDP</a> | <strong>Prerequisites</strong>: <em>python, gymnasium, numpy</em>
 <li><a href="https://textzip.github.io/posts/DRL-3/">Part 3 - Dynamic Programming</a> | <strong>Prerequisites</strong>: <em>python, gymnasium, numpy</em>
 <li><a href="https://textzip.github.io/posts/DRL-4/">Part 4 - Monte Carlo, Temporal Difference &amp; Bootstrapping Methods</a> | <strong>Prerequisites</strong>: <em>Python, gymnasium, numpy</em>
 <li>Part 5 - Deep SARSA and Q-Learning | <strong>Prerequisites</strong>: <em>python, gymnasium, numpy, torch</em>
 <li>Part 6 - REINFORCE, AC, DDPG, TD3, SAC
 <li>Part 7 - A2C, PPO, TRPO, GAE, A3C
 <li>TBA (HER, PER, Distillation)
</ul>
<h1 id="monte-carlo-methods">Monte Carlo Methods</h1>
<p>MC methods improvise over DP methods as they can be used in cases where we do not have a model of the environment. They do this by learning from episodes of experience. Therefore one caviat of MC methods is that they do not work on continous MDPs and learn only from complete episodes (Episodes must terminate).</p>
<h2 id="monte-carlo-prediction">Monte Carlo Prediction</h2>
<p>We begin by considering Monte Carlo methods for learning the state-value function for a given policy. Recall that the value of a state is the expected return—expected cumulative future discounted reward—starting from that state. An obvious way to estimate it from experience, then, is simply to average the returns observed after visits to that state. As more returns are observed, the average should converge to the expected value. This idea underlies all Monte Carlo methods.</p>
<h3 id="on-policy-mc-prediction">On-Policy MC Prediction</h3>
<p>We have a small variation in the fact that we can consider each state only the first time it has been visited while estimating the mean return or we can account for multiple visits to the same state(if any) and the follwoing pseudocodes illustrate both the variations 
<img src="/assets/img/DRL4/first-visit-mc-pred.png" alt="image1" class="shadow" /></p>
<p>The every-visit version can be implemented by removing the “Unless $S_t$ appears in $S_0$, $S_1$, … $S_{t-1}$” line.</p>
<h4 id="incremental-updates">Incremental Updates</h4>
<p>A more computationally efficient method would be to calculate the mean incrementally as follows:</p>
<ul>
 <li>Update $V(s)$ incrementally after episode $S_1,A_1,R_2,….,S_T$
 <li>For each state $S_t$ with return $G_t$
</ul>
\[N(S_t) \leftarrow  N(S_t) + 1\]
\[V(S_t) \leftarrow  V(S_t) + \dfrac{1}{N(S_t)}(G_t-V(S_t))\]
<p>For non-stationary problems, it can be useful to track a running mean (forgets old epiosdes and gives more weight to recent experiences).</p>
\[V(S_t) \leftarrow  V(S_t) + \alpha(G_t-V(S_t))\]
<h3 id="off-policy-mc-prediction">Off-Policy MC Prediction</h3>
<p>Almost all off-policy methods utilize importance sampling, a general technique for estimating expected values under one distribution given samples from another. We apply importance sampling to off-policy learning by weighting returns according to the relative probability of their trajectories occurring under the target and behavior policies, called the importance-sampling ratio. Off-Policy Monte Carlo Prediction can be implemented via two variations of importance sampling which are discussed below.</p>
<h4 id="ordinary-importance-sampling">Ordinary Importance Sampling</h4>
<p>For evaluating a terget policy $\pi(a|s)$ to compute $v_\pi(s)$ or $q_\pi(s,a)$ while following a behaviour policy $\mu(a|s)$.</p>
<p>Given, 
\(\{S_1,A_1,R_2,...,S_T\} \sim \mu\)</p>
<p>We can weight returns $G_t$ according to similarity between the two policies. By multiplying the importance sampling corrections along the whole episode we get:</p>
\[G_t^{\pi/\mu} = \dfrac{\pi(A_t|S_t)\pi(A_{t+1}|S_{t+1})...\pi(A_{T}|S_{T})}{\mu(A_{t}|S_{t})\mu(A_{t+1}|S_{t+1})...\mu(A_{T}|S_{T})}G_t\]
<p>We can then update the state value towards the corrected return like this</p>
\[V(S_t) \leftarrow  V(S_t) + \alpha(G_t^{\pi/\mu}-V(S_t))\]
<p>Note that we cannot use this if $\mu$ is zero when $\pi$ is non-zero, also that importance sampling can increase variance.</p>
<h4 id="weighted-importance-sampling">Weighted Importance Sampling</h4>
<p><img src="/assets/img/DRL4/off-policy-mc-weight-sample-pred.png" alt="image1" class="shadow" />
The derivation of the Weighted Importance Sampling equations has been left-out for the time being.</p>
<p>Ordinary importance sampling is unbiased whereas weighted importance sampling is biased (though the bias converges
asymptotically to zero). On the other hand, the variance of ordinary importance sampling is in general unbounded because the variance of the ratios can be unbounded, whereas in the weighted estimator the largest weight on any single return is one. In fact, assuming bounded returns, the variance of the weighted importance-sampling estimator converges to zero even if the variance of the ratios themselves is infinite (Precup, Sutton, and
Dasgupta 2001). In practice, the weighted estimator usually has dramatically lower variance and is strongly preferred.</p>
<h2 id="monte-carlo-control">Monte Carlo Control</h2>
<p>While it might seem straight forward to implement MC Methods in GPI by plugging MC Prediction for policy evaluation and using greedy policy improvement to complete the cycle, there is one key problem that needs to be addressed.</p>
<p>Greedy policy improvement over $V(s)$ requires knowledge about the MDP
\(\pi'(s) = \mathtt{argmax}_{a\in A}  r(a|s) + p(s'|s,a)V(s')\)</p>
<p>To remain model free we can instead switch to action value functions which will not require prior details about the MDP</p>
\[\pi'(s) = \mathtt{argmax}_{a\in A} Q(s,a)\]
<p>While this solves the issue of knowing the model MDP, we now have a deterministic policy and we will never be able to collect experiences of alternative actions and therefore might miss out on exploration altogether.</p>
<p>This can be solved in the following ways:</p>
<ul>
 <li>
   <p><strong>Exploring Starts:</strong> Every state-action pair has a non-zero probability of being selected as the starting pair, this ensures sufficient exploration but in reality, this might not always be possible.</p>
 <li>
   <p><strong>$\epsilon-$ soft policies:</strong> A small probability to explore every time an action is to be choosen.</p>
 <li>
   <p><strong>Off-Policy:</strong> Use a different policy to collect experience than the one target policy being improved.</p>
</ul>
<h3 id="on-policy-mc-control">On-Policy MC Control</h3>
<h4 id="exploring-starts">Exploring Starts</h4>
<p>The pseudocode for exploring starts can be found below:
<img src="/assets/img/DRL4/mc-es-control.png" alt="image1" class="shadow" /></p>
<h4 id="on-policy-first-visit-mc-control">On-Policy First Visit MC Control</h4>
<p>The pseudocode for On-Policy First Visit MC Control can be found below:
<img src="/assets/img/DRL4/on-policy-fv-mc-control.png" alt="image1" class="shadow" /></p>
<p>The python implementation for the following can be found below:
This code is part of my collection of RL algorithms, that can be found in my GitHub repo <a href="https://github.com/TextZip/drl-algorithms">drl-algorithms</a>.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">gymnasium</span> <span class="k">as</span> <span class="n">gym</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">'</span><span class="s">FrozenLake-v1</span><span class="sh">'</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">map_name</span><span class="o">=</span><span class="sh">"</span><span class="s">8x8</span><span class="sh">"</span><span class="p">,</span>
               <span class="n">is_slippery</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">render_mode</span><span class="o">=</span><span class="sh">"</span><span class="s">rgb_array</span><span class="sh">"</span><span class="p">)</span>

<span class="n">action_values</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">n</span><span class="p">,</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">policy</span><span class="p">(</span><span class="n">state</span><span class="p">,</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">random</span><span class="p">()</span><span class="o">&lt;</span><span class="n">epsilon</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">action_value</span> <span class="o">=</span> <span class="n">action_values</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">flatnonzero</span><span class="p">(</span><span class="n">action_value</span> <span class="o">==</span> <span class="n">action_value</span><span class="p">.</span><span class="nf">max</span><span class="p">()))</span>

<span class="k">def</span> <span class="nf">on_policy_monte_carlo</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span><span class="n">action_values</span><span class="p">,</span><span class="n">episodes</span><span class="p">,</span><span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
    <span class="n">sa_returns</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">episodes</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">state</span><span class="p">,</span><span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
        <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="n">transitions</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="nf">policy</span><span class="p">(</span><span class="n">state</span><span class="o">=</span><span class="n">state</span><span class="p">,</span><span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">)</span>
            <span class="n">next_state</span><span class="p">,</span><span class="n">reward</span><span class="p">,</span><span class="n">terminated</span><span class="p">,</span><span class="n">truncated</span><span class="p">,</span><span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">)</span>
            <span class="n">done</span> <span class="o">=</span> <span class="n">terminated</span> <span class="ow">or</span> <span class="n">truncated</span>
            <span class="n">transitions</span><span class="p">.</span><span class="nf">append</span><span class="p">([</span><span class="n">state</span><span class="p">,</span><span class="n">action</span><span class="p">,</span><span class="n">reward</span><span class="p">])</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

        <span class="n">G</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">state_t</span><span class="p">,</span><span class="n">action_t</span><span class="p">,</span><span class="n">reward_t</span> <span class="ow">in</span> <span class="nf">reversed</span><span class="p">(</span><span class="n">transitions</span><span class="p">):</span>
            <span class="n">G</span> <span class="o">=</span> <span class="n">reward_t</span> <span class="o">+</span> <span class="n">gamma</span><span class="o">*</span><span class="n">G</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">state_t</span><span class="p">,</span><span class="n">action_t</span><span class="p">)</span> <span class="ow">in</span> <span class="n">sa_returns</span><span class="p">:</span>
                <span class="n">sa_returns</span><span class="p">[(</span><span class="n">state_t</span><span class="p">,</span><span class="n">action_t</span><span class="p">)]</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">sa_returns</span><span class="p">[(</span><span class="n">state_t</span><span class="p">,</span><span class="n">action_t</span><span class="p">)].</span><span class="nf">append</span><span class="p">(</span><span class="n">G</span><span class="p">)</span>
                <span class="n">action_values</span><span class="p">[</span><span class="n">state_t</span><span class="p">,</span><span class="n">action_t</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">sa_returns</span><span class="p">[(</span><span class="n">state_t</span><span class="p">,</span><span class="n">action_t</span><span class="p">)])</span>
    <span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>


<span class="nf">on_policy_monte_carlo</span><span class="p">(</span><span class="n">policy</span><span class="o">=</span><span class="n">policy</span><span class="p">,</span><span class="n">action_values</span><span class="o">=</span><span class="n">action_values</span><span class="p">,</span><span class="n">episodes</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">'</span><span class="s">FrozenLake-v1</span><span class="sh">'</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">map_name</span><span class="o">=</span><span class="sh">"</span><span class="s">8x8</span><span class="sh">"</span><span class="p">,</span>
               <span class="n">is_slippery</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">render_mode</span><span class="o">=</span><span class="sh">"</span><span class="s">human</span><span class="sh">"</span><span class="p">)</span>
<span class="n">observation</span><span class="p">,</span><span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
<span class="n">terminated</span> <span class="o">=</span> <span class="bp">False</span>
<span class="k">while</span> <span class="ow">not</span> <span class="n">terminated</span><span class="p">:</span>
    <span class="n">action</span> <span class="o">=</span> <span class="nf">policy</span><span class="p">(</span><span class="n">observation</span><span class="p">,</span><span class="n">epsilon</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">)</span>
<span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
</pre></table></code></div></div>
<h4 id="on-policy-every-visit-mc-control">On-Policy Every Visit MC Control</h4>
<p>On-Policy Every Visit MC Control can be implemented by making a small change to the inner loop of the above code for the first visit version as follows:
This code is part of my collection of RL algorithms, that can be found in my GitHub repo <a href="https://github.com/TextZip/drl-algorithms">drl-algorithms</a>.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">on_policy_monte_carlo</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span><span class="n">action_values</span><span class="p">,</span><span class="n">episodes</span><span class="p">,</span><span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
    <span class="n">sa_returns</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">episodes</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">state</span><span class="p">,</span><span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
        <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="n">transitions</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="nf">policy</span><span class="p">(</span><span class="n">state</span><span class="o">=</span><span class="n">state</span><span class="p">,</span><span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">)</span>
            <span class="n">next_state</span><span class="p">,</span><span class="n">reward</span><span class="p">,</span><span class="n">terminated</span><span class="p">,</span><span class="n">truncated</span><span class="p">,</span><span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">)</span>
            <span class="n">done</span> <span class="o">=</span> <span class="n">terminated</span> <span class="ow">or</span> <span class="n">truncated</span>
            <span class="n">transitions</span><span class="p">.</span><span class="nf">append</span><span class="p">([</span><span class="n">state</span><span class="p">,</span><span class="n">action</span><span class="p">,</span><span class="n">reward</span><span class="p">])</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

        <span class="n">G</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">state_t</span><span class="p">,</span><span class="n">action_t</span><span class="p">,</span><span class="n">reward_t</span> <span class="ow">in</span> <span class="nf">reversed</span><span class="p">(</span><span class="n">transitions</span><span class="p">):</span>
            <span class="n">G</span> <span class="o">=</span> <span class="n">reward_t</span> <span class="o">+</span> <span class="n">gamma</span><span class="o">*</span><span class="n">G</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">state_t</span><span class="p">,</span><span class="n">action_t</span><span class="p">)</span> <span class="ow">in</span> <span class="n">sa_returns</span><span class="p">:</span>
                <span class="n">sa_returns</span><span class="p">[(</span><span class="n">state_t</span><span class="p">,</span><span class="n">action_t</span><span class="p">)]</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">sa_returns</span><span class="p">[(</span><span class="n">state_t</span><span class="p">,</span><span class="n">action_t</span><span class="p">)].</span><span class="nf">append</span><span class="p">(</span><span class="n">G</span><span class="p">)</span>
            <span class="n">action_values</span><span class="p">[</span><span class="n">state_t</span><span class="p">,</span><span class="n">action_t</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">sa_returns</span><span class="p">[(</span><span class="n">state_t</span><span class="p">,</span><span class="n">action_t</span><span class="p">)])</span>
    <span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>

</pre></table></code></div></div>
<h4 id="on-policy-every-visit-constant-alpha-mc-control">On-Policy Every Visit Constant Alpha MC Control</h4>
<p>The constant alpha version is based on the idea of using a running mean instead of using a normal return to deal with non-stationary problems.</p>
<p>The major change being the following equation: 
\(Q(S_t|A_t) \leftarrow  Q(S_t|A_t) + \alpha(G_t-Q(S_t|A_t))\)</p>
<p>The python implementation can be found below:
This code is part of my collection of RL algorithms, that can be found in my GitHub repo <a href="https://github.com/TextZip/drl-algorithms">drl-algorithms</a>.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">gymnasium</span> <span class="k">as</span> <span class="n">gym</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">'</span><span class="s">FrozenLake-v1</span><span class="sh">'</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">map_name</span><span class="o">=</span><span class="sh">"</span><span class="s">8x8</span><span class="sh">"</span><span class="p">,</span>
               <span class="n">is_slippery</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">render_mode</span><span class="o">=</span><span class="sh">"</span><span class="s">rgb_array</span><span class="sh">"</span><span class="p">)</span>

<span class="n">action_values</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">n</span><span class="p">,</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">policy</span><span class="p">(</span><span class="n">state</span><span class="p">,</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">random</span><span class="p">()</span><span class="o">&lt;</span><span class="n">epsilon</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">action_value</span> <span class="o">=</span> <span class="n">action_values</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">flatnonzero</span><span class="p">(</span><span class="n">action_value</span> <span class="o">==</span> <span class="n">action_value</span><span class="p">.</span><span class="nf">max</span><span class="p">()))</span>

<span class="k">def</span> <span class="nf">constant_alpha_monte_carlo</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span><span class="n">action_values</span><span class="p">,</span><span class="n">episodes</span><span class="p">,</span><span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>

    <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">episodes</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">state</span><span class="p">,</span><span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
        <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="n">transitions</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="nf">policy</span><span class="p">(</span><span class="n">state</span><span class="o">=</span><span class="n">state</span><span class="p">,</span><span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">)</span>
            <span class="n">next_state</span><span class="p">,</span><span class="n">reward</span><span class="p">,</span><span class="n">terminated</span><span class="p">,</span><span class="n">truncated</span><span class="p">,</span><span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">)</span>
            <span class="n">done</span> <span class="o">=</span> <span class="n">terminated</span> <span class="ow">or</span> <span class="n">truncated</span>
            <span class="n">transitions</span><span class="p">.</span><span class="nf">append</span><span class="p">([</span><span class="n">state</span><span class="p">,</span><span class="n">action</span><span class="p">,</span><span class="n">reward</span><span class="p">])</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

        <span class="n">G</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">state_t</span><span class="p">,</span><span class="n">action_t</span><span class="p">,</span><span class="n">reward_t</span> <span class="ow">in</span> <span class="nf">reversed</span><span class="p">(</span><span class="n">transitions</span><span class="p">):</span>
            <span class="n">G</span> <span class="o">=</span> <span class="n">reward_t</span> <span class="o">+</span> <span class="n">gamma</span><span class="o">*</span><span class="n">G</span>

            <span class="n">old_value</span> <span class="o">=</span> <span class="n">action_values</span><span class="p">[</span><span class="n">state_t</span><span class="p">,</span><span class="n">action_t</span><span class="p">]</span>
            <span class="n">action_values</span><span class="p">[</span><span class="n">state_t</span><span class="p">,</span><span class="n">action_t</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span><span class="o">*</span><span class="p">(</span><span class="n">G</span><span class="o">-</span><span class="n">old_value</span><span class="p">)</span>

    <span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>


<span class="nf">constant_alpha_monte_carlo</span><span class="p">(</span><span class="n">policy</span><span class="o">=</span><span class="n">policy</span><span class="p">,</span><span class="n">action_values</span><span class="o">=</span><span class="n">action_values</span><span class="p">,</span><span class="n">episodes</span><span class="o">=</span><span class="mi">20000</span><span class="p">)</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">'</span><span class="s">FrozenLake-v1</span><span class="sh">'</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">map_name</span><span class="o">=</span><span class="sh">"</span><span class="s">8x8</span><span class="sh">"</span><span class="p">,</span>
               <span class="n">is_slippery</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">render_mode</span><span class="o">=</span><span class="sh">"</span><span class="s">human</span><span class="sh">"</span><span class="p">)</span>
<span class="n">observation</span><span class="p">,</span><span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
<span class="n">terminated</span> <span class="o">=</span> <span class="bp">False</span>
<span class="k">while</span> <span class="ow">not</span> <span class="n">terminated</span><span class="p">:</span>
    <span class="n">action</span> <span class="o">=</span> <span class="nf">policy</span><span class="p">(</span><span class="n">observation</span><span class="p">,</span><span class="n">epsilon</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">)</span>
<span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
</pre></table></code></div></div>
<h3 id="off-policy-mc-control">Off-Policy MC Control</h3>
<p>In Off-Policy methods, the policy used to generate behavior, called the behavior policy, may in fact be unrelated to the policy that is evaluated and improved, called the target policy. An advantage of this separation is that the target policy may be deterministic (e.g., greedy), while the behavior policy can continue to sample all possible actions.</p>
<p>Off-policy Monte Carlo control methods use one of the techniques presented in the preceding two sections. They follow the behavior policy while learning about and improving the target policy. These techniques require that the behavior policy has a nonzero probability of selecting all actions that might be selected by the target policy (coverage). To explore all possibilities, we require that the behavior policy be soft (i.e., that it select all actions in all states with nonzero probability).</p>
<p><img src="/assets/img/DRL4/off-policy-mc-control.png" alt="image1" class="shadow" /></p>
<p>The python implementation of the above can be found here:
This code is part of my collection of RL algorithms, that can be found in my GitHub repo <a href="https://github.com/TextZip/drl-algorithms">drl-algorithms</a>.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">gymnasium</span> <span class="k">as</span> <span class="n">gym</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">'</span><span class="s">FrozenLake-v1</span><span class="sh">'</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">map_name</span><span class="o">=</span><span class="sh">"</span><span class="s">8x8</span><span class="sh">"</span><span class="p">,</span>
                <span class="n">is_slippery</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">render_mode</span><span class="o">=</span><span class="sh">"</span><span class="s">rgb_array</span><span class="sh">"</span><span class="p">)</span>

<span class="n">action_values</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">n</span><span class="p">,</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">target_policy</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
    <span class="n">action_value</span> <span class="o">=</span> <span class="n">action_values</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">flatnonzero</span><span class="p">(</span><span class="n">action_value</span> <span class="o">==</span> <span class="n">action_value</span><span class="p">.</span><span class="nf">max</span><span class="p">()))</span>

<span class="k">def</span> <span class="nf">exploratory_policy</span><span class="p">(</span><span class="n">state</span><span class="p">,</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">random</span><span class="p">()</span><span class="o">&lt;</span><span class="n">epsilon</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">action_value</span> <span class="o">=</span> <span class="n">action_values</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">flatnonzero</span><span class="p">(</span><span class="n">action_value</span> <span class="o">==</span> <span class="n">action_value</span><span class="p">.</span><span class="nf">max</span><span class="p">()))</span>


<span class="k">def</span> <span class="nf">off_policy_monte_carlo</span><span class="p">(</span><span class="n">action_values</span><span class="p">,</span><span class="n">target_policy</span><span class="p">,</span><span class="n">exploratory_ploicy</span><span class="p">,</span><span class="n">episodes</span><span class="p">,</span><span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
    <span class="n">counter_sa_values</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">n</span><span class="p">,</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">episodes</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">state</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
        <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="n">transitions</span> <span class="o">=</span> <span class="p">[]</span> 

        <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="nf">exploratory_ploicy</span><span class="p">(</span><span class="n">state</span><span class="o">=</span><span class="n">state</span><span class="p">,</span><span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">)</span>
            <span class="n">next_state</span><span class="p">,</span><span class="n">reward</span><span class="p">,</span><span class="n">terminated</span><span class="p">,</span><span class="n">truncated</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">)</span>
            <span class="n">done</span> <span class="o">=</span> <span class="n">terminated</span> <span class="ow">or</span> <span class="n">truncated</span>
            <span class="n">transitions</span><span class="p">.</span><span class="nf">append</span><span class="p">([</span><span class="n">state</span><span class="p">,</span><span class="n">action</span><span class="p">,</span><span class="n">reward</span><span class="p">])</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
        
        <span class="n">G</span> <span class="o">=</span> <span class="mi">0</span> 
        <span class="n">W</span> <span class="o">=</span> <span class="mi">1</span>     

        <span class="k">for</span> <span class="n">state_t</span><span class="p">,</span><span class="n">action_t</span><span class="p">,</span><span class="n">reward_t</span> <span class="ow">in</span> <span class="nf">reversed</span><span class="p">(</span><span class="n">transitions</span><span class="p">):</span>
            <span class="n">G</span> <span class="o">=</span> <span class="n">reward_t</span> <span class="o">+</span> <span class="n">gamma</span><span class="o">*</span><span class="n">G</span>
            <span class="n">counter_sa_values</span><span class="p">[</span><span class="n">state_t</span><span class="p">,</span><span class="n">action_t</span><span class="p">]</span> <span class="o">+=</span> <span class="n">W</span>
            <span class="n">old_value</span> <span class="o">=</span> <span class="n">action_values</span><span class="p">[</span><span class="n">state_t</span><span class="p">,</span><span class="n">action_t</span><span class="p">]</span>
            <span class="n">action_values</span><span class="p">[</span><span class="n">state_t</span><span class="p">,</span><span class="n">action_t</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">W</span><span class="o">/</span><span class="n">counter_sa_values</span><span class="p">[</span><span class="n">state_t</span><span class="p">,</span><span class="n">action_t</span><span class="p">])</span><span class="o">*</span> <span class="p">(</span><span class="n">G</span> <span class="o">-</span> <span class="n">old_value</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">action_t</span> <span class="o">!=</span> <span class="nf">target_policy</span><span class="p">(</span><span class="n">state_t</span><span class="p">):</span>
                <span class="k">break</span>

            <span class="n">W</span> <span class="o">=</span> <span class="n">W</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">epsilon</span> <span class="o">+</span> <span class="p">(</span><span class="n">epsilon</span><span class="o">/</span><span class="mi">4</span><span class="p">)))</span>
    <span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>

<span class="nf">off_policy_monte_carlo</span><span class="p">(</span><span class="n">action_values</span><span class="o">=</span><span class="n">action_values</span><span class="p">,</span><span class="n">target_policy</span><span class="o">=</span><span class="n">target_policy</span><span class="p">,</span><span class="n">exploratory_ploicy</span><span class="o">=</span><span class="n">exploratory_policy</span><span class="p">,</span><span class="n">episodes</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">'</span><span class="s">FrozenLake-v1</span><span class="sh">'</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">map_name</span><span class="o">=</span><span class="sh">"</span><span class="s">8x8</span><span class="sh">"</span><span class="p">,</span>
               <span class="n">is_slippery</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">render_mode</span><span class="o">=</span><span class="sh">"</span><span class="s">human</span><span class="sh">"</span><span class="p">)</span>
<span class="n">observation</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
<span class="n">terminated</span> <span class="o">=</span> <span class="bp">False</span>
<span class="k">while</span> <span class="ow">not</span> <span class="n">terminated</span><span class="p">:</span>
    <span class="n">action</span> <span class="o">=</span> <span class="nf">target_policy</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span>
    <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">)</span>
<span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
</pre></table></code></div></div>
<h1 id="temporal-difference-methods">Temporal Difference Methods</h1>
<p>Temporal Difference methods improvise over MC methods by learning from incomplete episodes of experience using bootstrapping.</p>
<h2 id="temporal-difference-prediction">Temporal Difference Prediction</h2>
<p>The simplest temporal-difference learning algorithm TD(0) works as follows:</p>
<p>The $V(S_t)$ can be updated using the actual return $G_t$</p>
\[V(S_t) \leftarrow V(S_t) + \alpha(G_t - V(S_t))\]
<p>We can replace the actual return $G_t$ with the estimated return $R_{t+1} + \gamma V(S_{t+1})$ as follows</p>
\[V(S_t) \leftarrow V(S_t) + \alpha(R_{t+1} + \gamma V(S_{t+1}) - V(S_t))\]
<p>Where,</p>
<p>$R_{t+1} + \gamma V(S_{t+1})$ is called the TD target</p>
<p>$\delta_t =  R_{t+1} + \gamma V(S_{t+1}) - V(S_t) $ is called the TD error</p>
<h3 id="on-policy-td-prediction">On-Policy TD Prediction</h3>
<p>On-Policy TD prediction can be implemented as explained below
<img src="/assets/img/DRL4/TD-0-pred-online.png" alt="image1" class="shadow" /></p>
<h3 id="off-policy-td-prediction">Off-Policy TD Prediction</h3>
<p>Using Importance Sampling for Off-Policy Learning, we can implement TD Prediction as follows:</p>
<p>Use TD targets generated from $\mu$ to evaluate $\pi$. We can weight the TD target $(R + \gamma V(S’))$ by the importance sampling ratio. Note that we only need to correct a single instane of the prediction unlike MC methods where the whole episde has to be corrected.</p>
\[V(S_t) \leftarrow V(S_t) + \alpha \left( \dfrac{\pi(A_t|S_t)}{\mu(A_t|S_t)}(R_{t+1} + \gamma V(S_{t+1})) - V(S_t) \right)\]
<p>Note that the variance is much lower than MC importance sampling</p>
<h2 id="temporal-difference-control">Temporal Difference Control</h2>
<p>The basic stratergy for using TD methods for control is to plug them into the GPI framework for policy evaluation by learning action values to remain model-free.</p>
<p>The general update rule for action value estimation can be written as:</p>
\[Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha [R_{t+1} + \gamma Q(S_{t+1},A_{t+1}) - Q(S_t,A_t)]\]
<h3 id="on-policy-td-control-sarsa">On-Policy TD Control (SARSA)</h3>
<p>The On-Policy TD Control method is also referred to as SARSA as it uses the quintuple of events $(S_t,A_t,R_{t+1},S_{t+1},A_{t+1})$ for every update.</p>
<p>The pseudocode for SARSA can be found below
<img src="/assets/img/DRL4/sarsa.png" alt="image1" class="shadow" /></p>
<p>The python implementation of the above can be found here:
This code is part of my collection of RL algorithms, that can be found in my GitHub repo <a href="https://github.com/TextZip/drl-algorithms">drl-algorithms</a>.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">gymnasium</span> <span class="k">as</span> <span class="n">gym</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">'</span><span class="s">FrozenLake-v1</span><span class="sh">'</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">map_name</span><span class="o">=</span><span class="sh">"</span><span class="s">8x8</span><span class="sh">"</span><span class="p">,</span>
               <span class="n">is_slippery</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">render_mode</span><span class="o">=</span><span class="sh">"</span><span class="s">rgb_array</span><span class="sh">"</span><span class="p">)</span>

<span class="n">action_values</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">n</span><span class="p">,</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">policy</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">action_value</span> <span class="o">=</span> <span class="n">action_values</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">flatnonzero</span><span class="p">(</span><span class="n">action_value</span> <span class="o">==</span> <span class="n">action_value</span><span class="p">.</span><span class="nf">max</span><span class="p">()))</span>


<span class="k">def</span> <span class="nf">sarsa</span><span class="p">(</span><span class="n">action_values</span><span class="p">,</span> <span class="n">episodes</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>  <span class="c1"># on-policy-td-learning
</span>    <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">episodes</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">state</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
        <span class="n">action</span> <span class="o">=</span> <span class="nf">policy</span><span class="p">(</span><span class="n">state</span><span class="o">=</span><span class="n">state</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">)</span>
        <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>

        <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">termination</span><span class="p">,</span> <span class="n">truncation</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span>
                <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">)</span>
            <span class="n">done</span> <span class="o">=</span> <span class="n">termination</span> <span class="ow">or</span> <span class="n">truncation</span>
            <span class="n">next_action</span> <span class="o">=</span> <span class="nf">policy</span><span class="p">(</span><span class="n">state</span><span class="o">=</span><span class="n">next_state</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">)</span>

            <span class="n">action_value</span> <span class="o">=</span> <span class="n">action_values</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span>
            <span class="n">next_action_value</span> <span class="o">=</span> <span class="n">action_values</span><span class="p">[</span><span class="n">next_state</span><span class="p">,</span> <span class="n">next_action</span><span class="p">]</span>
            <span class="n">action_values</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="n">action_value</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> \
                <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span><span class="o">*</span><span class="n">next_action_value</span> <span class="o">-</span> <span class="n">action_value</span><span class="p">)</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">next_action</span>
    <span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>


<span class="nf">sarsa</span><span class="p">(</span><span class="n">action_values</span><span class="o">=</span><span class="n">action_values</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="n">policy</span><span class="p">)</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">'</span><span class="s">FrozenLake-v1</span><span class="sh">'</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">map_name</span><span class="o">=</span><span class="sh">"</span><span class="s">8x8</span><span class="sh">"</span><span class="p">,</span>
               <span class="n">is_slippery</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">render_mode</span><span class="o">=</span><span class="sh">"</span><span class="s">human</span><span class="sh">"</span><span class="p">)</span>
<span class="n">observation</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
<span class="n">terminated</span> <span class="o">=</span> <span class="bp">False</span>
<span class="k">while</span> <span class="ow">not</span> <span class="n">terminated</span><span class="p">:</span>
    <span class="n">action</span> <span class="o">=</span> <span class="nf">policy</span><span class="p">(</span><span class="n">observation</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">)</span>
<span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
</pre></table></code></div></div>
<h3 id="off-policy-td-control-q-learning">Off-Policy TD Control (Q-Learning)</h3>
<p>Off-Policy TD Control is often referred to as Q-Learning and does not require importance sampling. For TD(0) based Q-Learning, since the action $A_t$ is already determined the  importance sampling ratio essentially becomes 1 and therefore can be ignored.</p>
<p>The next action is chosen using a behaviour policy $A_{t+1} \sim \mu(.|S_t)$ which is $\epsilon$-greedy w.r.t $Q(S,A)$. But we consider alternative successor action $a \sim \pi(.|S_t)$ using the target policy $\pi$ which is greedy w.r.t $Q(S,A)$</p>
<p>The update rule can be written down as follows:</p>
\[Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha[R_{t+1} + \gamma \mathtt{max}_{a} Q(S_{t+1},a) - Q(S_t,A_t)]\]
<p>The pseudocode for Q-Learning can be found below:
 <img src="/assets/img/DRL4/q-learning.png" alt="image1" class="shadow" /></p>
<p>The python implementation of the above can be found here:
This code is part of my collection of RL algorithms, that can be found in my GitHub repo <a href="https://github.com/TextZip/drl-algorithms">drl-algorithms</a>.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">gymnasium</span> <span class="k">as</span> <span class="n">gym</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">'</span><span class="s">FrozenLake-v1</span><span class="sh">'</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">map_name</span><span class="o">=</span><span class="sh">"</span><span class="s">8x8</span><span class="sh">"</span><span class="p">,</span>
               <span class="n">is_slippery</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">render_mode</span><span class="o">=</span><span class="sh">"</span><span class="s">rgb_array</span><span class="sh">"</span><span class="p">)</span>

<span class="n">action_values</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">n</span><span class="p">,</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">target_policy</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
    <span class="n">action_value</span> <span class="o">=</span> <span class="n">action_values</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">flatnonzero</span><span class="p">(</span><span class="n">action_value</span> <span class="o">==</span> <span class="n">action_value</span><span class="p">.</span><span class="nf">max</span><span class="p">()))</span>


<span class="k">def</span> <span class="nf">exploratory_policy</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">q_learning</span><span class="p">(</span><span class="n">action_values</span><span class="p">,</span> <span class="n">episodes</span><span class="p">,</span> <span class="n">target_policy</span><span class="p">,</span> <span class="n">exploratory_policy</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">):</span>  <span class="c1"># on-policy-td-learning
</span>    <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">episodes</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">state</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
        <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>

        <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="nf">exploratory_policy</span><span class="p">()</span>
            <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">termination</span><span class="p">,</span> <span class="n">truncation</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span>
                <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">)</span>
            <span class="n">done</span> <span class="o">=</span> <span class="n">termination</span> <span class="ow">or</span> <span class="n">truncation</span>
            <span class="n">next_action</span> <span class="o">=</span> <span class="nf">target_policy</span><span class="p">(</span><span class="n">state</span><span class="o">=</span><span class="n">next_state</span><span class="p">)</span>

            <span class="n">action_value</span> <span class="o">=</span> <span class="n">action_values</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span>
            <span class="n">next_action_value</span> <span class="o">=</span> <span class="n">action_values</span><span class="p">[</span><span class="n">next_state</span><span class="p">,</span> <span class="n">next_action</span><span class="p">]</span>
            <span class="n">action_values</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="n">action_value</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> \
                <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span><span class="o">*</span><span class="n">next_action_value</span> <span class="o">-</span> <span class="n">action_value</span><span class="p">)</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
    <span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>

<span class="nf">q_learning</span><span class="p">(</span><span class="n">action_values</span><span class="o">=</span><span class="n">action_values</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">target_policy</span><span class="o">=</span><span class="n">target_policy</span><span class="p">,</span> <span class="n">exploratory_policy</span><span class="o">=</span><span class="n">exploratory_policy</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">)</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">'</span><span class="s">FrozenLake-v1</span><span class="sh">'</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">map_name</span><span class="o">=</span><span class="sh">"</span><span class="s">8x8</span><span class="sh">"</span><span class="p">,</span>
               <span class="n">is_slippery</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">render_mode</span><span class="o">=</span><span class="sh">"</span><span class="s">human</span><span class="sh">"</span><span class="p">)</span>
<span class="n">observation</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
<span class="n">terminated</span> <span class="o">=</span> <span class="bp">False</span>

<span class="k">while</span> <span class="ow">not</span> <span class="n">terminated</span><span class="p">:</span>
    <span class="n">action</span> <span class="o">=</span> <span class="nf">target_policy</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span>
    <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">)</span>
<span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
</pre></table></code></div></div>
<h3 id="expected-sarsa">Expected SARSA</h3>
<p>Consider the learning algorithm that is just like Q-learning except that instead of the maximum over next state–action pairs it uses the expected value, taking into account how likely each action is under the current policy. That is, consider the algorithm with the update rule as follows:</p>
\[Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha[R_{t+1} + \gamma \mathbf{E}_\pi[Q(S_{t+1},A_{t+1} | S_{t+1})] - Q(S_t,A_t) ]\]
<p>which can be written as</p>
\[Q(S_t,A_t) = Q(S_t,A_t) + \alpha[R_{t+1} + \gamma \Sigma_a\pi(a|S_{t+1})Q(S_{t+1},a) - Q(S_t,A_t) ]\]
<p>Expected Sarsa is more complex computationally than Sarsa but, in return, it eliminates the variance due to the random selection of At+1. Given the same amount of experience we might expect it to perform slightly better than Sarss.</p>
<h1 id="bootstrapping">Bootstrapping</h1>
<p>While TD(0) methods take one step and estimate the return, MC methods wait till the end of the episode to calculate the return. While both these methods might appear to be very different they can unified by using TD methods to look n-steps into the future, as shown by the image below.
<img src="/assets/img/DRL4/n-step-td.png" alt="image1" class="shadow" /></p>
     </div>
     <div class="post-tail-wrapper text-muted">
       <div class="post-meta mb-3">
          <i class="far fa-folder-open fa-fw mr-1"></i>
            <a href='/categories/resources/'>Resources</a>,
            <a href='/categories/deep-reinforcement-learning/'>Deep Reinforcement Learning</a>
       </div>
       <div class="post-tags">
          <i class="fa fa-tags fa-fw mr-1"></i>
          <a href="/tags/mdp/"
            class="post-tag no-text-decoration" >mdp</a>
          <a href="/tags/optimal-value/"
            class="post-tag no-text-decoration" >optimal value</a>
          <a href="/tags/bellman/"
            class="post-tag no-text-decoration" >bellman</a>
         </div>
       <div class="post-tail-bottom
          d-flex justify-content-between align-items-center mt-3 pt-5 pb-2">
         <div class="license-wrapper">
            This post is licensed under
            <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a>
            by the author.
         </div>
<div class="share-wrapper">
  <span class="share-label text-muted mr-1">Share</span>
  <span class="share-icons">
        <a href="https://twitter.com/intent/tweet?text=Deep Reinforcement Learning - Part 4 - Monte Carlo, Temporal Difference & Bootstrapping Methods - Jai Krishna&url=https://textzip.github.io/posts/DRL-4/" data-toggle="tooltip" data-placement="top"
          title="Twitter" target="_blank" rel="noopener" aria-label="Twitter">
          <i class="fa-fw fab fa-twitter"></i>
        </a>
        <a href="https://www.facebook.com/sharer/sharer.php?title=Deep Reinforcement Learning - Part 4 - Monte Carlo, Temporal Difference & Bootstrapping Methods - Jai Krishna&u=https://textzip.github.io/posts/DRL-4/" data-toggle="tooltip" data-placement="top"
          title="Facebook" target="_blank" rel="noopener" aria-label="Facebook">
          <i class="fa-fw fab fa-facebook-square"></i>
        </a>
        <a href="https://telegram.me/share?text=Deep Reinforcement Learning - Part 4 - Monte Carlo, Temporal Difference & Bootstrapping Methods - Jai Krishna&url=https://textzip.github.io/posts/DRL-4/" data-toggle="tooltip" data-placement="top"
          title="Telegram" target="_blank" rel="noopener" aria-label="Telegram">
          <i class="fa-fw fab fa-telegram"></i>
        </a>
    <i class="fa-fw fas fa-link small" onclick="copyLink()"
        data-toggle="tooltip" data-placement="top" title="Copy link"></i>
  </span>
</div>
       </div>
     </div>
   </div>
 </div>
<div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down">
 <div class="access">
   <div id="access-lastmod" class="post">
      <span>Recent Update</span>
     <ul class="post-content pl-0 pb-1 ml-1 mt-2">
       <li><a href="/posts/Loco-DRL/">Proprioceptive Locomotion in Unstructured Environments</a>
       <li><a href="/posts/FTG-DRL/">Policy Modulated Trajectory Generation for Quadrupeds</a>
       <li><a href="/posts/NST-DRL/">Neural Style Transfer for Locomotion</a>
       <li><a href="/posts/LIDAR-DRL/">Locomotion with Weighted Belief in Exteroception</a>
       <li><a href="/posts/BiMan-DRL/">Bi-Manual Loco-Manipulation in Quadrupeds for Opening Doors</a>
     </ul>
   </div>
   <div id="access-tags">
      <span>Trending Tags</span>
     <div class="d-flex flex-wrap mt-3 mb-1 mr-3">
        <a class="post-tag" href="/tags/atmega328p/">atmega328p</a>
        <a class="post-tag" href="/tags/imu/">imu</a>
        <a class="post-tag" href="/tags/game/">game</a>
        <a class="post-tag" href="/tags/reinforcement-learning/">reinforcement learning</a>
        <a class="post-tag" href="/tags/bellman/">bellman</a>
        <a class="post-tag" href="/tags/mdp/">mdp</a>
        <a class="post-tag" href="/tags/optimal-value/">optimal value</a>
        <a class="post-tag" href="/tags/touch/">touch</a>
        <a class="post-tag" href="/tags/quadruped/">quadruped</a>
        <a class="post-tag" href="/tags/ros/">ros</a>
     </div>
   </div>
 </div>
    <script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script>
   <div id="toc-wrapper" class="pl-0 pr-4 mb-5">
      <span class="pl-3 pt-2 mb-2">Contents</span>
     <nav id="toc" data-toggle="toc"></nav>
   </div>
</div>
</div>
<div class="row">
 <div class="col-12 col-lg-11 col-xl-8">
   <div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4">
 <div id="related-posts" class="mt-5 mb-2 mb-sm-4">
   <h3 class="pt-2 mt-1 mb-4 ml-1"
      data-toc-skip>Further Reading</h3>
   <div class="card-deck mb-4">
     <div class="card">
        <a href="/posts/DRL-0/">
         <div class="card-body">
<em class="timeago small"
    data-ts="1674027800"
    >
  2023-01-18
</em>
           <h3 class="pt-0 mt-1 mb-3" data-toc-skip>Deep Reinforcement Learning - Part 0 - Getting Started</h3>
           <div class="text-muted small">
             <p>
                Getting into deep reinforcement learning is a tough but rewarding journey. The primary reason it is so difficult for beginners is due to the shear number of fields involved. Deep Reinforcement Lear...
             </p>
           </div>
         </div>
        </a>
     </div>
     <div class="card">
        <a href="/posts/DRL-3/">
         <div class="card-body">
<em class="timeago small"
    data-ts="1674200600"
    >
  2023-01-20
</em>
           <h3 class="pt-0 mt-1 mb-3" data-toc-skip>Deep Reinforcement Learning - Part 3 - Dynamic Programming</h3>
           <div class="text-muted small">
             <p>
                This post is part of the Deep Reinforcement Learning Blog Series.  For a detailed introduction about the series, the curicullum, sources and references used, please check Part 0 - Getting Started.
...
             </p>
           </div>
         </div>
        </a>
     </div>
     <div class="card">
        <a href="/posts/DRL-2/">
         <div class="card-body">
<em class="timeago small"
    data-ts="1668584600"
    >
  2022-11-16
</em>
           <h3 class="pt-0 mt-1 mb-3" data-toc-skip>Deep Reinforcement Learning - Part 2 - Finite MDP</h3>
           <div class="text-muted small">
             <p>
                This post is part of the Deep Reinforcement Learning Blog Series.  For a detailed introduction about the series, the curicullum, sources and references used, please check Part 0 - Getting Started.
...
             </p>
           </div>
         </div>
        </a>
     </div>
   </div>
 </div>
<div class="post-navigation d-flex justify-content-between">
  <a href="/posts/DRL-3/" class="btn btn-outline-primary"
    prompt="Older">
   <p>Deep Reinforcement Learning - Part 3 - Dynamic Programming</p>
  </a>
  <a href="/posts/Loco-DRL/" class="btn btn-outline-primary"
    prompt="Newer">
   <p>Proprioceptive Locomotion in Unstructured Environments</p>
  </a>
</div>
    <script src="https://utteranc.es/client.js"
        repo="TextZip/textzip.github.io"
        issue-term="title"
        theme="dark-blue"
        crossorigin="anonymous"
        async>
    </script>
   </div>
 </div>
</div>
<footer class="d-flex w-100 justify-content-center">
 <div class="d-flex justify-content-between align-items-center">
   <div class="footer-left">
     <p class="mb-0">
        © 2025
        <a href="https://github.com/TextZip">Bandi Jai Krishna</a>.
        <span data-toggle="tooltip" data-placement="top"
          title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span>
     </p>
   </div>
   <div class="footer-right">
     <p class="mb-0">
        Powered by
        <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>
        with
        <a href="https://github.com/cotes2020/jekyll-theme-chirpy"
          target="_blank" rel="noopener">Chirpy</a>
        theme.
     </p>
   </div>
 </div>
</footer>
     </div>
<div id="search-result-wrapper" class="d-flex justify-content-center unloaded">
 <div class="col-12 col-sm-11 post-content">
   <div id="search-hints">
     <h4 class="text-muted mb-4">Trending Tags</h4>
        <a class="post-tag" href="/tags/atmega328p/">atmega328p</a>
        <a class="post-tag" href="/tags/imu/">imu</a>
        <a class="post-tag" href="/tags/game/">game</a>
        <a class="post-tag" href="/tags/reinforcement-learning/">reinforcement learning</a>
        <a class="post-tag" href="/tags/bellman/">bellman</a>
        <a class="post-tag" href="/tags/mdp/">mdp</a>
        <a class="post-tag" href="/tags/optimal-value/">optimal value</a>
        <a class="post-tag" href="/tags/touch/">touch</a>
        <a class="post-tag" href="/tags/quadruped/">quadruped</a>
        <a class="post-tag" href="/tags/ros/">ros</a>
   </div>
   <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
 </div>
</div>
   </div>
   <div id="mask"></div>
    <a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button">
      <i class="fas fa-angle-up"></i>
    </a>
<script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script>
<script>
SimpleJekyllSearch({
  searchInput: document.getElementById('search-input'),
  resultsContainer: document.getElementById('search-results'),
  json: '/assets/js/data/search.json',
  searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0">  <a href="https://textzip.github.io{url}">{title}</a> <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">    {categories}    {tags} </div><p>{snippet}</p></div>',
  noResultsText: '<p class="mt-5">Oops! No result founds.</p>',
  templateMiddleware: function(prop, value, template) {
    if (prop === 'categories') {
      if (value === '') {
        return `${value}`;
      } else {
        return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
      }
    }
    if (prop === 'tags') {
      if (value === '') {
        return `${value}`;
      } else {
        return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
      }
    }
  }
});
</script>
  
